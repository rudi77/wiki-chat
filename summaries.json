{
    "C:\\Users\\rudi\\source\\repos\\Tools\\README.md": {
        "summary": "The README file provides instructions for setting up a virtual environment and using a Streamlit application designed for the BluDelta internal developer platform. It has been tested with Python 3.8.10.\n\n### Key Sections:\n\n1. **Creating a Virtual Environment**:\n   - Instructions to create and activate a virtual environment using Python and install required packages from `requirements.txt`.\n\n2. **TOOL App Features**:\n   - The application includes several features:\n     - **BluDelta Environment Builder**: Retrieves and displays a Docker Compose file.\n     - **BluDelta Capture Service**: Captures documents by sending a POST request with various parameters.\n     - **New Customer**: Allows the creation of new customer records in a SQL Server database.\n     - **List Customers**: Displays customer records with a search filter.\n\n3. **Starting the Frontend**:\n   - Command to run the Streamlit application.\n\n4. **Bennu Introduction**:\n   - A link to an introductory resource about Bennu.\n\n5. **Docker Instructions**:\n   - Steps to build, tag, and run a Docker image of the application, including necessary environment variables.\n\n### Notes:\n- The current implementation lacks extensive error handling and security features, which should be considered for production use.\n- The modular design allows for easy addition of new features through separate Python scripts.",
        "content": "# Create virtual environment\n\nNOTE: It is tested with python 3.8.10\n\n```sh\nPS> python -m venv .env\nPS> .env/Scripts/Activate.ps1\nPS> python -m pip install --upgrade pip\nPS> pip install -r ./requirements.txt\n```\n\n# TOOL App\n## Streamlit Application Features\n\nThis tool is designed to act as a simplified internal developer platform for BluDelta. It is intended to be used by developers, the support team, and anyone else working on BluDelta. New features will be added as appropriate to extend the functionality of the platform and aid in the development and support processes.\n\nThe current version of the tool consists of the following features:\n\n1. **Bludelta Environment Builder**: This feature retrieves a Docker Compose file from a provided service URL and displays it within the application with syntax highlighting. It also saves the Docker Compose file locally as `updated_docker_compose.yaml`.\n\n2. **Bludelta Capture Service**: This feature allows users to input various parameters, including a service URL, an x-apikey, and a document to be captured. The document can be a PDF, PNG, JPG, JPEG, or TIFF file. The feature sends a POST request to the service URL, including the entered parameters and the base64-encoded document.\n\n3. **New Customer**: This feature facilitates the creation of a new customer record. Users can input the required details for the new customer in a provided form. Upon form submission, these details are inserted into the `Customer` table in a specified SQL Server database.\n\n4. **List Customers**: This feature fetches and displays all records from the `Customer` table in a specified SQL Server database. The records are displayed in table format within the application. A search functionality is also provided, which allows users to filter the displayed table based on a regular expression.\n\nEach feature is implemented as a separate Python script and can be accessed from a sidebar menu in the main Streamlit application. This modular design makes it easy to add new features as needed - simply create a new Python script for each feature and add it to the `pages` dictionary in the main application.\n\nPlease note that the current implementation of these features is basic and lacks extensive error handling or security measures. For a production application, it would be necessary to implement robust error handling, input validation, and security measures.\n\n\n\n# Start Frontend\n```sh\ncd Bennu/Frontend\nstreamlit.cmd run .\\app.py\n```\n\n# Bennu Introduction\n[Bennu](https://blumatixconsultinggmbh-my.sharepoint.com/:v:/g/personal/r_dittrich_blumatix_at/EYnVGmDzwHZFhCvg1-X0r_MBJzJdTu8DnjW2tvivdsl7yQ?e=N0nl6c)\n\n# Docker \n\n## Build Docker Image\n```sh\ncd BludeltaEnvProvider/Frontend\ndocker build -t blumatixdevregistry.azurecr.io/bennu:version-number .\ndocker push blumatixdevregistry.azurecr.io/bennu:version-number\n```\n\n## Tag Repository\nPlease tag the repo with the version number, e.g. 1.0.0 after having built the image!\n```sh\ngit tag -a 1.0.0 -m \"Version 1.0.0\"\ngit push origin 1.0.0\n```\n\n## Run Docker Image\n\nOn our linux machines, e.g. 78, 80 or 88\n```sh\n docker run --rm -d -p 8511:8501 \\\n    -v /mnt/trainingsdata/bennu/image_classfication:/image_classification \\\n    -v /mnt/mlflow:/mlflow\n    -e GIT_USERNAME='YOUR_USERNAME' \\\n    -e GIT_PASSWORD='YOUR_GIT_PASSWORD' \\\n    -e APIKEY='YOUR_API_KEY' \\\n    blumatixdevregistry.azurecr.io/bennu:version-number\n```",
        "file_name": "README.md"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\app.py": {
        "summary": "The provided Python script is a Streamlit application that serves as a dashboard for various internal tools related to a project named \"Bennu\" for Blumatix. The application sets a wide layout and creates a sidebar with a title and a selectbox for navigation. \n\nIt imports multiple feature modules, each defined to handle specific functionalities within the application, such as capturing data, managing customers, configuring dynamic settings, uploading packages, analyzing logs, classifying documents, and interacting with machine learning workflows. The user can select a page from the sidebar, and the corresponding feature function will be executed based on the selection. \n\nKey features included are:\n- Bludelta Capture Service\n- Customer Management\n- Dynamic Configuration\n- Package Uploading\n- OCR and Document Classification\n- Log File Analysis\n- MLflow Explorer\n- Various LLM (Large Language Model) tools\n\nOverall, this script creates a user-friendly interface for accessing and utilizing different tools and features in the Bennu project.",
        "content": "import streamlit as st\nfrom bludelta_environment_feature import load_feature as bludelta_environment_feature\nfrom bludelta_capture_feature import load_feature as bludelta_capture_feature\nfrom customer_feature import customer_feature\nfrom dynamic_config_feature import dynamic_config_feature\nfrom package_upload_feature import package_upload\nfrom json_plugin_feature import json_plugin_feature\nfrom plugin_info_feature import plugin_info_feature\nfrom llm_chat_feature import llm_chat_feature\nfrom workflow_feature import document_bounding_boxes_feature\nfrom bludelta_logfile_viewer_feature import log_file_viewer_feature\nfrom document_page_classification_feature import doc_page_classification_feature\nfrom document_page_classification_prediction_feature import doc_classification_prediction_feature\nfrom code_ripeye_config_editor_feature import ripeye_config_editor_feature\nfrom llm_workflow_feature import llm_workflow_feature\nfrom ocr_feature import ocr_feature\nfrom code_label_package_allocator import label_package_allocator_feature\nfrom code_mlflow_explorer import mlflow_explorer_feature\nfrom llm_bm_tool import llm_bm_tool\n# from document_splitting_feature import document_splitting_feature\nfrom document_splitting_bm_analysis import benchmark_comparison_feature\n#from code_customer_registration import customer_registration_feature\nfrom http_client_feature import http_client_feature\nfrom label_qa_control_feature import qa_control\n\nst.set_page_config(layout=\"wide\")\nst.sidebar.title(\"Bennu - Blumatix Internal Tools :sunglasses:\\n\\nMenu\")\n\n# Define a dictionary of pages and their corresponding functions\npages = {\n  # \"Bludelta Environment Builder\": bludelta_environment_feature,\n    \"Bludelta Capture Service\": bludelta_capture_feature,\n    \"Bludelta Customers\": customer_feature,\n    \"Dynamic Config\": dynamic_config_feature,\n    \"Upload Package\": package_upload,\n    \"Json Plugin\": json_plugin_feature,\n    \"System or Plugin Info\": plugin_info_feature,\n    # \"LLM ChatBot\" : start_chatbot,\n    # \"LlM Invoice Extractor\": invoice_extractor_feature,\n    \"Bludelta Workflows\": document_bounding_boxes_feature,\n    \"Log File Analyzer\": log_file_viewer_feature,\n    \"Document Page Classification\": doc_page_classification_feature,\n    \"Document Page Classification Prediction\": doc_classification_prediction_feature,\n    \"RIPEye Config Editor\": ripeye_config_editor_feature,\n    #\"Customer Registration\": customer_registration_feature,\n    \"OCR\": ocr_feature,\n    \"Label Package Allocator\": label_package_allocator_feature,\n    #\"CodeKickstarter\": kickstarter_feature,\n    \"MLflow Explorer\": mlflow_explorer_feature,\n    # \"Document Splitting Workflows\": document_splitting_feature,\n    \"Document Splitting BM Analysis\": benchmark_comparison_feature,\n    \"ChatBot\": llm_chat_feature,\n    \"Send to Workflow\": llm_workflow_feature,\n    \"LLM Benchmark Tool\": llm_bm_tool,\n    \"HTTP Client\": http_client_feature,\n    \"Label QA Control\": qa_control\n}\n\n# Display a selectbox in the sidebar with the dictionary keys\npage = st.sidebar.selectbox(\"Choose a page\", list(pages.keys()))\n\n# Call the function associated with the selected page\npages[page]()",
        "file_name": "app.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\bludelta_capture_feature.py": {
        "summary": "The file `bludelta_capture_feature.py` is a Streamlit application that interfaces with the Bludelta Capture Service API for document processing. It allows users to upload documents (PDFs, images, or XML) and configure various service parameters via a sidebar.\n\n### Key Components:\n\n1. **Configuration Class**: \n   - `BludeltaCaptureServiceConfig` stores API keys, URL, filter values, property store, and options for PDF generation and OCR results.\n\n2. **Sidebar Creation**:\n   - `create_capture_sidebar()` function collects user inputs for API keys, URL, filters, property store (as key-value pairs), and options for PDF and OCR features.\n\n3. **Document Upload and Processing**:\n   - The main function `load_feature()` handles document uploads and sends requests to the Bludelta API. \n   - Documents are converted to a base64 string before being sent as part of the request payload.\n\n4. **API Interaction**:\n   - On clicking the \"Send Request\" button, it prepares the request headers and payload, then makes a POST request to the API.\n   - If successful, it displays the JSON response, an embedded PDF preview, and the request body in separate tabs.\n\n5. **Error Handling**:\n   - If the API request fails, an error message is displayed to the user.\n\nOverall, the script provides a user-friendly interface for capturing invoice details from documents using the Bludelta API, incorporating features for document previews and error management.",
        "content": "import requests\nimport streamlit as st\nfrom streamlit_pdf_viewer import pdf_viewer\nimport base64\nimport json\n\nclass BludeltaCaptureServiceConfig:\n    def __init__(self):\n        self.x_apikey = \"\"\n        self.x_api_identifier = \"\"\n        self.url = \"https://api.bludelta.ai/v1-18/invoicedetail/detect\"\n        self.filter_val = 0\n        self.format_val = 0\n        self.property_store = {}\n        self.create_result_pdf = False\n        self.add_ocr_result = False\n        self.add_document_text = False\n        self.languages = \"\"\n\ndef create_capture_sidebar() -> BludeltaCaptureServiceConfig:\n    with st.sidebar:\n        st.title('\u00f0\u0178\u2019\u00ac Bludelta Capture Service')\n        st.subheader('Service Configuration')\n\n        x_apikey = st.text_input(\"Enter the x-apikey of your service:\", \"\")\n        x_api_identifier = st.text_input(\"Enter the x-api-identifier of your service:\", \"\")\n\n        # Input field for service URL\n        url = st.text_input(\"Enter the URL of your service:\", \"https://api.bludelta.ai/v1-18/invoicedetail/detect\")\n\n        # Input fields for the parameters\n        filter_val = st.number_input(\"Filter:\", value=0, format=\"%d\")\n        format_val = st.number_input(\"Format:\", value=0, format=\"%d\")\n        property_store_str = st.text_input(\"PropertyStore (input as 'key1:value1,key2:value2,...'):\", \"\")\n        create_result_pdf = st.checkbox(\"CreateResultPdf\")\n        add_ocr_result = st.checkbox(\"AddOcrResult\")\n        add_document_text = st.checkbox(\"AddDocumentText\")\n        languages = st.text_input(\"Languages:\", \"\")\n\n        # Convert the PropertyStore string into a dictionary\n        property_store = dict(item.split(\":\") for item in property_store_str.split(\",\")) if property_store_str else {}\n\n        config = BludeltaCaptureServiceConfig()\n        config.x_apikey = x_apikey\n        config.x_api_identifier = x_api_identifier\n        config.url = url\n        config.filter_val = filter_val\n        config.format_val = format_val\n        config.property_store = property_store\n        config.create_result_pdf = create_result_pdf\n        config.add_ocr_result = add_ocr_result\n        config.add_document_text = add_document_text\n        config.languages = languages\n\n        return config\n\ndef load_feature():\n    st.title(\"Bludelta Capture Service\")\n\n    # Create the sidebar\n    config = create_capture_sidebar()\n\n    # File uploader for the document to be captured\n    document = st.file_uploader(\"Upload a document to capture:\", type=[\"pdf\", \"png\", \"jpg\", \"jpeg\", \"tiff\", \"xml\"])\n    # Convert the document to a base64 string\n    if document is not None:\n        document_base64 = base64.b64encode(document.read()).decode('utf-8')\n\n    # Button to send the request\n    if st.button(\"Send Request\"):\n        # Prepare the headers and payload\n        headers = {\n            \"X-ApiKey\": config.x_apikey,\n            \"X-ApiIdentifier\": config.x_api_identifier,\n            \"Content-Type\": \"application/json\",\n            'User-Agent': 'CaputreService/1.0'}\n        \n        payload = {\n            \"Filter\": None,\n            \"Invoice\": document_base64,\n            \"Format\": None,\n            \"PropertyStore\": config.property_store,\n            \"CreateResultPdf\": config.create_result_pdf,\n            \"AddOcrResult\": config.add_ocr_result,\n            \"AddDocumentText\": config.add_document_text,\n            \"Languages\": config.languages,\n        }\n\n\n        # Send the request\n        response = requests.post(config.url, headers=headers, json=payload)\n\n        # Check if the request was successful\n        if response.status_code == 200:\n            tab1, tab2, tab3 = st.tabs([\"JSON Result\", \"Embedded PDF\", \"Request Body\"])\n\n            json_result = response.json()\n            # Display the JSON response\n            with tab1:                \n                st.json(json_result)\n            # Display the embedded PDF\n            with tab2:\n                try:\n                    base64_pdf = json_result['EInvoice']['DocumentBinaries'][0]['Embedding']\n                    # Decode the base64 string into PDF content\n                    pdf_bytes = base64.b64decode(base64_pdf)\n                    # create two columns\n                    col1, col2 = st.columns((3,1))\n                    # Display the JSON response\n                    with col1:\n                        st.write(\"### PDF Preview:\")\n                        pdf_viewer(pdf_bytes)            \n                    with col2:\n                        st.download_button(label=\"Download PDF\", data=pdf_bytes, file_name=\"invoice.pdf\", mime=\"application/pdf\")\n                        st.write(\"To view the PDF in an external pdf reader, please download it above.\")                    \n                except Exception as e:\n                    st.error(f\"An error occurred: {e}\")\n        \n            # Display the request body\n            with tab3:\n                st.json(payload)\n        else:\n            print(response.text)\n            st.error(f\"An error occurred: {response.text}\")\n",
        "file_name": "bludelta_capture_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\bludelta_environment_feature.py": {
        "summary": "The script `bludelta_environment_feature.py` is a Streamlit application designed for updating Docker Compose files. It allows users to input a service URL, from which it retrieves YAML content via an HTTP GET request. If the request is successful (HTTP status code 200), the script parses the YAML, formats it for readability, and displays it with syntax highlighting. Additionally, it saves the formatted YAML to a file named `updated_docker_compose.yaml`. If the request fails, it provides an error message to the user.",
        "content": "import requests\nimport streamlit as st\nimport yaml\n\ndef load_feature():\n    st.title(\"Bludelta Environment - Docker Compose Updater\")\n\n    # Input field for service URL\n    url = st.text_input(\"Enter the URL of your service:\", \"\")\n\n    if url:\n        if st.button(\"Update Docker Compose\"):\n            # Send request to the service\n            response = requests.get(url)\n\n            # Check if the request was successful\n            if response.status_code == 200:\n                # Parse and format the YAML content\n                docker_compose = yaml.safe_load(response.text)\n                formatted_yaml = yaml.dump(docker_compose, default_flow_style=False, sort_keys=False)\n\n                # Display the YAML content with syntax highlighting\n                st.code(formatted_yaml, language=\"yaml\")\n\n                # Save the YAML content to disk\n                with open(\"updated_docker_compose.yaml\", \"w\") as f:\n                    f.write(formatted_yaml)\n                st.success(\"Docker Compose file saved as updated_docker_compose.yaml\")\n            else:\n                st.error(f\"An error occurred: {response.text}\")",
        "file_name": "bludelta_environment_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\bludelta_logfile_viewer_feature.py": {
        "summary": "The provided Python script is a Streamlit application designed to analyze log files. Here's a concise summary of its functionality:\n\n- **Title and Upload**: The app starts with a title \"Log File Analyzer\" and prompts users to upload a log file in '.log' format.\n- **Log Parsing**: Upon file upload, it reads the log lines and uses a regular expression to extract execution details, including execution IDs, phases (Start/End), step names, and execution times.\n- **Statistical Analysis**: The app calculates statistical metrics (average, median, maximum, and standard deviation) for each execution step based on the parsed log data.\n- **Display Results**: The results are displayed in a markdown table format, sortable by different criteria (Max Time, Average Time, Median Time).\n- **Visualizations**: It generates bar charts for average, median, maximum execution times, and standard deviation for each step, displayed alongside the statistics.\n\nOverall, this script provides a user-friendly interface for analyzing and visualizing execution times from log files.",
        "content": "import streamlit as st\nimport pandas as pd\nimport re\nfrom collections import defaultdict\nimport statistics\nimport matplotlib.pyplot as plt\n\n\ndef log_file_viewer_feature():\n\n    # Streamlit page configuration\n    st.title('Log File Analyzer')\n    st.write('Upload your log file for analysis.')\n\n    # File uploader\n    uploaded_file = st.file_uploader(\"Choose a log file\", type=['log'])\n    if uploaded_file is not None:\n        # Reading log file\n        log_lines = uploaded_file.readlines()\n        log_lines = [line.decode('utf-8') for line in log_lines]\n\n        # Regex pattern from the notebook\n        log_pattern = re.compile(\n            r'\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3} INFO \\[.*\\]: PID \\d+: (\\w+): (Start|End): ([\\w\\s]+):? ([\\d\\.,]+)? \\[sec\\]?')\n\n        # Replicating the parsing logic from the notebook\n        executions = defaultdict(lambda: defaultdict(list))\n        for line in log_lines:\n            match = log_pattern.search(line)\n            if match:\n                execution_id, phase, step_name, exec_time = match.groups()\n                if phase == \"End\" and exec_time:\n                    exec_time = exec_time.replace(',', '.')\n                    executions[execution_id][step_name].append(float(exec_time))\n\n        # Statistical analysis (adapted from the notebook)\n        step_times = defaultdict(list)\n        for steps in executions.values():\n            for step, time in steps.items():\n                step_times[step].append(time[0])  # Assuming [0] contains the execution time\n\n        stats_results = defaultdict(dict)\n        for step, times in step_times.items():\n            if times:\n                stats_results[step]['Average Time'] = statistics.mean(times)\n                stats_results[step]['Median Time'] = statistics.median(times)\n                stats_results[step]['Max Time'] = max(times)\n                if len(times) > 1:\n                    stats_results[step]['Standard Deviation'] = statistics.stdev(times)\n                else:\n                    stats_results[step]['Standard Deviation'] = 'N/A (single measurement)'\n\n\n        # Create two columns\n        col1, col2 = st.columns(2)\n\n        # Add statistical analysis to the first column\n        with col1:\n            # Display statistics\n            st.write('Statistical Analysis Results:')\n            \n            # Sort criteria selection\n            sort_criteria = st.selectbox('Sort Criteria', ['Max Time', 'Average Time', 'Median Time'])\n            \n            table_header = \"| Step | Average Time | Median Time | Max Time | Standard Deviation |\\n\"\n            table_divider = \"| --- | --- | --- | --- | --- |\\n\"\n            table_rows = \"\"\n            \n            # Sort the results based on the selected criteria\n            sorted_results = sorted(stats_results.items(), key=lambda x: x[1][sort_criteria], reverse=True)\n            \n            for step, metrics in sorted_results:\n                metrics = {k: round(v, 2) if isinstance(v, float) else v for k, v in metrics.items()}\n                table_rows += f\"| {step} | {metrics['Average Time']} | {metrics['Median Time']} | {metrics['Max Time']} | {metrics['Standard Deviation']} |\\n\"\n\n            markdown_table = table_header + table_divider + table_rows\n\n            st.write('Statistical Analysis Results:')\n            st.markdown(markdown_table)\n\n        # Add plots to the second column\n        with col2:\n            # Visualization (adapted from the notebook)\n            fig, axs = plt.subplots(4, 1, figsize=(12, 30))\n            steps = list(stats_results.keys())\n            average_times = [stats_results[step]['Average Time'] for step in steps]\n            median_times = [stats_results[step]['Median Time'] for step in steps]\n            max_times = [stats_results[step]['Max Time'] for step in steps]\n            std_devs = [stats_results[step]['Standard Deviation'] if isinstance(stats_results[step]['Standard Deviation'], float) else 0 for step in steps]\n\n            axs[0].barh(steps, average_times)\n            axs[0].set_title('Average Execution Time per Step')\n\n            axs[1].barh(steps, median_times)\n            axs[1].set_title('Median Execution Time per Step')\n\n            axs[2].barh(steps, max_times)\n            axs[2].set_title('Max Execution Time per Step')\n\n            axs[3].barh(steps, std_devs)\n            axs[3].set_title('Standard Deviation of Execution Time per Step')\n\n            plt.tight_layout()\n            st.pyplot(fig)\n",
        "file_name": "bludelta_logfile_viewer_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\code_customer_registration.py": {
        "summary": "The file `code_customer_registration.py` is a Streamlit application for customer registration, primarily designed for two systems: BLU DELTA and RIPEye. \n\n### Key Features:\n1. **Database Connection**: The app connects to a SQL Server database using the `pyodbc` library. It prompts the user for database credentials and handles connection errors.\n\n2. **Partner and Customer Management**: \n   - It retrieves existing partners and customers from the database and allows users to either select existing entries or create new ones.\n   - Users are guided through a multi-step registration process that includes creating a partner, registering a customer under that partner, and optionally adding a client (mandator) associated with the customer.\n\n3. **Input Fields**: \n   - Users can input various details such as partner and customer names, contact emails, document limits, expiration dates, and specific configuration paths.\n   - There are additional optional fields for adding invoice listener emails and VAT IDs.\n\n4. **User Creation**: The app allows for the creation of a user for the customer or mandator, with associated username and password fields.\n\n5. **Submission**: Upon clicking the \"Register Customer\" button, the app confirms successful registration and displays the entered details.\n\n### User Interface:\n- The app features a sidebar for database connection parameters and uses tabs to separate the registration processes for BLU DELTA and RIPEye.\n\nOverall, the script is structured to facilitate user-friendly customer registration while maintaining backend connectivity to a database.",
        "content": "import streamlit as st\nimport sys\nimport pyodbc\n\n# BACK END\n\ndef connect_to_database(server, database, user, password): #driver, \n    \"\"\" Create a database connection using pyodbc \"\"\"\n\n    if sys.platform == 'linux':\n        driver_name = 'ODBC Driver 17 for SQL Server'\n    elif sys.platform == 'win32':\n        driver_name = 'SQL Server'      \n\n    try:\n        conn_string = f'DRIVER={{{driver_name}}};SERVER={server};DATABASE={database};UID={user};PWD={password}'\n        conn = pyodbc.connect(conn_string)\n        return conn\n    except pyodbc.Error as e:\n        st.sidebar.error(\"Database connection failed: \" + str(e))\n        return None\n    \n# Simulate existing partners and customers (in a real app, this would come from a database)\nexisting_partners = [\"Partner A\", \"Partner B\", \"Partner C\"]\n\nexisting_customers = [\"Customer A\", \"Customer B\", \"Customer C\"]\n\ndef get_partner(conn):\n    \"\"\" Fetch customer list from the database where customers have configurations \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                        SELECT Id, Name \n                        FROM Customer\n                        WHERE BcsId is not NULL;\"\"\")\n            partner = cur.fetchall()\n        return partner\n    return []\n\n\ndef get_customer(conn, partner_id):\n    \"\"\" Fetch customer list from the database where customers have configurations \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"\"\"\n                        SELECT Id, Name \n                        FROM Customer\n                        WHERE CustomerId = ?;\"\"\", (partner_id,))\n            customers = cur.fetchall()\n        return customers\n    return []\n\n###############################################################################\n# FRONT END\n\ndef customer_registration_feature():\n    st.title(\"BLU DELTA Customer Registration\")\n\n    # Database connection parameters\n    st.sidebar.title(\"Database Connection\")\n    #db = st.sidebar.selectbox('Select Database', ('smedbdev_current', 'Database2'))\n    #driver = st.sidebar.text_input('Driver', 'SQL Server')\n    \n    server = st.sidebar.selectbox('Server', ('bcdbserverdev.database.windows.net', 'bcdbserver.database.windows.net'))\n    database = st.sidebar.selectbox('Select RIPEye Database', ('smedbdev_current', 'smedbtest', 'smedb-v1.0'))\n    database2 = st.sidebar.selectbox('Select BLU DELTA Database', ('bcsdb-auth', 'test'))\n    user = st.sidebar.text_input('Username', 'user')\n    password = st.sidebar.text_input('Password', 'password', type=\"password\")\n    conn = connect_to_database(server, database, user, password) #driver, \n\n\n    # Create tabs\n    tab1, tab2 = st.tabs([\"BLU DELTA Registration\", \"RIPEye Registration\"])\n\n    with tab1:\n        st.header(\"Register a new BLU DELTA Customer\")\n\n\n    with tab2:\n        st.header(\"Register a new RIPEye Customer\")\n    \n        # Step 1: Choose or Create a Partner\n        st.header(\"Step 1: Choose an existing Partner or create a new one.\")\n        #st.subheader(\"Choose an existing Partner or create a new one.\")\n        st.markdown(\"\"\"\n                    ***ATTENTION:*** Partners follow the naming convention: ***'partner.partnername'***.\n                    However, there are exceptions such as ***'Ramsauer & St\u00c3\u00bcrmer Software GmbH'*** (= Aptean AT) and ***'RVS Production Partner'***.\n                    Also, ***'partner.aptean'*** is reserved for Aptean DACH and is usually only used for Aptean DACH customers who would like to provide Feedback for Training via RIPEye.\n                    \"\"\")\n        \n        # Simulate existing partners (in a real app, this would come from a database)\n        #existing_partners = [\"Partner A\", \"Partner B\", \"Partner C\"]\n        existing_partners = get_partner(conn)\n        partner_names = [name for _, name in existing_partners]\n        partner_choice = st.selectbox(\"Choose a Partner\", options=[\"Create New Partner\"] + partner_names)\n\n        if partner_choice == \"Create New Partner\":\n            partner_name = st.text_input(\"New Partner Name\", placeholder=\"Enter new partner name such as partner.blumatix\")\n            contact_email = st.text_input(\"Contact Email\", placeholder=\"Enter contact email...\")\n            partner_document_limit = st.number_input(\"Partner Document Limit (Default = 10000)\", min_value=1, value=10000)\n            partner_expiration_date = st.text_input(\"Partner Expiration Date (Default = 2100-01-01)\", value='2100-01-01')\n        else:\n            partner_name = partner_choice\n            #contact_email = st.text_input(\"Contact Email\", placeholder=\"Enter contact email for the chosen partner...\")\n            selected_partner_id = [id for id, name in existing_partners if name == partner_name][0]\n\n        \n\n        st.markdown(\"---\")\n\n        # Step 2: Add a Customer to the Chosen Partner\n        st.header(\"Step 2: Add a new customer to the chosen or created partner.\")\n        #st.subheader(\"Add a new customer to the chosen or created partner.\")\n        st.markdown(\"\"\"\n                    ***ATTENTION:*** Customers follow the naming convention: ***'partner.partnername.customername'***.\n                    However, there are exceptions such as ***'traunerverlag'*** which is an Aptean AT (former Ramsauer & St\u00c3\u00bcrmer Software GmbH) customer.\n                    New Aptean AT customers should be named ***'partner.ramsauerstuermer.customername'***.\n                    \"\"\")\n                    \n\n        #existing_customers = [\"Customer A\", \"Customer B\", \"Customer C\"]\n        \n        if partner_choice == \"Create New Partner\":\n            customer_names = []\n        else:\n            existing_customers = get_customer(conn, selected_partner_id) or []\n            customer_names = [name for _, name in existing_customers] if existing_customers else []\n        \n        customer_choice = st.selectbox(\"Choose a Customer\", options=[\"Create New Customer\"] + customer_names)\n        \n\n        if customer_choice == \"Create New Customer\":\n            customer_name = st.text_input(\"Customer Name\", placeholder=\"Enter customer name such as partner.blumatix.customer1\")\n            customer_contact_email = st.text_input(\"Customer Contact Email\", placeholder=\"Enter customer contact email...\")\n            customer_config_path = st.text_input(\"Customer Config Path\", value=\"\\\\\\\\nas-01\\\\CustomerData\\\\ripeye\\\\RIPEyeDefaultConfig.json\")\n            delete_data_days = st.number_input(\"Delete Data After X Days (Default = 90) from DB and Inbox\", min_value=1, value=90)\n            customer_document_limit = st.number_input(\"Customer Document Limit (Default = 10000)\", min_value=1, value=10000)\n            customer_expiration_date = st.text_input(\"Customer Expiration Date (Default = 2100-01-01)\", value='2100-01-01')\n        \n            # Optional fields\n            st.markdown(\"\"\"\n                        ***Attention**** Aptean AT Customer/Mandator: Add the e-mail address according the csv from Aptean AT. Aptean AT usually needs this for their client/mandator logic.\n                        \n                        Also, don't put the ***same address for mandator and customer***, this causes problems. If a customer has mandators, there should be specific E-Mail Adresses for each mandator. It's possible to add the Invoice Listener only to the mandator (see Step 3).\n                        \"\"\")\n            invoice_listener_email = st.text_input(\"Email for Invoice Listener (Optional)\", placeholder=\"Enter invoice listener email...\")\n            feedback_email = st.text_input(\"Feedback Email informing the Customer about Issues with the received E-Mail (Optional)\", placeholder=\"Enter feedback email...\")\n            receiver_vat_id = st.text_input(\"Receiver VAT ID (Optional)\", placeholder=\"Enter Receiver VAT ID provided by the Partner or Customer\")\n        else:\n            customer_name = customer_choice\n        \n\n        st.markdown(\"---\")\n\n        # Step 3: Add a Client (Mandator) to the Created Customer (Optional)\n        st.header(\"Step 3: Add a Client (Mandator) to the Customer (Optional)\")\n        #st.subheader(\"Add a new client (mandator) to the created customer.\")\n\n        mandator_name = st.text_input(\"Mandator Name\", placeholder=\"Enter mandator name such as partner.blumatix.customer1.mandator1\")\n        mandator_id = st.text_input(\"Mandator ID\", placeholder=\"Enter mandator ID such as 100 provided by the Partner or Customer\")\n        \n        # Optional fields for Mandator\n        st.markdown(\"\"\"\n                    ***Attention**** Aptean AT Customer/Mandator: Add the e-mail address according the csv from Aptean AT. Aptean AT usually needs this for their client/mandator logic.\n                    \n                    Also, don't put the ***same address for mandator and customer***, this causes problems. If a customer has mandators, there should be specific E-Mail Adresses for each mandator.\n                    \"\"\")\n        mandator_invoice_listener_email = st.text_input(\"Mandator Email for Invoice Listener (Optional)\", placeholder=\"Enter invoice listener email...\")\n        mandator_feedback_email = st.text_input(\"Mandator Feedback Email informing the Customer about Issues with the received E-Mail (Optional)\", placeholder=\"Enter feedback email...\")\n        mandator_receiver_vat_id = st.text_input(\"Mandator Receiver VAT ID (Optional)\", placeholder=\"Enter Receiver VAT ID provided by the Partner or Customer\")\n\n        st.markdown(\"---\")\n\n        # Step 4: Create a User for the Customer or Mandator\n        st.header(\"Step 4: Create a User for the Customer or Mandator\")\n        #st.subheader(\"Create a User for the above Customer or Mandator.\")\n\n        # add Option if the user should be related to the customer or mandator.\n        st.markdown(\"\"\"\n                    Aptean AT Customers: Access RIPEye directly. They don't need a user. Other Customers might need a user to access RIPEye (such as Lagerhaus/RVS). \n                    Please check the Wiki for Customer Specific Requirements such as password policies.\n                    \n                    Please add an internal user (***'customername_internal'***) for our Support-Team to access RIPEye for troubleshooting.\n                    \"\"\")\n        username = st.text_input(\"Username\", placeholder=\"Enter username such as customer1_internal\")\n        password = st.text_input(\"Password\", placeholder=\"Enter password...\", type=\"password\")\n        \n\n        st.markdown(\"---\")\n\n        # Submit button to finalize registration\n        if st.button(\"Register Customer\"):\n            st.success(\"Customer registration is successful!\")\n            st.write(f\"Partner Name: {partner_name}\")\n            st.write(f\"Customer Name: {customer_name}\")\n            st.write(f\"Customer Config Path: {customer_config_path}\")\n            st.write(f\"Data Deletion After: {delete_data_days} days\")\n            if mandator_name:\n                st.write(f\"Mandator Name: {mandator_name}\")\n            st.write(f\"Username: {username}\")\n            st.write(\"API Identifier has been generated and sent to the customer.\")\n\nif __name__ == \"__main__\":\n    customer_registration_feature()\n",
        "file_name": "code_customer_registration.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\code_kickstarter_feature.py": {
        "summary": "The file `code_kickstarter_feature.py` implements a Streamlit application called \"CodeKickstarter\" that allows users to create and push a new project. The main function, `kickstarter_feature`, presents a form where users can input a project name, select a programming language (Python or C#), and choose a template. Upon submission, the app triggers the `create_and_push_project` function, which handles the project generation using Cookiecutter, creates a new repository in Azure DevOps, and initializes a Git repository to push the project.\n\nKey components include:\n- A form for user inputs (project name, language, template).\n- Functions to generate the project, create an Azure DevOps repository, and push the project to the repository, although the actual implementation for project generation and Git operations is currently placeholder code. \n\nThe application provides feedback to the user upon successful project creation and pushing.",
        "content": "import streamlit as st\n\ndef kickstarter_feature():\n    # Title for the feature\n    st.title(\"CodeKickstarter\")\n\n    # Form for user inputs\n    with st.form(\"project_form\", clear_on_submit=True):\n        project_name = st.text_input(\"Project Name\", help=\"Enter the name of your new project.\")\n        project_lang = st.selectbox(\"Language\", [\"Python\", \"C#\"], help=\"Select the project language.\")\n        template_choice = st.selectbox(\"Template\", [\"Template 1\", \"Template 2\"], help=\"Select a template for your project.\")\n        submit_button = st.form_submit_button(\"Create Project\")\n\n    if submit_button:\n        # Placeholder for success message\n        create_and_push_project(project_name, project_lang, template_choice)\n        st.success(\"Project created and pushed successfully!\")\n\ndef create_and_push_project(name, lang, template):\n    # 1. Generate project using Cookiecutter based on user selections\n    generate_project(name, lang, template)\n    # 2. Create a new repo in Azure DevOps\n    repo_url = create_azure_devops_repo(name)\n    # 3. Initialize git in the generated project, add, commit, and push to the new repo\n    push_project_to_repo(name, repo_url)\n\ndef generate_project(name, lang, template):\n    # Placeholder for Cookiecutter project generation logic\n    pass\n\ndef create_azure_devops_repo(name):\n    # Placeholder for Azure DevOps repo creation logic\n    return \"https://dev.azure.com/yourOrganization/yourProject/_git/\" + name\n\ndef push_project_to_repo(name, repo_url):\n    # Placeholder for git initialization, add, commit, and push logic\n    pass\n",
        "file_name": "code_kickstarter_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\code_label_package_allocator.py": {
        "summary": "The script `code_label_package_allocator.py` is a Streamlit application designed to manage label package allocation for users based on data stored in a SQL Server database. It allows users to connect to the database, select label types, and calculate label quality metrics using bootstrap analysis.\n\n### Key Functions:\n1. **Database Connection**: \n   - Establishes a connection to a SQL Server database using `pyodbc`, handling different platforms (Linux and Windows).\n\n2. **Data Retrieval**:\n   - Fetches label types, user information, and label data based on selected criteria (verification status, creation date, etc.).\n\n3. **Data Processing**:\n   - Groups label data by user, filters based on minimum and maximum data points, and computes metrics (mean and confidence intervals) for labelers using bootstrap methods.\n\n4. **Excel Handling**:\n   - Manages an Excel file to log the history of allocated label packages, creating the file if it doesn\u2019t exist and appending new entries as necessary.\n\n5. **User Interface**:\n   - Utilizes Streamlit to create an interactive UI where users can select database credentials, choose label types, enter data points, and allocate label packages to users.\n\n6. **Error Handling**:\n   - Provides user feedback on errors during database operations, data fetching, and Excel file manipulation.\n\n### User Workflow:\n- Users provide database connection details and select label types.\n- They specify parameters for calculating label metrics, after which the application computes and displays the results.\n- Users can then allocate label packages to selected users, which updates the database and logs the allocation history in an Excel file.\n\n### Summary:\nOverall, the script serves as a comprehensive tool for analyzing and managing label allocations within a user-friendly interface, leveraging database interactions and statistical methods for quality assessment.",
        "content": "import streamlit as st\nimport pyodbc\nimport sys\nimport random\nfrom scipy import stats\nfrom scipy.stats import bootstrap\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport openpyxl\nfrom datetime import datetime\nimport os\n\ndef connect_to_database(server, database, user, password): #driver, \n    \"\"\" Create a database connection using pyodbc \"\"\"\n\n    if sys.platform == 'linux':\n        driver_name = 'ODBC Driver 17 for SQL Server'\n    elif sys.platform == 'win32':\n        driver_name = 'SQL Server'      \n\n    try:\n        conn_string = f'DRIVER={{{driver_name}}};SERVER={server};DATABASE={database};UID={user};PWD={password}'\n        conn = pyodbc.connect(conn_string)\n        return conn\n    except pyodbc.Error as e:\n        st.sidebar.error(\"Database connection failed: \" + str(e))\n        return None\n    \n\ndef get_label_types(conn):\n    \"\"\" Fetch label types from the database and return them as a list of strings \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT Name FROM LabelType;\")\n            # Fetch all results and extract the 'Name' from each tuple in the list\n            label_types = [row[0] for row in cur.fetchall()]  # Converts list of tuples to list of strings\n        return label_types\n    return []\n\n\ndef get_username(conn):\n    \"\"\"Fetch a dictionary of user IDs to usernames.\"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT Id, UserName FROM [User];\")\n            users = cur.fetchall()\n            # Create a dictionary mapping user IDs to usernames\n            user_dict = {row[0]: row[1] for row in users}\n        return user_dict\n    return {}\n\n\ndef get_label_data(conn, label_types, min_data_points, max_data_points):\n    \"\"\" Fetch label data from the database based on given conditions \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            placeholders = ', '.join(['?'] * len(label_types))\n            sql_query = f\"\"\"\n                SELECT * FROM Label \n                WHERE Type IN (SELECT [Id] FROM LabelType WHERE Name IN ({placeholders})) \n                AND Verified = 1 \n                AND CreatedBy_Id IN (SELECT [Id] FROM [User] WHERE Email IS NOT NULL) \n                AND Created >= DATEADD(day, -185, GETDATE());\n            \"\"\"\n            cur.execute(sql_query, label_types)\n            label_data = fetch_data_as_dict(cur)\n            # Debug: print the first few rows to check structure\n            #st.write(\"Debug: Sample label data:\", label_data[:5])\n\n        # Get usernames\n        user_dict = get_username(conn)\n\n        # Group data by CreatedBy_Id\n        grouped_data = defaultdict(list)\n        for row in label_data:\n            row['UserName'] = user_dict.get(row['CreatedBy_Id'], 'Unknown')\n            grouped_data[row['CreatedBy_Id']].append(row)\n\n        # Filter out groups with fewer than min_data_points\n        filtered_data = []\n        for labeler, data in grouped_data.items():\n            if len(data) >= min_data_points:\n                if len(data) > max_data_points:\n                    data = random.sample(data, max_data_points)  # Randomly sample to max_data_points\n                filtered_data.extend(data)\n            #else:\n            #    st.write(f\"Labeler {labeler} has fewer than {min_data_points} data points and will be excluded.\")\n\n        return filtered_data\n    return []\n\n\ndef fetch_data_as_dict(cursor):\n    \"\"\" Convert cursor rows to a list of dictionaries based on cursor description. \"\"\"\n    columns = [col[0] for col in cursor.description]\n    return [dict(zip(columns, row)) for row in cursor.fetchall()]\n\n\ndef get_metrics_by_labeler(run_bootstrap_calculation, label_data, n_bootstrap_runs, confidence_level, min_data_points):\n    if run_bootstrap_calculation and label_data:\n        metrics_by_labeler = {}\n        labelers = set(row['CreatedBy_Id'] for row in label_data)\n\n        for labeler in labelers:\n            labeler_data = [row['Correct'] for row in label_data if row['CreatedBy_Id'] == labeler]\n\n            # Debug: Print the data for each labeler to check before bootstrap\n            #st.write(f\"Data for labeler {labeler}: {labeler_data}\")\n\n            if len(labeler_data) < min_data_points:\n                st.write(f\"Not enough data for labeler {labeler}. At least two data points are required.\")\n                continue\n\n            try:\n                results = bootstrap((np.array(labeler_data),), np.mean, confidence_level=confidence_level, n_resamples=n_bootstrap_runs)\n                metrics_by_labeler[labeler] = {\n                    'mean': np.mean(labeler_data),\n                    'confidence_interval': (results.confidence_interval.low, results.confidence_interval.high),\n                    'N': len(labeler_data)\n                }\n            except Exception as e:\n                st.write(f\"Bootstrap calculation failed for labeler {labeler}: {e}\")\n\n        return metrics_by_labeler\n    return {}\n\n\ndef get_label_type_ids(conn, label_type_names):\n    \"\"\"Fetch label type IDs from the LabelType table given a list of names.\"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            placeholders = ', '.join(['?'] * len(label_type_names))\n            sql_query = f\"SELECT Id, Name FROM LabelType WHERE Name IN ({placeholders});\"\n            cur.execute(sql_query, label_type_names)\n            label_types = cur.fetchall()\n            # Create a dictionary mapping names to IDs\n            label_type_dict = {row[1]: row[0] for row in label_types}\n        return label_type_dict\n    return {}       \n\n\ndef extend_allocated_package_history(selected_users, label_package_name, combined_label_types, document_ids_input, conn):\n    # File and sheet information\n    excel_file = r\"\\\\nas-01\\CustomerData\\data_ValueDelivery\\zugewiesenePakete_BennuAllocator_2024-08-21.xlsx\"\n    sheet_name = \"LabelerPakete\"\n\n    # Ensure the directory exists\n    directory = os.path.dirname(excel_file)\n    if not os.path.exists(directory):\n        try:\n            os.makedirs(directory)\n            print(f\"Directory created: {directory}\")\n        except OSError as e:\n            print(f\"Error creating directory {directory}: {e}\")\n            return\n\n    # Ensure the Excel file exists\n    if not os.path.exists(excel_file):\n        try:\n            # Create a new file with the required columns if it does not exist\n            df = pd.DataFrame(columns=[\"User Id\", \"Username\", \"Label Package Name\", \"Label Types\", \"Document Ids\", \"Timestamp\"])\n            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n                df.to_excel(writer, sheet_name=sheet_name, index=False)\n            print(f\"Excel file created: {excel_file}\")\n        except Exception as e:\n            print(f\"Error creating Excel file {excel_file}: {e}\")\n            return\n\n    # Load the existing Excel file\n    try:\n        df = pd.read_excel(excel_file, sheet_name=sheet_name)\n    except Exception as e:\n        print(f\"Error reading Excel file {excel_file}: {e}\")\n        return\n\n    # Convert combined_label_types and document_ids_input to strings if they are lists\n    label_types_str = combined_label_types #', '.join(combined_label_types)\n    document_ids_str = document_ids_input #', '.join(map(str, document_ids_input))\n\n    # Get the current timestamp\n    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n    \n    # Get usernames for the selected users\n    username_dict = get_username(conn)\n    \n    # Append new data for each selected user\n    for user_id in selected_users:\n        username = username_dict.get(user_id, \"Unknown\")\n        new_row = {\n            \"User Id\": user_id,\n            \"Username\": username,\n            \"Label Package Name\": label_package_name,\n            \"Label Types\": label_types_str,\n            \"Document Ids\": document_ids_str,\n            \"Timestamp\": timestamp\n        }\n        df = df.append(new_row, ignore_index=True)\n    \n    # Save the updated DataFrame back to the Excel file\n    try:\n        with pd.ExcelWriter(excel_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n            df.to_excel(writer, sheet_name=sheet_name, index=False)\n        print(f\"Data successfully appended to {excel_file}\")\n    except Exception as e:\n        print(f\"Error writing to Excel file {excel_file}: {e}\")\n\n\n###############################################################################\n\ndef label_package_allocator_feature():\n    \"\"\" Main function to render the Streamlit app \"\"\"\n    st.title('Label Package Allocator')\n\n    # Database connection parameters\n    st.sidebar.title(\"Database Connection\")\n    server = st.sidebar.selectbox('Server', ('serverbdsql01.ad.blumatix.com,62434', 'serverbdsql01.ad.blumatix.com,62442', 'SERVERBDSQL01\\BLUDELTA', '192.168.137.60,62434'))\n    database = st.sidebar.selectbox('Select Database', ('bcidb', 'bcidb_dev'))\n    user = st.sidebar.text_input('Username', 'user')\n    password = st.sidebar.text_input('Password', 'password', type=\"password\")\n    \n    conn = connect_to_database(server, database, user, password)\n\n\n    # Selecting label types from the LabelType table\n    st.write('Please select the label types you want to include in the label package.')\n\n    # Define sets of label types\n    label_type_sets = {\n        'Invoice': ['DocumentType', 'InvoiceCurrency', 'DeliveryDate', 'InvoiceDate', 'InvoiceId', 'CustomerId', 'GrandTotalAmount', \n                    'NetAmount', 'VatRate', 'VatAmount', 'SenderVatId', 'ReceiverVatId', 'Iban', 'Bic', 'ReceiverOrderId', \n                    'SenderOrderId', 'ReceiverOrderDate', 'SenderOrderDate', 'TotalNetAmount', 'TotalVatAmount', 'DeliveryNoteId', \n                    'BankCode', 'BankAccount', 'DeliveryPeriodKey', 'DeliveryPeriodValue', 'ReceiverTaxId', 'SenderTaxId'],\n\n        'OrderConfirmation': ['DocumentType', 'InvoiceCurrency', 'DeliveryDate', 'CustomerId', 'GrandTotalAmount', 'NetAmount', 'VatRate',\n                              'VatAmount', 'SenderVatId', 'ReceiverVatId', 'Iban', 'Bic', 'ReceiverOrderId', 'SenderOrderId',\n                              'ReceiverOrderDate', 'SenderOrderDate', 'TotalNetAmount', 'TotalVatAmount', 'BankCode', 'BankAccount',\n                              'ReceiverTaxId', 'SenderTaxId', 'OrderConfirmationId', 'OrderConfirmationDate', 'SurchargeType', 'SurchargeRate',\n                              'SurchargeAmount', 'DeliveryTerm'],\n\n        'Quotation': ['DocumentType', 'InvoiceCurrency', 'DeliveryDate', 'CustomerId', 'GrandTotalAmount', 'NetAmount', 'VatRate',\n                              'VatAmount', 'SenderVatId', 'ReceiverVatId', 'Iban', 'Bic', 'ReceiverOrderId', 'SenderOrderId',\n                              'ReceiverOrderDate', 'SenderOrderDate', 'TotalNetAmount', 'TotalVatAmount', 'BankCode', 'BankAccount',\n                              'ReceiverTaxId', 'SenderTaxId', 'QuotationId', 'QuotationDate'],\n\n        'Contact': ['ContactName', 'Street', 'ZipCode', 'City', 'Country', 'SenderReceiverClassification', 'AttentionName', 'Region'],\n\n        'LineItems': ['LineItemPositionNumberHeader', 'LineItemDescriptionHeader', 'LineItemArticleNumberHeader', 'LineItemQuantityHeader', \n                      'LineItemUnitHeader', 'LineItemUnitPriceHeader', 'LineItemVatRateHeader', 'LineItemAmountHeader', 'LineItemCustomDetailHeader', \n                      'LineItemPositionNumber', 'LineItemDescription', 'LineItemArticleNumber', 'LineItemQuantity', 'LineItemUnit', 'LineItemUnitPrice', \n                      'LineItemVatRate', 'LineItemAmount', 'LineItemCustomDetail', 'LineItemBufferText', 'LineItemDeliveryNoteIdHeader', \n                      'LineItemDeliveryDateHeader', 'LineItemOrderIdHeader', 'LineItemUnitPriceCoefficientHeader', 'LineItemDiscountHeader', \n                      'LineItemDeliveryNoteId', 'LineItemDeliveryDate', 'LineItemOrderId', 'LineItemUnitPriceCoefficient', 'LineItemDiscount', 'LineItemCount'],\n\n        'Automotive': ['FirstRegistrationDate', 'PlateId', 'Mileage', 'VehicleIdentificationNumber']\n    }\n\n    # Predefined sets selection\n    selected_sets = st.multiselect('Select Predefined Label Type Sets', list(label_type_sets.keys()))\n\n    # Combine selected predefined sets into a single list\n    combined_label_types = []\n    for set_name in selected_sets:\n        combined_label_types.extend(label_type_sets[set_name])\n\n    # Allow selection of individual types and combine with predefined sets\n    label_types = get_label_types(conn)\n    selected_individual_types = st.multiselect('Select or add Individual Label Types', label_types, default=label_types[1])\n\n    # Combine individual selections with predefined sets, remove duplicates\n    combined_label_types.extend(selected_individual_types)\n    combined_label_types = list(set(combined_label_types))  # Remove duplicates\n    st.write(f\"Selected Label Types: {combined_label_types}\")\n\n    st.write('Let\\'s find out who the best labeler is for the selected label types. Please provide the following information to calculate the label quality metrics.')\n\n    # Min data length by CreatedBy_Id\n    min_data_points = st.text_input('Min Data Points', '10')\n    max_data_points = st.text_input('Max Data Points', '10000')\n\n    try:\n        min_data_points = int(min_data_points)  # Convert input to integer\n        max_data_points = int(max_data_points)  # Convert input to integer\n    except ValueError:\n        st.error(\"Please enter a valid integer for minimum data points > 2.\")\n        return\n    \n    # Get label data based on the selected label types\n    label_data = get_label_data(conn, combined_label_types, min_data_points, max_data_points)\n\n    # User input for the number of bootstrap runs\n    bootstrap_run_options = [1000, 10000, 100000]\n    n_bootstrap_runs = st.selectbox('Select the number of bootstrap runs:', bootstrap_run_options, index=0)\n\n    \n    # User input for confidence interval\n    confidence_level_options = [0.95, 0.9, 0.99]\n    confidence_level = st.selectbox('Select the confidence level:', confidence_level_options)\n\n    # Run button to initiate bootstrap calculation and display the best labeler for the selected label types\n    if st.button(\"Calculate Label Quality Metrics\"):\n        # Set a flag to indicate that the button is pressed\n        run_bootstrap_calculation = True\n        # Ensure the correct arguments are passed, including the run_bootstrap_calculation flag\n        metrics = get_metrics_by_labeler(run_bootstrap_calculation, label_data, n_bootstrap_runs, confidence_level, min_data_points)\n        #st.write(metrics)\n\n        # Convert metrics to DataFrame\n        metrics_df = pd.DataFrame.from_dict(metrics, orient='index')\n        metrics_df.reset_index(inplace=True)\n        metrics_df.columns = ['Labeler Id', 'Mean', 'Confidence Interval', 'N']\n\n        # Get usernames and map them to the DataFrame\n        username_dict = get_username(conn)\n        metrics_df['Username'] = metrics_df['Labeler Id'].map(username_dict)\n\n        # Round values to 3 decimal places\n        metrics_df['Mean'] = metrics_df['Mean'].round(3)\n\n        # Split Confidence Interval into two columns\n        metrics_df[['CI.Low', 'CI.High']] = pd.DataFrame(metrics_df['Confidence Interval'].tolist(), index=metrics_df.index)\n\n        # Round CI values to 3 decimal places\n        metrics_df['CI.Low'] = metrics_df['CI.Low'].round(3)\n        metrics_df['CI.High'] = metrics_df['CI.High'].round(3)\n\n        # Drop the original Confidence Interval column\n        metrics_df.drop(columns=['Confidence Interval'], inplace=True)\n\n        # Sort by Mean in descending order\n        metrics_df = metrics_df.sort_values(by='Mean', ascending=False)\n\n        # Reorder columns for better readability\n        metrics_df = metrics_df[['Labeler Id', 'Username', 'Mean', 'CI.Low', 'CI.High', 'N']]\n\n        # Reset the index to remove the index column from the display\n        metrics_df.reset_index(drop=True, inplace=True)\n\n        # Show the DataFrame as a table\n        st.dataframe(metrics_df)\n\n        # Extract user IDs for dynamic selection\n        user_ids_from_metrics = metrics_df['Labeler Id'].tolist()\n    else:\n        run_bootstrap_calculation = False\n        user_ids_from_metrics = []\n\n    st.write('Please allocate the label package to your prefered users.')\n\n    # Input for package IDs\n    try:\n\n        selected_users_input = st.text_input('Enter User IDs', '540, 2')\n        selected_users = [int(user.strip()) for user in selected_users_input.split(',') if user.strip().isdigit()]\n        st.write(f\"Selected User IDs: {selected_users}\")\n    except ValueError as e:\n        st.error(\"Please enter valid integers for User IDs separated by commas.\")\n    \"\"\"\n    if user_ids_from_metrics:\n        selected_users = st.multiselect('Select Users', user_ids_from_metrics)\n    else:\n        selected_users = []\n    \"\"\"\n    document_ids_input = st.text_input('Document IDs', '1, 2, 3, 4, 5')\n    st.write(f\"You selected: {document_ids_input}\")\n\n    # Input for label package name\n    st.write('Please provide a short description of the label package. The name should clearly indicate the content of the package such as Customer_Benchmark or ValueDelivery2024-05.')\n    label_package_name = st.text_input('Label Package Name/Description')\n\n    # Update user table in bcidb\n    if st.button(\"Allocate Label Package\"):\n        try:\n            document_ids = [int(x.strip()) for x in document_ids_input.split(',') if x.strip().isdigit()]\n\n            # Fetch the label type IDs using the selected label type names\n            label_type_ids_dict = get_label_type_ids(conn, combined_label_types)\n            label_type_ids = [label_type_ids_dict[name] for name in combined_label_types]\n\n            # Join list of document ids and label type ids into a string\n            assigned_labels = ','.join(map(str, label_type_ids))\n            assigned_invoice_ids = ','.join(map(str, document_ids))\n\n            # Ensure user IDs are integers\n            #selected_users = [int(user.strip()) for user in selected_users if user.strip().isdigit()]\n\n            # Generate the SQL update statement\n            sql_update = f\"\"\"\n                UPDATE [bcidb].[dbo].[User]\n                SET AssignedLabels = '{assigned_labels}', \n                AssignedInvoiceIds = '{assigned_invoice_ids}'\n                WHERE Id IN ({','.join(['?' for _ in selected_users])});\n            \"\"\"\n                \n            with conn.cursor() as cur:\n                # Unpack the selected_users list to match the placeholders\n                cur.execute(sql_update, *selected_users)\n                conn.commit()\n            st.success(\"Labels and document IDs allocated successfully.\")\n\n            # If label allcoation was successful, add the package information to the \n            # zugewiesenePakete.xlsx file where the history of allocated packages is stored.\n            extend_allocated_package_history(selected_users, label_package_name, combined_label_types, document_ids_input, conn)\n\n        except Exception as e:\n            st.error(f\"An error occurred: {e}\")\n\n      \n\nif __name__ == \"__main__\":\n    label_package_allocator_feature()",
        "file_name": "code_label_package_allocator.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\code_mlflow_explorer.py": {
        "summary": "The `code_mlflow_explorer.py` script is a Streamlit application designed to explore MLflow artifacts. It allows users to interactively search for specific document IDs within datasets stored as TSV files associated with MLflow experiment runs.\n\n### Key Features:\n1. **MLflow Integration**:\n   - Connects to an MLflow tracking server specified by a URI.\n   - Allows users to select an experiment and a specific run within that experiment.\n\n2. **Artifact Loading**:\n   - Loads artifacts (train, validation, test datasets) as pandas DataFrames from specified file paths based on the selected experiment and run.\n   - Users can choose a log exception file for additional analysis.\n\n3. **Document ID Search**:\n   - Accepts a list of document IDs input by users and searches these IDs in the loaded DataFrames.\n   - Returns the DataFrames containing the matched document IDs along with a count of unique matches.\n\n4. **User Interaction**:\n   - Provides a simple UI for inputting experiment IDs, selecting runs, and uploading files.\n   - Displays results, including successes and warnings for missing files or no matches found.\n\n5. **Error Handling**:\n   - Includes error handling to alert users to issues when loading files or processing results.\n\nOverall, the script facilitates the exploration of MLflow artifacts, particularly for datasets related to invoices, making it easier for users to find and analyze specific records.",
        "content": "import streamlit as st\nimport mlflow\nfrom mlflow.tracking import MlflowClient\nimport pandas as pd\n#from io import BytesIO\nimport os\nimport sys\n\n\"\"\"\n# Tried to use the MLflow API to download artifacts, but it didn't work\ndef list_experiment_ids():\n    experiments = mlflow.search_experiments()\n    experiment_info = [(experiment.experiment_id, experiment.name) for experiment in experiments]\n    return experiment_info\n\ndef list_run_artifacts(run_id):\n    client = MlflowClient()\n    artifacts = client.list_artifacts(run_id)\n    return [artifact.path for artifact in artifacts]\n\ndef load_artifact_as_dataframe(run_id, artifact_path):\n    client = MlflowClient()\n    artifact_data = client.download_artifacts(run_id, artifact_path)\n    \n    # Read the artifact data directly into a pandas DataFrame\n    df = pd.read_csv(BytesIO(artifact_data), sep='\\t')\n    \n    return df\n\ndef search_document_ids(df, document_ids):\n    # Convert document_ids to integers if they are numeric\n    document_ids = [int(doc_id.strip()) for doc_id in document_ids]\n    \n    # Search for the document IDs in the 'invoice_id' column\n    found_ids = df[df['invoice_id'].isin(document_ids)]\n    return found_ids\n\"\"\"\n\ndef load_artifact_as_dataframe(file_path, has_header=True):\n    if has_header:\n        df = pd.read_csv(file_path, sep='\\t')\n    else:\n        df = pd.read_csv(file_path, sep='\\t', header=None)\n    \n    # Debug: Print the header of the DataFrame\n    #st.write(\"DataFrame Header:\", df.head())  # Display the first few rows of the DataFrame in Streamlit\n    \n    return df\n\n\ndef search_document_ids(df, document_ids):\n    # Ensure document_ids are integers\n    document_ids = [int(doc_id.strip()) for doc_id in document_ids]\n\n    try:\n        if 'invoice_id' in df.columns:\n            # Case 1: DataFrame with header and 'invoice_id' column\n            filtered_df = df[df['invoice_id'].isin(document_ids)]\n            \n            # Drop duplicates to ensure each 'invoice_id' is counted only once\n            unique_invoice_ids = filtered_df['invoice_id'].drop_duplicates()\n        \n        else:\n            # Case 2: DataFrame without header where IDs are in the first column\n            filtered_df = df[df.iloc[:, 0].isin(document_ids)]\n            \n            # Drop duplicates to ensure each ID is counted only once\n            unique_invoice_ids = filtered_df.iloc[:, 0].drop_duplicates()\n\n    except Exception as e:\n        st.error(f\"An error occurred while searching for document IDs: {e}\")\n        return pd.DataFrame(), 0\n\n    # The 'filtered_df' will still contain all rows including duplicates, but the count will be based on unique IDs\n    found_count = unique_invoice_ids.shape[0]\n    \n    return filtered_df, found_count\n\n\n    \n###############################################################################\ndef mlflow_explorer_feature():\n    st.title(\"MLflow Artifact Explorer\")\n\n    tracking_uri = st.text_input(\"MLflow Tracking URI\", \"http://192.168.137.78:30200\")\n    mlflow.set_tracking_uri(tracking_uri)\n\n    # Define base path on NAS\n    if sys.platform == 'linux':\n        base_path = \"/mlflow/development/artifacts/artifacts\"    \n    elif sys.platform == 'win32':\n        base_path = r\"\\\\nas-01\\MLFlow\\development\\artifacts\\artifacts\"\n    \n    \n    # Select Experiment ID\n    selected_experiment_id = st.text_input(\"Experiment Id\", \"12\")\n    \"\"\"\n    experiment_info = list_experiment_ids()\n    experiment_dict = {name: experiment_id for experiment_id, name in experiment_info}\n\n    selected_experiment = st.selectbox(\"Choose an Experiment\", list(experiment_dict.keys()))\n    selected_experiment_id = experiment_dict[selected_experiment]\n    \"\"\"\n\n    client = MlflowClient()\n    runs = client.search_runs(experiment_ids=[selected_experiment_id])\n    run_dict = {run.data.tags.get(\"mlflow.runName\", run.info.run_id): run.info.run_id for run in runs}\n\n    selected_run_name = st.selectbox(\"Choose a Run\", list(run_dict.keys()))\n    selected_run_id = run_dict[selected_run_name]\n\n    document_ids_input = st.text_input(\"Document IDs (comma-separated)\", \"1, 2, 3\")\n    document_ids = [doc_id.strip() for doc_id in document_ids_input.split(',')]\n    len_document_ids = len(document_ids)\n\n    # Define the file names for train, validation, and test sets\n    file_sets = {\n        \"Train\": \"train_index.tsv\",\n        \"Validation\": \"val_index.tsv\",\n        \"Test\": \"test_index.tsv\"\n    }\n\n        # Construct the full path to the artifacts directory\n    artifacts_dir = os.path.join(base_path, selected_experiment_id, selected_run_id, \"artifacts\")\n\n    # Specify the path to the log_exception file\n    #log_exception_input = st.text_input(\"Path to log_exception file\")\n    st.markdown(\"Choose a log_exception file from `\\\\\\\\nas-01\\TrainingsDataExt4` -> model_name -> logs_exceptions\")\n    log_exception_input = st.file_uploader(\"Choose a file\")\n\n    if st.button(\"Search Document IDs in Artifacts\"):\n        #st.write(\"Button clicked!\")  # This should display when the button is clicked\n        results = {}\n        for set_name, file_name in file_sets.items():\n            file_path = os.path.join(artifacts_dir, file_name)\n            #st.write(f\"Checking for file: {file_path}\")  # Display the file being checked\n\n            if os.path.exists(file_path):\n                #st.write(f\"Processing file: {file_name}\")  # Confirm the file is being processed\n                \n                try:\n                    # Load the artifact directly into a DataFrame\n                    df = load_artifact_as_dataframe(file_path)\n                    \n                    # Search for the document IDs in the DataFrame\n                    found_ids_df, found_count = search_document_ids(df, document_ids)\n                    #found_count = found_ids_df.shape[0]\n\n                    # Store the result\n                    results[set_name] = {\n                        \"found_ids\": found_ids_df,\n                        \"count\": found_count\n                    }\n                    #st.write(f\"DataFrame dimensions (rows, columns): {found_ids_df.shape}\")\n                except Exception as e:\n                    st.error(f\"An error occurred while processing {file_name}: {e}\")\n            else:\n                st.warning(f\"File {file_name} not found at {file_path}.\")\n                results[set_name] = {\n                    \"found_ids\": pd.DataFrame(),\n                    \"count\": 0\n                }\n        \n        # Display the results\n        for set_name, result in results.items():\n            st.write(f\"**{set_name} Set**\")\n            if result[\"count\"] > 0:\n                st.success(f\"{result['count']} document IDs from {len_document_ids} found in {file_sets[set_name]}:\")\n                st.write(result[\"found_ids\"])\n            else:\n                st.warning(f\"No document IDs found in {file_sets[set_name]}.\")\n\n        results_log_exception = {}\n        if log_exception_input is not None:\n            try:\n                # Load the artifact directly into a DataFrame\n                df = pd.read_csv(log_exception_input, header=None, sep='\\t')\n                \n                # Search for the document IDs in the DataFrame\n                found_ids_df, found_count = search_document_ids(df, document_ids)\n                #found_count = found_ids_df.shape[0]\n\n                # Store the result\n                results_log_exception = {\n                    \"found_ids\": found_ids_df,\n                    \"count\": found_count\n                }\n                #st.write(f\"DataFrame dimensions (rows, columns): {found_ids_df.shape}\")\n            except Exception as e:\n                st.error(f\"An error occurred while processing {log_exception_input}: {e}\")\n                results_log_exception = {\n                    \"found_ids\": pd.DataFrame(),\n                    \"count\": 0\n                }\n        else:\n            st.warning(f\"No file uploaded yet.\")\n            results_log_exception = {\n                \"found_ids\": pd.DataFrame(),\n                \"count\": 0\n            }\n\n        # Display the results\n        st.write(f\"**Log Exception File**\")\n        if results_log_exception[\"count\"] > 0:\n            st.success(f\"{results_log_exception['count']} document IDs from {len_document_ids} found in log_exception:\")\n\n            # Check if the first row is mistakenly being treated as a header\n            if results_log_exception[\"found_ids\"].columns[0] == results_log_exception[\"found_ids\"].iloc[0, 0]:\n                # Reset the header if the first row is being treated as the header\n                results_log_exception[\"found_ids\"].columns = [f\"Column_{i}\" for i in range(results_log_exception[\"found_ids\"].shape[1])]\n            \n            # Use st.dataframe for better display control\n            st.dataframe(results_log_exception[\"found_ids\"], height=400, width=800)\n        else:\n            st.warning(f\"No document IDs found in log_exception.\")\n\n\n\n                \n    \n    \n    \"\"\"\n    # Tried to use the MLflow API to download artifacts, but it didn't work\n    if st.button(\"Search Document IDs in Artifacts\"):\n        st.write(\"Button clicked!\")  # This should display when the button is clicked\n        results = {}\n        for set_name, file_name in file_sets.items():\n            if file_name in artifacts:\n                st.write(f\"Processing file: {file_name}\")  # This should display the file name being processed\n                try:\n                    # Load the artifact directly into a DataFrame\n                    df = load_artifact_as_dataframe(selected_run_id, file_name)\n                    \n                    # Debug: Print the header of the DataFrame\n                    st.write(f\"DataFrame Header for {file_name}:\")\n                    st.write(df.head())  # Display the first few rows of the DataFrame in Streamlit\n                    \n\n                    # Search for the document IDs in the DataFrame\n                    found_ids_df = search_document_ids(df, document_ids)\n                    found_count = found_ids_df.shape[0]\n\n                    # Store the result\n                    results[set_name] = {\n                        \"found_ids\": found_ids_df,\n                        \"count\": found_count\n                    }\n\n                except Exception as e:\n                    st.error(f\"An error occurred while processing {file_name}: {e}\")\n            else:\n                results[set_name] = {\n                    \"found_ids\": pd.DataFrame(),\n                    \"count\": 0\n                }\n        \n        # Display the results\n        for set_name, result in results.items():\n            st.write(f\"**{set_name} Set**\")\n            if result[\"count\"] > 0:\n                st.success(f\"{result['count']} document IDs found in {file_sets[set_name]}:\")\n                st.write(result[\"found_ids\"])\n            else:\n                st.warning(f\"No document IDs found in {file_sets[set_name]}.\")\n\"\"\"\nif __name__ == \"__main__\":\n    mlflow_explorer_feature()\n",
        "file_name": "code_mlflow_explorer.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\code_ripeye_config_editor_feature.py": {
        "summary": "The Python script `code_ripeye_config_editor_feature.py` is designed as a Streamlit application for editing customer configuration data stored in a SQL Server database. It includes the following key components:\n\n1. **Database Connection**: A function (`connect_to_database`) establishes a connection to the SQL Server database using the `pyodbc` library, handling different drivers based on the operating system.\n\n2. **Customer Retrieval**: The function `get_customers` retrieves a list of customers who have configurations, using a SQL JOIN query between the `Customer` and `CustomerConfiguration` tables.\n\n3. **Configuration Fetching**: The function `get_customer_config` fetches the JSON configuration for a selected customer, formatting it for readability.\n\n4. **Configuration Update**: The function `update_customer_config` updates the configuration JSON for a specific customer in the database.\n\n5. **Streamlit Interface**: The main function (`ripeye_config_editor_feature`) sets up the Streamlit app interface, allowing users to select a database server and input credentials. Users can select a customer, view and edit their configuration JSON, and save changes back to the database.\n\nOverall, the script provides a user-friendly interface for managing customer configurations in a SQL database, with a focus on error handling and user feedback.",
        "content": "import streamlit as st\nimport pyodbc\nimport json\nimport sys\n\ndef connect_to_database(server, database, user, password): #driver, \n    \"\"\" Create a database connection using pyodbc \"\"\"\n\n    if sys.platform == 'linux':\n        driver_name = 'ODBC Driver 17 for SQL Server'\n    elif sys.platform == 'win32':\n        driver_name = 'SQL Server'      \n\n    try:\n        conn_string = f'DRIVER={{{driver_name}}};SERVER={server};DATABASE={database};UID={user};PWD={password}'\n        conn = pyodbc.connect(conn_string)\n        return conn\n    except pyodbc.Error as e:\n        st.sidebar.error(\"Database connection failed: \" + str(e))\n        return None\n'''\ndef get_customers(conn):\n    \"\"\" Fetch customer list from the database \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT Id, Name FROM Customer;\")\n            customers = cur.fetchall()\n        return customers\n    return []\n'''\ndef get_customers(conn):\n    \"\"\" Fetch customer list from the database where customers have configurations \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            #cur.execute(\"SELECT Id, Name FROM Customer;\")\n            cur.execute(\"\"\"\n                SELECT c.Id, c.Name \n                FROM Customer c\n                JOIN CustomerConfiguration cc ON c.Id = cc.CustomerId;\n            \"\"\")\n            customers = cur.fetchall()\n        return customers\n    return []\n\n'''\ndef get_customer_config(conn, customer_id):\n    \"\"\" Fetch the configuration JSON for a specific customer \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT ConfigurationJson FROM CustomerConfiguration WHERE CustomerId = ?;\", (customer_id,))\n            config_json = cur.fetchone()\n        return config_json[0] if config_json else \"{}\"\n    return \"{}\"\n'''\ndef get_customer_config(conn, customer_id):\n    \"\"\" Fetch the configuration JSON for a specific customer, formatted for readability \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT ConfigurationJson FROM CustomerConfiguration WHERE CustomerId = ?;\", (customer_id,))\n            result = cur.fetchone()\n        # Check if result is not None and not empty\n        if result and result[0]:\n            # Parse JSON string into Python dictionary\n            config_data = json.loads(result[0])\n            # Convert dictionary back to JSON string with indentation\n            formatted_json = json.dumps(config_data, indent=4)\n            return formatted_json\n        else:\n            return \"{}\"\n    return \"{}\"\n\ndef update_customer_config(conn, customer_id, new_json):\n    \"\"\" Update the customer configuration JSON \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"UPDATE CustomerConfiguration SET ConfigurationJson = ? WHERE CustomerId = ?;\", (new_json, customer_id))\n            conn.commit()\n\ndef ripeye_config_editor_feature():\n    \"\"\" Main function to render the Streamlit app \"\"\"\n    st.title('Customer Configuration Editor')\n\n    # Database connection parameters\n    st.sidebar.title(\"Database Connection\")\n    #db = st.sidebar.selectbox('Select Database', ('smedbdev_current', 'Database2'))\n    #driver = st.sidebar.text_input('Driver', 'SQL Server')\n    server = st.sidebar.selectbox('Server', ('bcdbserverdev.database.windows.net', 'bcdbserver.database.windows.net'))\n    database = st.sidebar.selectbox('Select Database', ('smedbdev_current', 'smedbtest', 'smedb-v1.0'))\n    user = st.sidebar.text_input('Username', 'user')\n    password = st.sidebar.text_input('Password', 'password', type=\"password\")\n    conn = connect_to_database(server, database, user, password) #driver, \n\n    # Check if the connection was successful\n    if conn is not None:\n        try:\n            customers = get_customers(conn)\n            if customers:\n                customer_names = [name for _, name in customers]\n                selected_customer_name = st.selectbox('Select a Customer', customer_names)\n                selected_customer_id = [id for id, name in customers if name == selected_customer_name][0]\n                \n                # Fetch and edit JSON configuration\n                json_config = get_customer_config(conn, selected_customer_id)\n                edited_json = st.text_area(\"Edit JSON Configuration\", json_config, height=800)\n                if st.button('Save Changes'):\n                    update_customer_config(conn, selected_customer_id, edited_json)\n                    st.success(\"Configuration updated successfully!\")\n            else:\n                st.error(\"No customers found.\")\n        except Exception as e:\n            st.error(f\"An error occurred: {e}\")\n    else:\n        st.sidebar.error(\"Failed to establish a database connection.\")\n\nif __name__ == \"__main__\":\n    ripeye_config_editor_feature()\n",
        "file_name": "code_ripeye_config_editor_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\constants.py": {
        "summary": "The file `constants.py` defines several constants used in a program. These constants include:\n\n- `BOS` and `EOS`: Represent the beginning and end of a sequence, marked by \"<s>\" and \"</s>\", respectively.\n- `B_INST` and `E_INST`: Indicate the start and end of an instruction, marked by \"[INST]\" and \"[/INST]\".\n- `B_SYS` and `E_SYS`: Define the beginning and end of a system message, marked by \"<<SYS>>\\n\" and \"\\n<</SYS>>\\n\\n\".\n- `inst_msg`: A template string that includes a user prompt asking to extract contacts from an invoice and return them as a JSON object, with a placeholder for the invoice content.\n\nThese constants likely serve as markup or formatting elements for processing text or messages in the frontend of the Bennu tool.",
        "content": "BOS, EOS = \"<s>\", \"</s>\"\n\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\n\nB_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n\ninst_msg = \"\"\"User: Extract all contacts from the invoice and return them as a JSON object.\n{invoice}\n\"\"\"\n",
        "file_name": "constants.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\customer_feature.py": {
        "summary": "The file `customer_feature.py` is a Streamlit application that provides a user interface for managing customer records in a database. It includes the following key features:\n\n1. **Database Connection**: The application allows users to select a database and input their credentials (username and password) to establish a connection using `pyodbc`.\n\n2. **Customer Creation**: Users can input details for a new customer, such as name, token, limit, expiration date, email, and various other attributes. Upon clicking the \"Create Customer\" button, the information is inserted into the database.\n\n3. **Customer Listing**: Users can view a list of existing customers retrieved from the database. The list can be filtered using a regex search on customer names.\n\n4. **Customer Update and Delete**: There are placeholders for functionalities to update and delete customer records based on customer ID, though these features are not yet implemented.\n\n5. **Session State Management**: The application uses Streamlit's session state to manage API keys and identifiers across user interactions.\n\nOverall, the script serves as a basic interface for CRUD operations (Create, Read, Update, Delete) on customer data in a specified database.",
        "content": "import sys\nimport streamlit as st\nimport pyodbc\nimport pandas as pd\n\nfrom utils import get_api_identifier, get_api_key\n\ndef get_db_connection():\n    # Create three columns\n    col1, col2, col3 = st.columns(3)\n\n    # add a selectbox from where the user can select the database\n    database = col1.selectbox(\"Select the database\", [\"None\", \"bcsdbdev\", \"bcsdb-auth\"])\n\n    conn = None\n    # check if the user has selected a database and if so, connect to it\n    if database == \"None\":\n        return\n\n    username = col2.text_input(\"Username:\", value='None')\n    password = col3.text_input(\"Password:\", type=\"password\", value='None')\n\n    if username == 'None' or password == 'None':\n        return\n\n    if sys.platform == 'linux':\n        driver_name = 'ODBC Driver 17 for SQL Server'\n    elif sys.platform == 'win32':\n        driver_name = 'SQL Server'\n        \n    if database == \"bcsdbdev\":\n        conn = pyodbc.connect(f\"DRIVER={{{driver_name}}};SERVER=bcdbserverdev.database.windows.net;DATABASE=bcsdbdev;UID={username};PWD={password};\")\n    elif database == \"bcsdb-auth\":\n        conn = pyodbc.connect(f\"DRIVER={{{driver_name}}};SERVER=bcdbserver.database.windows.net;DATABASE=bcsdb-auth;UID={username};PWD={password};\")\n\n    return conn\n\n\ndef customer_feature():\n    st.title(\"Bludelta Customer\")\n    conn = get_db_connection()\n    if conn == None:\n        return\n\n    # Initialize the session state\n    # Session State also supports attribute based syntax\n    if 'apiKey' not in st.session_state:\n        st.session_state.apiKey= None\n\n    if 'apiIdentifier' not in st.session_state:\n        st.session_state.apiIdentifier = None\n\n    with st.expander(\"New Customer\"):\n\n        # Input fields for the customer details\n        name = st.text_input(\"Name\", \"\") or None  # Empty string will be converted to None\n        token = st.text_input(\"Token\", value=st.session_state.apiKey or get_api_key()) or None\n        token = None if token == None else token\n        limit = st.number_input(\"Limit\", value=5000, format=\"%d\")  # -1 represents None\n        expiration_date = st.date_input(\"Expiration Date\")\n        invoice_detail_types = st.number_input(\"Invoice Detail Types\", value=-2, format=\"%d\")  # -1 represents None\n        invoice_detail_types = None if invoice_detail_types == -1 else invoice_detail_types\n        email = st.text_input(\"Email\", \"\") or None  # Empty string will be converted to None\n        is_refused = st.selectbox(\"Is Refused\", options=[0, 1])  # None is an option\n        version = st.number_input(\"Version\", value=0, format=\"%d\")  # -1 represents None\n        api_identifier_key = st.text_input(\"API Identifier Key\", value=st.session_state.apiIdentifier or get_api_identifier()) or None\n        api_identifier_key = None if api_identifier_key == None else api_identifier_key\n        customer_id = st.number_input(\"Customer ID\", value=-1, format=\"%d\")  # -1 represents None\n        customer_id = None if customer_id == -1 else customer_id\n\n        # Update the session state\n        st.session_state.apiKey = token\n        st.session_state.apiIdentifier = api_identifier_key\n\n        # Button to insert the data into the database\n        if st.button(\"Create Customer\"):                    \n            # Create a cursor\n            cursor = conn.cursor()\n\n            # Prepare the INSERT statement\n            sql = \"\"\"\n            INSERT INTO [dbo].[Customer]\n                ([Name], [Token], [Limit], [ExpirationDate], [InvoiceDetailTypes], [Email], [IsRefused], [Version], [ApiIdentifierKey], [CustomerId])\n            VALUES\n                (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n            \"\"\"\n\n            # Execute the INSERT statement\n            cursor.execute(sql, name, token, limit, expiration_date.isoformat(), invoice_detail_types, email, is_refused, version, api_identifier_key, customer_id)\n\n            # Commit the transaction\n            conn.commit()\n\n            st.session_state.apiKey = None\n            st.session_state.apiIdentifier = None\n\n            st.success(\"Customer created successfully!\")\n\n\n    with st.expander(\"List Customers\"):\n        # Execute the SELECT statement\n        query = \"\"\"\n            SELECT [Id], [Name], [Token], [Limit], [ExpirationDate], [InvoiceDetailTypes], [Email], [IsRefused], [Version], [ApiIdentifierKey], [CustomerId]\n            FROM dbo.Customer\n        \"\"\"\n        df = pd.read_sql(query, conn)\n\n        # Search field\n        search_query = st.text_input(\"Search for a customer (using regex):\", \"\")\n\n        if search_query:\n            # Filter the DataFrame using the search query\n            df = df[df['Name'].str.contains(search_query, regex=True, na=False, case=False)]\n\n        # Display the results in a table\n        st.dataframe(df, use_container_width=True, height=900)\n\n    with st.expander(\"Update Customer\"):\n        # Input for customer ID\n        customer_id = st.number_input(\"Customer ID:\", value=0, format=\"%d\", key=\"customer_id_update\")\n\n        # # Inputs for each field in the SQL table\n        # name = st.text_input(\"Name:\")\n        # # ... add the rest of your fields here ...\n\n        # # Button to update the customer in the database\n        # if st.button(\"Update Customer\"):\n        #     cursor = conn.cursor()\n\n        #     # Execute the UPDATE statement\n        #     cursor.execute(\"\"\"\n        #         UPDATE dbo.Customer\n        #         SET Name = ?, ...\n        #         WHERE Id = ?\n        #     \"\"\", name, ..., customer_id)\n\n        #     # Commit the changes\n        #     conn.commit()\n\n        #     st.success(\"Customer updated successfully!\")\n        pass\n\n    with st.expander(\"Delete Customer\"):\n        # Input for customer ID\n        customer_id = st.number_input(\"Customer ID:\", value=0, format=\"%d\", key=\"customer_id_delete\")\n\n        # # Button to delete the customer from the database\n        # if st.button(\"Delete Customer\"):\n        #     cursor = conn.cursor()\n\n        #     # Execute the DELETE statement\n        #     cursor.execute(\"\"\"\n        #         DELETE FROM dbo.Customer\n        #         WHERE Id = ?\n        #     \"\"\", customer_id)\n\n        #     # Commit the changes\n        #     conn.commit()\n\n        #     st.success(\"Customer deleted successfully!\")\n        pass\n",
        "file_name": "customer_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\document_page_classification_feature.py": {
        "summary": "The file `document_page_classification_feature.py` is a Streamlit application designed for classifying and saving pages from PDF documents. It includes the following key functionalities:\n\n1. **PDF to Image Conversion**: The application converts PDF files to images using the `pdf2image` library.\n\n2. **Image Resizing**: Images are resized for display, specifically to a resolution of 300x361 pixels.\n\n3. **Thumbnail Display**: It displays thumbnails of the converted images in a grid layout, allowing users to categorize each page.\n\n4. **Category Management**: Users can add custom categories for classifying the pages. The default categories are preset but can be extended through a sidebar input.\n\n5. **File Management**: Users select a PDF file from a specified folder, which is checked for existence. The folder must be within a predefined mounted directory.\n\n6. **Batch Saving**: After classification, the application saves the categorized images into corresponding folders based on user-selected categories.\n\n7. **User Interface**: The app features a structured UI with tabs for classifying pages and previewing the document, enhancing user interaction.\n\nOverall, the script serves as a tool for efficiently managing and classifying PDF documents within a specified file structure.",
        "content": "from io import BytesIO\nimport os\nimport streamlit as st\nfrom streamlit_pdf_viewer import pdf_viewer\nfrom PIL import Image\nfrom pdf2image import convert_from_bytes\n\n#@st.cache_data(hash_funcs={Image.Image: id})\ndef convert_pdf_to_images(file_bytes):\n    return convert_from_bytes(file_bytes)\n\ndef resize_image(image: Image.Image, base_width=150) -> Image.Image:\n    # w_percent = (base_width / float(image.size[0]))\n    # h_size = int((float(image.size[1]) * float(w_percent)))\n    # return image.resize((base_width, h_size), Image.ANTIALIAS)\n    return image.resize((300, 361), Image.Resampling.LANCZOS)\n\ndef display_thumbnails(images, reset_categories):\n    cols = st.columns(3)  # Adjust the number of columns based on your layout preference\n    for index, image in enumerate(images):\n        with cols[index % 3]:\n            resized_image = resize_image(image)\n            st.image(resized_image, caption=f\"Page {index + 1}\")\n            category_key = f\"category_{index}\"\n\n            if category_key not in st.session_state or reset_categories:\n                st.session_state[category_key] = \"Default\"\n            options = st.session_state.categories            \n            selected_option = st.selectbox(\"Category\", options, index=options.index(st.session_state[category_key]), key=category_key)            \n\ndef batch_save_images_to_categories(folder_path, images):\n    for index, image in enumerate(images):\n        category_key = f\"category_{index}\"\n        if category_key in st.session_state and st.session_state[category_key]:\n            category = st.session_state[category_key]\n            category_path = os.path.join(folder_path, category)\n            os.makedirs(category_path, exist_ok=True)\n            selected_file = st.session_state.selected_file\n            image_path = os.path.join(category_path, f\"{selected_file.split('.')[0]}_page_{index + 1}.png\")\n            image.save(image_path)\n    st.success(\"All pages saved successfully to their categories.\")\n\ndef doc_page_classification_feature():\n    st.title(\"Split & Classify\")\n\n    if 'categories' not in st.session_state:\n        st.session_state.categories = [\"Invoice\", \"Agb\", \"Lieferschein\", \"Frachtbrief\", \"Ladeauftrag\", \"Wareneingangsbeleg\", \"Other\", \"Default\"]\n    \n    new_category = st.sidebar.text_input(\"Add a new category\")\n    if st.sidebar.button(\"Add Category\") and new_category:\n        if new_category not in st.session_state.categories:\n            st.session_state.categories.append(new_category)\n            st.sidebar.success(f\"Added category: {new_category}\")\n            \n\n    folder_name = st.sidebar.text_input(\"Enter the name of the root folder with the pdf documents. Note: The folder must be inside the \\\\\\\\nas-01\\\\traininsdata\\\\bennu\\\\image_classification folder because this folder is mounted into the docker container as /image_classification\")\n\n    # check if os is linux\n    if os.name == 'posix':\n        folder_path = os.path.join(\"/image_classification\", folder_name)\n    else:\n        folder_path = folder_name\n\n    if not os.path.exists(folder_path):\n        st.sidebar.error(f\"Folder {folder_path} does not exist\")\n        return\n\n    pdf_files = [file for file in os.listdir(folder_path) if file.lower().endswith(\".pdf\")]\n\n    with st.sidebar.expander(\"Documents\", expanded=True):\n        selected_file = st.radio(\"Select a PDF file\", pdf_files)\n\n    if selected_file:\n        file_path = os.path.join(folder_path, selected_file)\n        with open(file_path, \"rb\") as file:\n            file_bytes = file.read()\n\n        # show the images in one tab and the pdf file in another tab\n\n        tab1, tab2 = st.tabs([\"Classify Pages\", \"Document Preview\"])\n\n        with tab1:\n            old_selected_file = st.session_state.get(\"selected_file\")\n            reset_categories = old_selected_file != selected_file\n            st.session_state.selected_file = selected_file\n\n            images = convert_pdf_to_images(file_bytes)\n            display_thumbnails(images, reset_categories=reset_categories)\n\n            if st.button(\"Save Classified Pages\"):\n                batch_save_images_to_categories(folder_path, images)\n\n        with tab2:\n            pdf_viewer(file_bytes)\n\n    \n",
        "file_name": "document_page_classification_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\document_page_classification_prediction_feature.py": {
        "summary": "The file `document_page_classification_prediction_feature.py` is a Python script designed to facilitate document page classification using a web interface created with Streamlit. It allows users to upload PDF files, which are then converted into images for classification. \n\n### Key Components:\n1. **Model Configuration**: The `ModelConfig` class encapsulates the model's metadata, including its name, version, dimensions, and recognized category names.\n\n2. **Client Class**: The `DocPageClassifierClient` class is responsible for sending image data and model configurations to a server endpoint for classification. It constructs a multipart/form-data request and handles server responses.\n\n3. **Streamlit Interface**: The main function, `doc_classification_prediction_feature`, creates a user interface where users can:\n   - Input the server URL and endpoint.\n   - Upload a PDF file.\n   - View classified images of each page with their corresponding categories.\n\n4. **Image Processing**: The script utilizes the `pdf2image` library to convert PDF pages to PNG images, which are resized and sent to the server for classification.\n\n5. **Classification Results**: For each classified page, the script displays the image along with the classification result and score. If the classification fails, it displays an error message.\n\n6. **Example Usage**: The script includes a sample usage section that demonstrates how to classify a single image by directly sending it to the server.\n\nOverall, the script aims to provide an interactive tool for document classification, integrating backend server communication and frontend user interface functionalities.",
        "content": "import streamlit as st\nimport http.client\nimport json\nfrom PIL import Image\nfrom io import BytesIO\nfrom pdf2image import convert_from_bytes\n\nclass ModelConfig:\n    def __init__(self):\n        self.Name = \"\"\n        self.Version = \"\"\n        self.Height = 0\n        self.Width = 0\n        self.Names = []\n\nclass DocPageClassifierClient:\n    def __init__(self, url, endpoint):\n        self.url = url\n        self.endpoint = endpoint\n        self.boundary = '----WebKitFormBoundary7MA4YWxkTrZu0gW'\n        self.headers = {\n            'Content-Type': f'multipart/form-data; boundary={self.boundary}'\n        }\n\n    def post_image(self, image_bytes, model_config):\n        \"\"\"\n        Posts an image to the server for classification\n        :param image_bytes: The image bytes\n        :param model_config: The model configuration\n        :return: The server response\n        \"\"\"\n\n        # Convert the model configuration to JSON\n        json_content = json.dumps({\"ModelConfig\": model_config.__dict__})\n\n        # Prepare the multipart/form-data body\n        body = (\n            f'--{self.boundary}\\r\\n'\n            'Content-Disposition: form-data; name=\"json\"\\r\\n'\n            'Content-Type: application/json\\r\\n\\r\\n'\n            f'{json_content}\\r\\n'\n            f'--{self.boundary}\\r\\n'\n            'Content-Disposition: form-data; name=\"item0\"; filename=\"item0.png\"\\r\\n'\n            'Content-Type: image/png\\r\\n\\r\\n'\n        )\n        footer = f'\\r\\n--{self.boundary}--\\r\\n'\n\n        # Create a connection to the server\n        conn = http.client.HTTPConnection(self.url)\n        conn.request(\"POST\", self.endpoint, body=body.encode('utf-8') + image_bytes + footer.encode('utf-8'), headers=self.headers)\n\n        # Handle the server response\n        response = conn.getresponse()\n        if response.status != 200:\n            return None\n\n        return json.loads(response.read())\n\n# implement the feature here\ndef doc_classification_prediction_feature():\n    \"\"\"\n    This feature allows the user to classify document pages. The user can upload a PDF file and view the pages as images.\n    Moreover, each page is classified into one of the predefined categories. For the classfication the DocPageClassifierClient\n    is used to send the images to the server for classification.\n    At the sidebar the user can add new categories and set the url of the server.    \n    \"\"\"\n\n    st.title(\"Document Page Classification\")\n\n    server_url = st.sidebar.text_input(\"Enter the server URL. Only http is currently support\", value=\"52.236.159.89\")\n    if not server_url:\n        st.sidebar.error(\"Please enter the server URL\")\n        return\n    \n    endpoint = st.sidebar.text_input(\"Enter the endpoint\", value=\"/doc-page-classifier/v1/classification_result\")\n    if not endpoint:\n        st.sidebar.error(\"Please enter the endpoint\")\n        return\n\n    uploaded_file = st.file_uploader(\"Upload a PDF file\", type=[\"pdf\"])\n    \n    if uploaded_file:\n        images = convert_from_bytes(uploaded_file.read(), dpi=96, fmt=\"png\")\n        poster = DocPageClassifierClient(server_url, endpoint=endpoint)\n\n        model_config = ModelConfig()\n        model_config.Name = \"junk_detection\"\n        model_config.Version = \"1\"\n        model_config.Height = 224\n        model_config.Width = 224\n        model_config.Names = [\"CMRConsignmentNote\", \"TermsAndConditions\", \"Invoice\"]\n\n        cols_per_row = 3  # You can adjust the number of columns based on your preference and screen size\n        cols = st.columns(cols_per_row)\n        \n        for index, image in enumerate(images):\n            resized_image = image.resize((224, 224))\n            image_byte_array = BytesIO()\n            image.save(image_byte_array, format=\"PNG\")\n            image_bytes = image_byte_array.getvalue()\n            response = poster.post_image(image_bytes, model_config)\n            \n            # Determine which column to place this image in\n            col = cols[index % cols_per_row]\n            with col:\n                if response:\n                    page_type = response['ResponseItems'][0]['PageType']\n                    col.image(resized_image, caption=f\"Page {index + 1}, Class: {page_type}\")\n                    col.success(f\"Classified as: {page_type}, Score: {response['ResponseItems'][0]['Score']:.2f}\")\n                else:\n                    col.error(f\"Failed to classify page {index + 1}\")\n# Example usage:\nif __name__ == \"__main__\":\n    poster = DocPageClassifierClient('52.236.159.89', '/doc-page-classifier/v1/classification_result')\n\n    # Load the image\n    image_path = r\"H:\\mlnet_prosa_detector\\Frachtbrief\\0004AC1E5729ABD5A08D22BE20002C38_page_10.png\"\n    image = Image.open(image_path)\n    image = image.resize((224, 224))\n\n    # Convert image to bytes\n    image_byte_array = BytesIO()\n    image.save(image_byte_array, format=\"PNG\")\n    image_bytes = image_byte_array.getvalue()\n    \n    model_config = ModelConfig()\n    model_config.Name = \"junk_detection\"\n    model_config.Version = \"1\"\n    model_config.Height = 224\n    model_config.Width = 224\n    model_config.Names = [\"CMRConsignmentNote\", \"TermsAndConditions\", \"Invoice\"]  # Example list of class names\n\n    response = poster.post_image(image_bytes, model_config)\n    print(response)\n",
        "file_name": "document_page_classification_prediction_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\document_splitting_bm_analysis.py": {
        "summary": "The file `document_splitting_bm_analysis.py` is a Streamlit application designed for analyzing benchmark results of models used in a document-splitting application. Users can upload TSV files containing benchmark data, which are then processed to calculate various metrics such as accuracy, precision, recall, and F1 score. The app employs two different language models (OpenAI and Azure) to assist users in generating reports and answering queries about the benchmark data.\n\nKey components of the script include:\n\n1. **Imports**: Essential libraries like Streamlit, pandas, NumPy, seaborn, and scikit-learn are imported for data manipulation, visualization, and metrics calculations.\n\n2. **Prompts for LLM**: Predefined prompts are set up to generate management reports, identify top models based on accuracy, and create visualizations.\n\n3. **LLM Query Functions**: Functions for querying OpenAI and Azure GPT models are implemented to fetch responses based on user input and context data.\n\n4. **User Interface**: A chat interface allows users to interact with the benchmarks, ask questions, and receive insights based on the uploaded data.\n\n5. **Metrics Calculation**: Functions are included to calculate metrics from the benchmark data, including confusion matrices and error rates.\n\n6. **File Upload and Data Processing**: Users can upload benchmark results, which are processed to generate metrics and visualizations, displayed in the app.\n\n7. **Visualization**: The app includes a bar chart comparing the accuracy of different models and can display confusion matrices.\n\nOverall, this script provides a user-friendly interface for analyzing and interpreting benchmark results from document-splitting models, aiming to minimize manual work in document processing tasks.",
        "content": "import streamlit as st\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport openai\nimport re\nimport traceback\nimport os\nimport requests\n\n# Import necessary modules for metrics calculation\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\n\nsystem_prompt = (\n    \"You are an assistant for benchmark analysis. \"\n    \"You have access to a pandas DataFrame called 'metrics_df'. \"\n    \"You can use pandas as 'pd', numpy as 'np', matplotlib.pyplot as 'plt', seaborn as 'sns', and streamlit as 'st'. \"\n    \"DO NOT include import statements in your code. \"\n    \"DO NOT use 'print' statements; instead, use 'st.write()' to display outputs. \"\n    \"**When creating plots, use 'st.pyplot(fig)' instead of 'plt.show()'.** \"\n    \"Provide Python code for any data manipulations or visualizations, enclosed within triple backticks (```python ... ```).\"\n)\n\nsummary_prompt=\"\"\"\nGenerate a management report. Explain in detail why we shall use model x (take only top 3 models into account) for our document splitting app where customers can upload a pdf files which may contain multipe independent documents. Models predicts for every page whether it is the start page of a document or not. One import requirement is that manual work shall be reduced to a minimum, i.e. splitting document manually.\nAdditional information. This is our current model which is used in production:\n\n| Model                                   | Accuracy | Precision  | Recall  | F1 Score  | False Positives (FP) | False Negatives (FN) | Sprint |\n|-----------------------------------------|----------|------------|---------|-----------|----------------------|----------------------|--------|\n| **Prod-Model (t=0.75)**                 | 0.8145   | 0.9237     | 0.5112  | 0.6581    | 51                   | 590                  |        |\n\"\"\"\n\ntop_5_models = \"\"\"\nGive me the top 5 models based on the accuracy. Proved the results in a table.\n\"\"\"\n\n\nuser_prompts = {\n    \"SummaryReport\" : summary_prompt,\n    \"Top5Models\" : top_5_models,\n    \"PlotAccuracy\" : \"Plot the accuracy of the models in a bar chart.\"\n}\n\n\n# Function to query GPT model\ndef ask_llm(question, context):\n    openai.api_key = st.secrets[\"OPENAI_API_KEY\"]  # Use Streamlit secrets for API keys\n\n    # System prompt to guide the LLM\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",\n        messages=[\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"Data:\\n{context}\\n\\nQuestion:\\n{question}\"}\n        ],\n        max_tokens=4096,\n        temperature=0  # Lower temperature for more predictable outputs\n    )\n\n    return response['choices'][0]['message']['content']\n\n\n# Function to query Azure OpenAI model\ndef ask_azure_gpt(question, context):\n    # Replace with your Azure OpenAI API details\n    api_base = st.secrets[\"AZURE_OPENAI_API_BASE\"]  # The base URL of your Azure OpenAI resource\n    api_key = st.secrets[\"AZURE_OPENAI_API_KEY\"]    # The API key for your Azure OpenAI resource\n    deployment_id = st.secrets[\"AZURE_OPENAI_DEPLOYMENT_ID\"]  # The deployment name for your Azure OpenAI model\n    api_version = \"2024-02-15-preview\"  # API version to use for Azure OpenAI (adjust as needed)\n\n    # Azure OpenAI endpoint for completions\n    endpoint = f\"https://{api_base}.openai.azure.com/openai/deployments/{deployment_id}/chat/completions?api-version={api_version}\"\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"api-key\": api_key\n    }\n\n    # Construct the payload for the API call\n    payload = {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": f\"Data:\\n{context}\\n\\nQuestion:\\n{question}\"}\n        ],\n        \"max_tokens\": 4096,\n        \"temperature\": 0\n    }\n\n    # Send the request to Azure OpenAI\n    try:\n        response = requests.post(endpoint, headers=headers, json=payload)\n        response.raise_for_status()\n        completion = response.json()\n        return completion['choices'][0]['message']['content']\n    except Exception as e:\n        st.error(f\"Error querying Azure OpenAI: {e}\")\n        return None\n\n# Function to handle the chat interface\ndef chat_interface():\n    st.subheader(\"Chat with your Benchmarks\")\n\n    # Initialize conversation history\n    if 'conversation' not in st.session_state:\n        st.session_state.conversation = []\n\n    # User input\n    col_input, col_predefine_prompt = st.columns([3, 2])\n\n    with col_input:\n        user_input = st.text_area(\"Ask the LLM about your benchmark results:\")\n    # with col_predefine_prompt:\n    #     prompt = st.selectbox(\"Choose a predefined prompt:\", list(user_prompts.keys()))        \n    #     user_input = user_prompts[prompt]\n    #     # also show it in the text area\n    #     st.text_area(\"Ask the LLM about your benchmark results:\", value=user_input)\n\n\n    if user_input and 'metrics_df' in st.session_state:\n        # Check if the same input has already been processed\n        if 'last_user_input' not in st.session_state or user_input != st.session_state.last_user_input:\n            st.session_state.last_user_input = user_input\n\n            # Convert DataFrame to CSV for context\n            context = st.session_state.metrics_df.to_csv(index=False)\n\n            # Get response from LLM\n            #response = ask_llm(user_input, context)\n            response = ask_azure_gpt(user_input, context)\n\n            # Save conversation\n            st.session_state.conversation.append((user_input, response))\n\n            # Display LLM response\n            #st.markdown(f\"**LLM:** {response}\")\n\n            # Process LLM response\n            process_llm_response(response)\n    elif user_input:\n        st.warning(\"Please upload and process benchmark data before asking questions.\")\n\n# Function to process the LLM response\ndef process_llm_response(response):\n    # Display the response. Remove the code blocks first\n    response_ = re.sub(r\"```python(.*?)```\", \"\", response, flags=re.DOTALL)\n    st.markdown(response_)\n\n    # Extract code blocks from the response\n    code_blocks = re.findall(r\"```python(.*?)```\", response, re.DOTALL)\n\n    if code_blocks:\n        for code in code_blocks:\n            col1, col2 = st.columns([3,2])\n            \n            with col2:\n                st.markdown(\"**Generated Code:**\")\n                st.code(code, language='python')\n            \n            with col1:\n                st.markdown(\"**Executing generated code:**\")\n                # Execute the code safely\n                execute_generated_code(code)\n    else:\n        st.markdown(\"**LLM did not provide any code to execute.**\")\n\n# Function to execute the generated code safely\ndef execute_generated_code(code):\n    # Preprocess code: remove import statements\n    code = re.sub(r'^\\s*import .+$', '', code, flags=re.MULTILINE)\n    # Replace print statements with st.write\n    code = code.replace('print(', 'st.write(')\n    # Replace plt.show() with st.pyplot()\n    code = code.replace('plt.show()', 'st.pyplot(plt.gcf())')\n\n    # Replace str.replace without regex parameter to include regex=False\n    code = re.sub(r\"str\\.replace\\(([^,]+), ([^)]+)\\)\", r\"str.replace(\\1, \\2, regex=False)\", code)\n\n    # Define safe built-in functions\n    safe_builtins = {\n        'len': len,\n        'range': range,\n        'min': min,\n        'max': max,\n        'sum': sum,\n        'abs': abs,\n    }\n\n    # Safe execution environment\n    safe_globals = {\n        \"__builtins__\": safe_builtins,\n        \"pd\": pd,\n        \"np\": np,\n        \"plt\": plt,\n        \"sns\": sns,\n        \"st\": st,\n    }\n\n    safe_locals = {\n        \"metrics_df\": st.session_state.metrics_df.copy()\n    }\n\n    try:\n        exec(code, safe_globals, safe_locals)\n    except Exception as e:\n        st.error(\"An error occurred while executing the code.\")\n        st.error(traceback.format_exc())\n\n# Function to calculate metrics from benchmark data\ndef calc_metrics(df):\n    y_true = df['label'].apply(lambda x: 1 if x.lower() == 'first' else 0)\n    y_pred = df['prediction'].apply(lambda x: 1 if x.lower() == 'first' else 0)\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred, zero_division=0)\n    recall = recall_score(y_true, y_pred, zero_division=0)\n    f1 = f1_score(y_true, y_pred, zero_division=0)\n    tn, fp, fn, tp = conf_matrix.ravel()\n\n    # Exclude 'confusion_matrix' from the returned dictionary\n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'tp': tp,\n        'fp': fp,\n        'fn': fn,\n        'tn': tn,\n    }, conf_matrix  # Return confusion matrix separately\n\ndef calc_error_rate(df):\n    try:\n        LLM_SCORE = 0.99\n\n        # get the errors, which is shown in the 'correct' column. A value of 0 means the model was wrong\n        errors = df[df['correct'] == 0]\n\n        errors_llm = errors[errors['score'] == LLM_SCORE]     \n        errors_not_llm = errors[errors['score'] != LLM_SCORE]\n\n        # calculate the the accuracy for each group. get all results where the score is 0.99 and calculate the accuracy\n        # for this group\n        predictions_llm = df[df['score'] == 0.99]\n        accuracy_llm = 1 - (len(errors_llm)/ len(predictions_llm))\n        \n        # calculate the accuracy for the other group\n        predictions_not_llm = df[df['score'] != 0.99]\n        accuracy_not_llm = 1 - (len(errors_not_llm) / len(predictions_not_llm))\n\n        return accuracy_llm, accuracy_not_llm\n    except Exception as e:\n        #st.error(f\"Error calculating error rate: {e}\")\n        return None, None\n\n# Main function\ndef benchmark_comparison_feature():\n    st.title(\"LLM-Powered Benchmark Analysis Tool\")\n\n    # Upload benchmark data\n    uploaded_files = st.file_uploader(\"Upload Benchmark Result Files (TSV)\", type=\"tsv\", accept_multiple_files=True)\n\n    if uploaded_files:\n        all_metrics = []\n        conf_matrices = {}  # Dictionary to store confusion matrices\n\n        for uploaded_file in uploaded_files:\n            df = pd.read_csv(uploaded_file, sep='\\t')\n            accuracy_llm, accuracy_not_llm = calc_error_rate(df)\n            metrics, conf_matrix = calc_metrics(df)\n            model_name = os.path.basename(uploaded_file.name)\n            metrics['model'] = model_name\n            metrics['accuracy_llm'] = accuracy_llm\n            metrics['accuracy_not_llm'] = accuracy_not_llm\n\n            # Store confusion matrix separately\n            conf_matrices[model_name] = conf_matrix\n\n            all_metrics.append(metrics)\n\n        # Create a DataFrame from all metrics\n        metrics_df = pd.DataFrame(all_metrics)\n        st.session_state.metrics_df = metrics_df\n\n        st.success(\"Files uploaded and processed successfully!\")\n\n        # add two streamlit columns one the datafame and another one for a plot\n        col1, col2 = st.columns([3, 2])\n\n        with col1:\n            # Display the DataFrame\n            st.subheader(\"Metrics Data\")\n            st.dataframe(metrics_df)\n\n        with col2:\n            # Display a bar plot of accuracy for each model\n            st.subheader(\"Accuracy Comparison\")\n            fig, ax = plt.subplots()\n            sns.barplot(x='model', y='accuracy', data=metrics_df, palette='viridis', ax=ax)\n            ax.set_title(\"Accuracy Comparison\")\n            ax.set_ylabel(\"Accuracy\")\n            ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n            st.pyplot(fig)\n\n        # Optionally, display confusion matrices\n        # st.subheader(\"Confusion Matrices\")\n        # for model_name, conf_matrix in conf_matrices.items():\n        #     st.write(f\"Confusion Matrix for {model_name}:\")\n        #     fig, ax = plt.subplots()\n        #     sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n        #     ax.set_title(f\"Confusion Matrix for {model_name}\")\n        #     ax.set_xlabel('Predicted')\n        #     ax.set_ylabel('Actual')\n        #     st.pyplot(fig)\n\n        # Launch chat interface\n        chat_interface()\n    else:\n        st.info(\"Please upload benchmark result files to get started.\")\n\n",
        "file_name": "document_splitting_bm_analysis.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\document_splitting_feature.py": {
        "summary": "The file `document_splitting_feature.py` is a Streamlit-based web application designed to facilitate document splitting workflows. It allows users to upload PDF documents and JSON files containing ground truth data, and it communicates with a specified API for processing.\n\nKey components of the script include:\n\n1. **Sidebar Creation**: The function `create_sidebar()` enables users to select a predefined API URL or edit it directly, and to set a timeout for requests.\n\n2. **Header Configuration**: The function `add_customer_name_role_to_header()` is used to append customer-specific information to request headers.\n\n3. **Metric Calculation**: The function `calc_metrics()` computes various performance metrics (accuracy, precision, recall, F1 score) based on true and predicted labels using scikit-learn.\n\n4. **Main Functionality**: The `document_splitting_feature()` function orchestrates the application by:\n   - Displaying the title and sidebar.\n   - Allowing users to upload a PDF and a JSON ground truth file.\n   - Validating the presence of an API key from environment variables.\n   - Making a POST request to the specified API with the uploaded file(s) and headers.\n   - Displaying the API response and metrics analysis in separate tabs.\n\nOverall, the script provides a user-friendly interface to interact with a document splitting service, making it easier to evaluate the performance of document classification models against ground truth data.",
        "content": "import os\nimport json\nimport streamlit as st\nimport httpx\nfrom PIL import Image, ImageDraw\nimport io\nfrom pdf2image import convert_from_bytes\nimport base64\nimport requests\nfrom requests_toolbelt.multipart.encoder import MultipartEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\n\n\n\ndef create_sidebar():\n    with st.sidebar:\n        st.title('Document Splitting Workflows')\n\n        # List of predefined URLs to choose from\n        predefined_urls = [\n            \"http://localhost:8080/llm-doc-splitter\"\n        ]\n\n        # Display a text input field for the URL\n        selected_url_index = st.selectbox(\"Select URL\", [(url, idx) for idx, url in enumerate(predefined_urls)], format_func=lambda x: x[0])\n\n        # Retrieve the selected URL\n        selected_url = selected_url_index[0]\n\n        # Allow users to edit the URL directly\n        edited_url = st.text_input(\"Edit URL (Final URL)\", selected_url)\n\n        # If the edited URL is different from the selected URL, update it\n        if edited_url != selected_url:\n            selected_url = edited_url\n\n        timeout = st.text_input(\"Enter timeout [sec]\", \"120\")\n        # convert to int\n        try:\n            timeout = int(timeout)\n        except ValueError:\n            st.error(\"Timeout must be an integer\")\n            return\n\n        return selected_url, timeout\n\ndef add_customer_name_role_to_header(header, customer_name, customer_role):\n    \"\"\"\n    Add the CustomerName and CustomerRole to the header.\n    @param header: The header dictionary to which the CustomerName and CustomerRole will be added.\n    @param customer_name: The customer name to be added.\n    @param customer_role: The customer role to be added.\n    \"\"\"\n    if customer_name:\n        header[\"X-CustomerName\"] = customer_name\n    if customer_role:\n        header[\"X-Role\"] = customer_role\n    return header\n\ndef calc_metrics(y_true, y_pred):\n\n    conf_matrix = confusion_matrix(y_true, y_pred)\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    tn, fp, fn, tp = conf_matrix.ravel()\n    \n    #return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'confusion_matrix': conf_matrix, 'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn}\n    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1, 'tp': tp, 'fp': fp, 'fn': fn, 'tn': tn}\n\n\ndef document_splitting_feature():\n    st.title(\"Document Splitting Workflows\")\n\n    url, timeout = create_sidebar()\n    st.write(\"URL:\", url)\n    st.write(\"Timeout:\", timeout)\n\n    uploaded_file = st.file_uploader(\"Choose a document (pdf)\", type=[\"pdf\"])\n    ground_truth_file = st.file_uploader(\"Choose a ground truth file (json)\", type=[\"json\"])\n\n    # read APIKey from env variable, check if it exists\n    if 'APIKEY' not in os.environ:\n        st.error(\"APIKEY not found in environment variables\")\n        return\n    apiKey = os.environ['APIKEY']\n\n\n    # Define the URL and headers        \n    url = url\n    headers = {\n        \"X-ApiKey\": apiKey,\n        \"User-Agent\": \"Bludelta Workflow Client\"\n    }\n\n    if uploaded_file is None:\n        return\n\n    #headers = add_customer_name_role_to_header(headers, customer_name, customer_role)\n\n    payload = None\n    # Make the POST request\n    if payload is not None:\n        m = MultipartEncoder(\n            fields={\n                'json': ('json_data', payload, 'application/json'),  # JSON part with explicit content type\n                'files': ('file', uploaded_file.getvalue(), 'application/octet-stream')  # File part\n            }\n        )\n    else:   \n        m = MultipartEncoder(\n            fields={\n                'files': ('file', uploaded_file.getvalue(), 'application/octet-stream')  # File part\n            }\n        )\n\n    headers['Content-Type'] = m.content_type  # Setting the content type to multipart/form-data\n\n    response = requests.post(f\"{url}\", data=m, headers=headers)        \n\n    data = response.json()\n\n    tab1, tab2 = st.tabs([\"Splitting Result\", \"Analysis Result\"])\n\n    with tab1:\n        st.json(data)\n\n    with tab2:\n        if ground_truth_file is not None:\n            ground_truth = json.load(ground_truth_file)\n\n            # convert the ground truth to a pandas dataframe\n            df_ground_truth = pd.DataFrame(ground_truth)\n            y_true = df_ground_truth['Class'].map({'First': 1, 'Other': 0}).tolist()\n            df_prediction = pd.DataFrame(data)\n            y_pred = df_prediction['Class'].map({'First': 1, 'Other': 0}).tolist()\n\n            metrics = calc_metrics(y_true, y_pred)\n\n            metrics_df = pd.DataFrame([metrics])\n            # write as markdown\n            st.write(metrics_df.to_markdown(index=False))\n\n\n\n ",
        "file_name": "document_splitting_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\dynamic_config_feature.py": {
        "summary": "The file `dynamic_config_feature.py` is a Python script designed to manage a dynamic configuration file using a Git repository. It utilizes several libraries, including `os`, `shutil`, `json`, `requests`, and `streamlit`, to provide a web interface for interacting with the repository. \n\nKey functionalities include:\n\n1. **Repository Management**: The script can clone a Git repository or update an existing local copy by pulling the latest changes.\n\n2. **Configuration Loading**: It loads a JSON configuration file (`dynamic_config.json`) from a specified directory within the repository.\n\n3. **Configuration Editing**: Users can view and edit the configuration in a text area, with changes saved back to the file.\n\n4. **Pushing Changes**: Users can commit changes with a message and push them to the remote repository.\n\n5. **Undoing Changes**: There is an option to undo the last commit made to the repository.\n\n6. **Tagging**: Users can create a tag in the repository and push it to the remote.\n\nThe script provides a user-friendly interface through Streamlit for all these operations, enhancing the management of dynamic configurations in a collaborative environment.",
        "content": "import os\nimport shutil\nimport json\nimport requests\nfrom git import Repo, GitCommandError\nimport streamlit as st\n\ndef get_repo(repo_url, repo_dir):\n    if os.path.exists(repo_dir):\n        repo = Repo(repo_dir)\n        try:\n            origin = repo.remotes.origin\n            origin.pull()\n        except Exception as e:\n            st.error(f\"Error updating the local repo: {e}\")\n    else:\n        try:\n            git_username = os.environ['GIT_USERNAME']\n            git_password = os.environ['GIT_PASSWORD']\n            git_credentials_url = repo_url.replace(\"https://\", f\"https://{git_username}:{git_password}@\")\n            print(repo_url)\n            repo = Repo.clone_from(git_credentials_url, repo_dir)\n        except Exception as e:\n            st.error(f\"Error cloning the repo: {e}\")\n    return repo\n\n\ndef load_config(repo_dir):\n    # Load the dynamic_config.json file\n    with open(os.path.join(repo_dir, \"CaptureSdk\", \"dynamic_config.json\"), \"r\", encoding=\"utf-8\") as f:\n        config = json.load(f)\n    return config\n\ndef push_config(repo, commit_comment):\n    repo.git.add(\"CaptureSdk/dynamic_config.json\")\n    repo.git.commit(\"-m\", f\"{commit_comment}\")\n    repo.remotes.origin.push()\n    st.success(\"Config saved and pushed successfully!\")\n\ndef undo_last_checkin(repo):\n    repo.git.reset(\"--hard\", \"HEAD~1\")\n    repo.remotes.origin.push(force=True)\n    st.success(\"Last checkin undone successfully!\")\n\ndef tag_repo(repo, tag_name):\n    repo.create_tag(tag_name)\n    repo.remotes.origin.push(\"--tags\")\n    st.success(\"Repo tagged successfully!\")\n\ndef dynamic_config_feature():\n    st.title(\"Bludelta Dynamic Config\")\n\n    # Clone the dynamic_config repo\n    repo_dir = \"dynamic_config\"\n    repo_url = \"https://blumatix.visualstudio.com/DefaultCollection/Rechnungserkennung/_git/DynamicConfig\"\n    repo = get_repo(repo_url, repo_dir)\n\n    # get the latest tag\n    latest_tag = repo.git.describe(\"--tags\", \"--abbrev=0\")\n\n    # load and display the config\n    config = load_config(repo_dir)\n    col1, col2 = st.columns([1,1])\n    config_str = col1.text_area(\"Dynamic Config\", json.dumps(config, indent=4, ensure_ascii=False), height=900)\n    col2.code(config_str, language='json')\n\n    # Save the config to the file\n    if col1.button(\"Save dynamic_config.json\", key=\"SaveDynamic\"):\n        config = json.loads(config_str)    \n        with open(os.path.join(repo_dir, \"CaptureSdk\", \"dynamic_config.json\"), \"w\", encoding=\"utf-8\") as f:\n            json.dump(config, f, indent=4, ensure_ascii=False)\n    \n    col1.markdown(\"---\")\n\n    # Push config. Align the button and text input in a single row\n    commit_comment = col1.text_input(\"Comment\", \"Write a commit comment e.g. Update dynamic_config.json\")\n    if col1.button(\"Push Config\", use_container_width=True):\n        push_config(repo, commit_comment)\n\n    # Add a button which can be used to remove the latest checkin\n    if col1.button(\"Undo Last Checkin\", key=\"UndoLastCheckin\"):\n        undo_last_checkin(repo)\n\n    col1.markdown(\"---\")\n\n    # Button to tag the repo and push the tag\n    if col1.button(\"Tag Repo\"):\n        tag_name = st.text_input(\"Tag name\", \"\")\n        if tag_name:\n            tag_repo(repo, tag_name)\n            # on success delete local repo\n            if os.path.exists(repo_dir):\n                shutil.rmtree(repo_dir)\n",
        "file_name": "dynamic_config_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\http_client_feature.py": {
        "summary": "The file `http_client_feature.py` implements a Streamlit application that functions as a Postman-like API client. The application allows users to send HTTP requests with various configurations, including different HTTP methods (GET, POST, PUT, DELETE, PATCH), URL input, and customizable parameters, headers, and body content.\n\nKey features of the application include:\n\n1. **User Interface**: The app is designed with a user-friendly interface using Streamlit, featuring columns for method selection, URL input, and tabs for organizing input fields.\n\n2. **Input Fields**:\n   - **Parameters**: Users can define key-value pairs for request parameters.\n   - **Headers**: Users can specify custom headers for the request.\n   - **Body**: The body can be specified in multiple formats: form-data, raw JSON, or file uploads.\n\n3. **Multipart Handling**: It uses `requests_toolbelt` to handle multipart form-data uploads efficiently.\n\n4. **Sending Requests**: A \"Send\" button triggers the request, and the app handles different methods based on user input, displaying the response including status code, headers, and body.\n\n5. **Error Handling**: The application includes error handling to provide feedback if something goes wrong during the request process.\n\nOverall, this script provides a comprehensive tool for testing and interacting with APIs in a manner similar to Postman, while being directly integrated into a Python web app using Streamlit.",
        "content": "import streamlit as st\nimport requests\nimport json\nimport requests_toolbelt\nfrom streamlit_ace import st_ace\n\ndef http_client_feature():\n    st.title(\"Postman-like API Client\")\n\n    # Use form to group URL input, method dropdown, and send button for better alignment\n    col1, col2, col3 = st.columns([1, 4, 1])\n\n    # HTTP method selector\n    with col1:\n        method = st.selectbox(\"Method\", [\"GET\", \"POST\", \"PUT\", \"DELETE\", \"PATCH\"])\n\n    # URL input field\n    with col2:\n        url = st.text_input(\"Enter the URL\", \"https://\")\n\n    # Tabs for params, authorization, headers, body, etc.\n    tabs = st.tabs([\"Params\", \"Authorization\", \"Headers\", \"Body\", \"Response scripting\"])\n\n    # Params Tab\n    with tabs[0]:\n        st.subheader(\"Params\")\n        params_data = {}\n        num_params = st.number_input(\"Number of Params\", min_value=0, step=1)\n        for i in range(num_params):\n            param_key = st.text_input(f\"Param {i+1} Key\", key=f\"param_key_{i}\")\n            param_value = st.text_input(f\"Param {i+1} Value\", key=f\"param_value_{i}\")\n            params_data[param_key] = param_value\n\n    # Headers Tab (aligned horizontally)\n    with tabs[2]:\n        st.subheader(\"Headers\")\n        headers_data = {}\n        num_headers = st.number_input(\"Number of Headers\", min_value=0, step=1)\n        \n        for i in range(num_headers):\n            col1, col2 = st.columns([1, 3])\n            with col1:\n                header_key = st.text_input(f\"Header {i+1} Key\", key=f\"header_key_{i}\")\n            with col2:\n                header_value = st.text_input(f\"Header {i+1} Value\", key=f\"header_value_{i}\")\n            headers_data[header_key] = header_value\n\n    # Body Tab with multipart/form-data handling\n    with tabs[3]:\n        st.subheader(\"Body\")\n        body_type = st.radio(\"Body Type\", [\"none\", \"form-data\", \"raw\", \"file\"], horizontal=True)\n\n        form_data = {}\n        uploaded_files = []\n\n        if body_type == \"form-data\":\n            num_fields = st.number_input(\"Number of Fields\", min_value=1, step=1)\n\n            # Create columns for form-data fields\n            for i in range(num_fields):\n                col1, col2, col3 = st.columns([1, 2, 3])\n                with col1:\n                    field_key = st.text_input(f\"Field {i+1} Key\", key=f\"form_key_{i}\")\n                with col2:\n                    field_type = st.selectbox(f\"Field {i+1} Type\", [\"Text\", \"File\"], key=f\"field_type_{i}\")\n                with col3:\n                    if field_type == \"Text\":\n                        # use a text area instead of text input to allow for multi-line values\n                        field_value = st.text_area(f\"Field {i+1} Value\", key=f\"form_value_{i}\", height=100)\n\n                        form_data[field_key] = field_value\n                    elif field_type == \"File\":\n                        uploaded_file = st.file_uploader(f\"Upload File for {field_key}\", key=f\"form_file_{i}\")\n                        if uploaded_file:\n                            uploaded_files.append((field_key, uploaded_file))\n\n        elif body_type == \"raw\":\n            raw_data = st.text_area(\"Raw JSON Body\", \"{}\")\n            form_data['json'] = json.dumps(raw_data)\n\n        elif body_type == \"file\":\n            uploaded_files = st.file_uploader(\"Upload Files\", accept_multiple_files=True)\n            for uploaded_file in uploaded_files:\n                form_data[uploaded_file.name] = uploaded_file\n\n    # Scripting Tab with Ace Editor\n    # with tabs[4]:\n    #     st.subheader(\"Response Scripting\")\n    #     test_script = st_ace(\n    #         language='python',\n    #         theme='chrome',\n    #         value=\"\"\"# Example test cases\n    # # response.status_code == 200\n    # assert response.status_code == 200, 'Expected status code 200, but got ' + str(response.status_code)\n    # \"\"\",\n    #         height=200,\n    #         key=\"test_script_editor\"\n    #     )\n\n    # Send button\n    submit_button = st.button(\"Send\", use_container_width=True)\n    if submit_button:\n        try:\n            # Prepare multipart form-data using requests_toolbelt.MultipartEncoder\n            if body_type == \"form-data\" or body_type == \"file\":\n                fields = {**form_data}\n\n                if 'json' in fields:\n                    fields['json'] = ('json_data', fields['json'], 'application/json')\n\n                for file_key, file in uploaded_files:\n                    fields[file_key] = (file.name, file, 'application/octet-stream')\n\n                m = requests_toolbelt.MultipartEncoder(fields=fields)\n                headers_data['Content-Type'] = m.content_type\n\n                response = requests.post(url, headers=headers_data, data=m)\n\n            elif body_type == \"raw\":\n                response = requests.post(url, headers=headers_data, json=json.loads(raw_data))\n\n            else:  # Handle GET or methods without a body\n                response = requests.get(url, params=params_data, headers=headers_data)\n\n            # Display response\n            st.subheader(\"Response\")\n\n            st.write(f\"Status Code: {response.status_code}\")\n            st.write(\"Headers:\", response.headers)\n            st.write(\"Body:\")\n            st.json(response.json())\n\n            # # Executing the script in the Scripting tab\n            # st.subheader(\"Script Execution\")\n            # try:\n            #     # Execute the script (using the response object in the script context)\n            #     exec(test_script, {'response': response})\n            #     st.success(\"Script executed successfully!\")\n            # except AssertionError as e:\n            #     st.error(f\"Assertion Error: {e}\")\n            # except Exception as e:\n            #     st.error(f\"Error executing script: {e}\")\n\n        except Exception as e:\n            st.error(f\"An error occurred: {e}\")        \n\n\n",
        "file_name": "http_client_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\json_plugin_feature.py": {
        "summary": "The provided Python script defines a Streamlit application for editing a JSON configuration for a plugin named \"DachserPurchaseOrderIdSmartFilters.\" \n\n### Key Features:\n- **Title and JSON Loading**: The application starts with a title and loads a hardcoded JSON configuration containing details such as version, plugin name, customer names, and patterns for regex matching.\n  \n- **User Input Fields**: The app creates three columns where users can input or modify various fields:\n  - **Column 1**: Version, Plugin Name, Customer Name.\n  - **Column 2**: Invoice Detail Type Name, Value to Set, Return Single Result (with a dropdown).\n  - **Column 3**: Overwrite Existing Prediction (with a dropdown).\n\n- **Multi-select Options**: Users can select API keys and customer names from multi-select inputs.\n\n- **Pattern Editing**: The application allows users to edit multiple regex patterns, including options for matching behavior and regex string input.\n\n- **Pattern Groups**: Similar to patterns, users can modify options for pattern groups.\n\n- **Save Functionality**: A button is provided to save the modified configuration back to a JSON file named \"updated_config.json.\"\n\nOverall, this script is designed to facilitate the configuration and management of a specific plugin through a user-friendly web interface.",
        "content": "import streamlit as st\nimport json\n\ndef json_plugin_feature():\n    st.title(\"JSON Plugin Editor\")\n\n    # Load the JSON data\n    json_data = \"\"\"\n                {\n                \"Version\": \"1.0.0\",\n                \"PluginName\": \"DachserPurchaseOrderIdSmartFilters\",\n                \"CustomerName\": \"Dachser\",\n                \"ApiKeys\": [\n                ],\n                \"CustomerNames\": [\n                    \"Dachser Dev\",\n                    \"dachser\",\n                    \"Dachser Test\",\n                    \"dachser.integration\"\n                ],\n                \"InvoiceDetailTypeName\": \"DachserPurchaseOrderId\",\n                \"ValueToSet\": null,\n                \"ReturnSingleResult\": \"false\",\n                \"OverwriteExistingPrediction\": \"false\",\n                \"Patterns\": [\n                    {\n                    \"PatternName\": \"SelfMatchRegexLength10Prefix2\",\n                    \"MatchOnlyIfFalse\": \"false\",\n                    \"Topology\": \"Self\",\n                    \"TopologyPositionSpecifier\": \"1\",\n                    \"PatternType\": \"Regex\",\n                    \"IncludeMatchInValue\": \"true\",\n                    \"PatternProperties\": {\n                        \"RegexString\": \"^[^a-zA-Z0-9]*(Charge){0,1}(PO|P0){0,1}(45)[0-9]{8}([^a-zA-Z0-9]*|[^a-zA-Z0-9]{1,3}[0-9]{0,4})$\",\n                        \"RegexesExcludeInValue\": [\n                        \"(PO|P0)\",\n                        \"(?<=(^[^a-zA-Z0-9]*(Charge){0,1}(PO|P0){0,1}(45)[0-9]{8}))[^a-zA-Z0-9]{1,3}[0-9]{0,4}\",\n                        \"[^a-zA-Z0-9]*\",\n                        \"(Charge)\"\n                        ],\n                        \"AdditionalSplitSigns\": [\n                        \"+\",\n                        \",\"\n                        ]\n                    }\n                    }\n                ],\n                \"PatternGroups\": [\n                    {\n                    \"ContinueAfterGroupMatches\": \"true\",\n                    \"SkipIdPostProcessing\": \"true\",\n                    \"PatternNames\": [\n                        \"SelfMatchRegexLength10Prefix2\"\n                    ]\n                    }\n                ]\n                }\n                \"\"\"\n\n    # Convert the JSON string to a Python dictionary\n    data = json.loads(json_data)\n\n    st.markdown(\"## Plugin Details\")\n\n    # Create 3 columns\n    col1, col2, col3 = st.columns(3)\n\n    # Assign fields to each column\n    with col1:\n        data[\"Version\"] = st.text_input(\"Version:\", data[\"Version\"])\n        data[\"PluginName\"] = st.text_input(\"Plugin Name:\", data[\"PluginName\"])\n        data[\"CustomerName\"] = st.text_input(\"Customer Name:\", data[\"CustomerName\"])\n\n    with col2:\n        data[\"InvoiceDetailTypeName\"] = st.text_input(\"Invoice Detail Type Name:\", data[\"InvoiceDetailTypeName\"])\n        data[\"ValueToSet\"] = st.text_input(\"Value To Set:\", str(data[\"ValueToSet\"]))\n        data[\"ReturnSingleResult\"] = st.selectbox(\"Return Single Result:\", [\"true\", \"false\"], index=0 if data[\"ReturnSingleResult\"] == \"true\" else 1)\n\n    with col3:\n        data[\"OverwriteExistingPrediction\"] = st.selectbox(\"Overwrite Existing Prediction:\", [\"true\", \"false\"], index=0 if data[\"OverwriteExistingPrediction\"] == \"true\" else 1)\n\n\n    st.markdown(\"## API Keys\")\n    api_keys = st.multiselect(\"API Keys\", options=data[\"ApiKeys\"])\n    \n    st.markdown(\"## Customer Names\")\n    selected_customer_names = st.multiselect(\"Customer Names\", options=data[\"CustomerNames\"], default=data[\"CustomerNames\"])\n\n    st.markdown(\"## Patterns\")\n    for pattern in data[\"Patterns\"]:\n        st.subheader(pattern[\"PatternName\"])\n        pattern[\"MatchOnlyIfFalse\"] = st.selectbox(f\"{pattern['PatternName']} - Match Only If False:\", [\"true\", \"false\"], index=0 if pattern[\"MatchOnlyIfFalse\"] == \"true\" else 1)\n        pattern[\"Topology\"] = st.text_input(f\"{pattern['PatternName']} - Topology:\", pattern[\"Topology\"])\n        pattern[\"TopologyPositionSpecifier\"] = st.text_input(f\"{pattern['PatternName']} - Topology Position Specifier:\", pattern[\"TopologyPositionSpecifier\"])\n        pattern[\"PatternType\"] = st.text_input(f\"{pattern['PatternName']} - Pattern Type:\", pattern[\"PatternType\"])\n        pattern[\"IncludeMatchInValue\"] = st.selectbox(f\"{pattern['PatternName']} - Include Match In Value:\", [\"true\", \"false\"], index=0 if pattern[\"IncludeMatchInValue\"] == \"true\" else 1)\n        pattern[\"PatternProperties\"][\"RegexString\"] = st.text_area(f\"{pattern['PatternName']} - Regex String:\", pattern[\"PatternProperties\"][\"RegexString\"])\n\n    st.markdown(\"## Pattern Groups\")\n    for group in data[\"PatternGroups\"]:\n        group[\"ContinueAfterGroupMatches\"] = st.selectbox(\"Continue After Group Matches:\", [\"true\", \"false\"], index=0 if group[\"ContinueAfterGroupMatches\"] == \"true\" else 1)\n        group[\"SkipIdPostProcessing\"] = st.selectbox(\"Skip Id Post Processing:\", [\"true\", \"false\"], index=0 if group[\"SkipIdPostProcessing\"] == \"true\" else 1)\n        selected_pattern_names = st.multiselect(\"Pattern Names\", options=group[\"PatternNames\"], default=group[\"PatternNames\"])\n\n    # Button to save the changes\n    if st.button(\"Save Configuration\"):\n        # Here you can serialize the 'data' dictionary back to JSON and save it\n        with open(\"updated_config.json\", \"w\") as f:\n            json.dump(data, f, indent=4)\n        st.success(\"Configuration saved successfully!\")\n",
        "file_name": "json_plugin_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\label_qa_control_feature.py": {
        "summary": "The file `label_qa_control_feature.py` is a Streamlit application designed for quality assurance (QA) control of labeled data in invoices. It connects to a SQL Server database using the `pyodbc` library, allowing users to execute complex queries based on various filters such as label types, tags, and timeframes.\n\n### Key Features:\n- **Database Connection:** A function (`connect_to_database`) establishes a connection to a specified SQL Server database, handling both Linux and Windows drivers.\n- **Dynamic Query Generation:** It generates SQL queries based on user inputs for filtering invoices by label types, tags, and values, among other parameters.\n- **User Interface:** The app provides a sidebar for users to input database connection details (server, database, username, password) and other filtering criteria for invoice selection.\n- **Label Types Management:** It allows users to select predefined sets of label types or individual label types and combines them for querying.\n- **Data Retrieval:** After form submission, the app executes the generated SQL query, retrieves the results, and displays the corresponding invoice IDs for evaluation.\n\n### Workflow:\n1. Users connect to the database by selecting server details and entering credentials.\n2. Users specify various filters for invoices, such as timeframe, document count, label types, tags, and exclusions.\n3. Upon submission, the app constructs and executes a SQL query based on the user\u2019s input, then presents the filtered invoice IDs.\n\nThis application facilitates a streamlined process for evaluating labeled invoices, enhancing data quality and management in the context of invoice processing.",
        "content": "import sys\nimport streamlit as st\nimport requests\nimport pyodbc\nimport pandas as pd\nimport json\n\ndef connect_to_database(server, database, user, password):\n    \"\"\" Create a database connection using pyodbc \"\"\"\n\n    if sys.platform == 'linux':\n        driver_name = 'ODBC Driver 17 for SQL Server'\n    elif sys.platform == 'win32':\n        driver_name = 'SQL Server'      \n\n    try:\n        conn_string = f'DRIVER={{{driver_name}}};SERVER={server};DATABASE={database};UID={user};PWD={password}'\n        conn = pyodbc.connect(conn_string)\n        return conn\n    except pyodbc.Error as e:\n        st.sidebar.error(\"Database connection failed: \" + str(e))\n        return None\n\ndef get_label_values_query(users, label_values):\n    if label_values == \"\":\n        return \"\"\n    label_values = label_values.split(\",\")\n    query_string = f\"\"\n    for label_value in label_values:\n        key,value = label_value.split(\":\")\n        query_string += f\"\"\"InvoiceEntity_Id IN (SELECT InvoiceEntity_Id FROM Label WHERE Type = '{key}' AND Text = '{value}')\n        AND\"\"\"\n    return query_string\n\ndef get_tags_query(tags):\n    tags_list = tags.split(\",\")\n    tag_string = \"\"\n    for tag in tags_list:\n        tag_string + f\"('{tag}'), \"\n    tag_string = tag_string[:-2] + \";\" # remove \", \"    \n\n    return f\"\"\"\n    DECLARE @TagNames TABLE (TagName NVARCHAR(255));\n    INSERT INTO @TagNames (TagName)\n    VALUES {tag_string}    \n    \"\"\"\n\ndef get_label_query(users, label_types, label_group_types, label_values):\n    \n    if label_types == \"\" and label_group_types == \"\" and label_values == \"\":\n        return \"\"\n    \n    query = \"\"\"AND Id IN (SELECT [InvoiceEntity_Id] FROM Label\n                    WHERE\"\"\"\n\n    if label_types != \"\":\n        # THIS ONLY WORKS FOR ONE USER TODO\n        # with 2 Users only one needs a label\n        label_types_str = \",\".join(f\"'{label}'\" for label in label_types)\n        query += f\"\"\" Type IN (SELECT [Id] FROM LabelType WHERE Name IN ({label_types_str}))\n                    AND released = 1 \n                    AND CreatedBy_Id IN ({users})\n                    AND\"\"\"\n    if label_values != \"\":\n        label_values_query = get_label_values_query(users, label_values)\n        query += label_values_query\n\n    if label_types != \"\" or label_values != \"\":\n        # remove last AND\n        query = query[:-3]+\")\\nAND\"\n\n    if label_group_types != \"\":\n        formated_groups = \",\".join([f\"'{name.strip()}'\" for name in label_group_types.split(\",\")])\n        query += f\"\"\"Id IN (SELECT [InvoiceEntity_Id] FROM LabelGroup\n                    WHERE Type IN (SELECT [Id] FROM LabelType WHERE Name IN ({formated_groups}))\n                    AND ReleaseItem_Id != NULL\n                    AND CreatedBy_Id IN ({users})) \n                    AND\"\"\"\n        \n    return query[:-3] #remove last AND\n\n\ndef get_invoices_query(users, timeframe, doc_count, docs_per_cluster, label_types, label_group_types, tags, label_values, excluded_ids):\n              \n    sql_query = \"\"\n\n    if tags != \"\":\n        tags_query = get_tags_query(tags)\n        sql_query += tags_query\n\n    # add clusterinfo to query\n    sql_query += f\"\"\"                \n\n        WITH ClusterInvoiceRanked AS (\n            SELECT \n                [Invoice_Id],\n                ROW_NUMBER() OVER (PARTITION BY [Cluster_Id] ORDER BY [Created] DESC) AS RowNum\n            FROM [bcidb].[dbo].[ClusterInvoice]\n        )\n        \"\"\"\n    # filter invoices by timeframe    \n    sql_query += f\"\"\"SELECT TOP({doc_count}) Id FROM Invoice \n        WHERE Created >= DATEADD(day, -{timeframe}, GETDATE())\"\"\"\n    \n    label_query = get_label_query(users, label_types, label_group_types, label_values)\n    sql_query += label_query\n\n    if tags != \"\":\n        sql_query += f\"\"\"AND Id IN (SELECT DISTINCT di.InvoiceId\n                FROM [bcidb].[dbo].[DocumentInfo] di\n                INNER JOIN [bcidb].[dbo].[Package] p ON di.PackageId = p.Id\n                INNER JOIN [bcidb].[dbo].[PackageTag] pt ON p.Id = pt.PackageId\n                INNER JOIN [bcidb].[dbo].[Tag] t ON pt.TagId = t.Id\n                WHERE t.TagName IN (SELECT TagName FROM @TagNames)\n                GROUP BY di.InvoiceId\n                HAVING COUNT(DISTINCT t.TagName) = (SELECT COUNT(DISTINCT TagName) FROM @TagNames)\n                AND di.InvoiceId IS NOT NULL)\"\"\"  \n    \n    sql_query += f\"\"\"AND Id IN (SELECT \n            [Invoice_Id]\n            FROM ClusterInvoiceRanked\n            WHERE RowNum <= {docs_per_cluster})\n            ORDER BY Created DESC\n            \"\"\"\n    \n    if excluded_ids != \"\":            \n        sql_query = f\"AND Id NOT IN ({excluded_ids})\"\n    return sql_query\n\ndef fetch_data_as_dict(cursor):\n    \"\"\" Convert cursor rows to a list of dictionaries based on cursor description. \"\"\"\n    columns = [col[0] for col in cursor.description]\n    return [dict(zip(columns, row)) for row in cursor.fetchall()]\n\ndef get_label_types(conn):\n    \"\"\" Fetch label types from the database and return them as a list of strings \"\"\"\n    if conn is not None:\n        with conn.cursor() as cur:\n            cur.execute(\"SELECT Name FROM LabelType;\")\n            # Fetch all results and extract the 'Name' from each tuple in the list\n            label_types = [row[0] for row in cur.fetchall()]  # Converts list of tuples to list of strings\n        return label_types\n    return []\n\ndef qa_control():\n    st.title(\"Label QA Control\")\n    # Database connection parameters\n    st.sidebar.title(\"Database Connection\")\n    server = st.sidebar.selectbox('Server', ('serverbdsql01.ad.blumatix.com,62434', 'serverbdsql01.ad.blumatix.com,62442', 'SERVERBDSQL01\\BLUDELTA', '192.168.137.60,62434'))\n    database = st.sidebar.selectbox('Select Database', ('bcidb', 'bcidb_dev'))\n    user = st.sidebar.text_input('Username', 'user')\n    password = st.sidebar.text_input('Password', 'password', type=\"password\")\n    \n    conn = connect_to_database(server, database, user, password)\n    \n    st.markdown(\"\"\"\n    ### Description: what does the feature do\n    \"\"\")\n\n    # Define sets of label types\n    label_type_sets = {\n        'Invoice': ['DocumentType', 'InvoiceCurrency', 'DeliveryDate', 'InvoiceDate', 'InvoiceId', 'CustomerId', 'GrandTotalAmount', \n                    'NetAmount', 'VatRate', 'VatAmount', 'SenderVatId', 'ReceiverVatId', 'Iban', 'Bic', 'ReceiverOrderId', \n                    'SenderOrderId', 'ReceiverOrderDate', 'SenderOrderDate', 'TotalNetAmount', 'TotalVatAmount', 'DeliveryNoteId', \n                    'BankCode', 'BankAccount', 'DeliveryPeriodKey', 'DeliveryPeriodValue', 'ReceiverTaxId', 'SenderTaxId'],\n\n        'OrderConfirmation': ['DocumentType', 'InvoiceCurrency', 'DeliveryDate', 'CustomerId', 'GrandTotalAmount', 'NetAmount', 'VatRate',\n                              'VatAmount', 'SenderVatId', 'ReceiverVatId', 'Iban', 'Bic', 'ReceiverOrderId', 'SenderOrderId',\n                              'ReceiverOrderDate', 'SenderOrderDate', 'TotalNetAmount', 'TotalVatAmount', 'BankCode', 'BankAccount',\n                              'ReceiverTaxId', 'SenderTaxId', 'OrderConfirmationId', 'OrderConfirmationDate', 'SurchargeType', 'SurchargeRate',\n                              'SurchargeAmount', 'DeliveryTerm'],\n\n        'Quotation': ['DocumentType', 'InvoiceCurrency', 'DeliveryDate', 'CustomerId', 'GrandTotalAmount', 'NetAmount', 'VatRate',\n                              'VatAmount', 'SenderVatId', 'ReceiverVatId', 'Iban', 'Bic', 'ReceiverOrderId', 'SenderOrderId',\n                              'ReceiverOrderDate', 'SenderOrderDate', 'TotalNetAmount', 'TotalVatAmount', 'BankCode', 'BankAccount',\n                              'ReceiverTaxId', 'SenderTaxId', 'QuotationId', 'QuotationDate'],\n\n        'Contact': ['ContactName', 'Street', 'ZipCode', 'City', 'Country', 'SenderReceiverClassification', 'AttentionName', 'Region'],\n\n        'LineItems': ['LineItemPositionNumberHeader', 'LineItemDescriptionHeader', 'LineItemArticleNumberHeader', 'LineItemQuantityHeader', \n                      'LineItemUnitHeader', 'LineItemUnitPriceHeader', 'LineItemVatRateHeader', 'LineItemAmountHeader', 'LineItemCustomDetailHeader', \n                      'LineItemPositionNumber', 'LineItemDescription', 'LineItemArticleNumber', 'LineItemQuantity', 'LineItemUnit', 'LineItemUnitPrice', \n                      'LineItemVatRate', 'LineItemAmount', 'LineItemCustomDetail', 'LineItemBufferText', 'LineItemDeliveryNoteIdHeader', \n                      'LineItemDeliveryDateHeader', 'LineItemOrderIdHeader', 'LineItemUnitPriceCoefficientHeader', 'LineItemDiscountHeader', \n                      'LineItemDeliveryNoteId', 'LineItemDeliveryDate', 'LineItemOrderId', 'LineItemUnitPriceCoefficient', 'LineItemDiscount', 'LineItemCount'],\n\n        'Automotive': ['FirstRegistrationDate', 'PlateId', 'Mileage', 'VehicleIdentificationNumber']\n    }\n\n    \n    \n    users = st.text_input(\"Users to Evaluate\")\n    col1, col2, col3 = st.columns(3)\n    timeframe = col1.number_input(\"Timeframe in days\", value=180)\n    doc_count = col2.number_input(\"Number of Documents \", value=100)\n    docs_per_cluster = col3.number_input(\"Maximum Number of Documents per Cluster\", value=3)\n\n    # Predefined sets selection\n    selected_sets = st.multiselect('Select Predefined Label Type Sets', list(label_type_sets.keys()))\n\n    # Combine selected predefined sets into a single list\n    combined_label_types = []\n    for set_name in selected_sets:\n        combined_label_types.extend(label_type_sets[set_name])\n\n    # Allow selection of individual types and combine with predefined sets\n    label_types = get_label_types(conn)\n    selected_individual_types = st.multiselect('Select or add Individual Label Types', label_types, default=label_types[1])\n    # Combine individual selections with predefined sets, remove duplicates\n    combined_label_types.extend(selected_individual_types)\n    combined_label_types = list(set(combined_label_types))  # Remove duplicates\n    st.write(f\"Selected Label Types: {combined_label_types}\")\n\n    label_group_types = st.text_input(\"LabelGroupTypes to Evaluate\")\n    tags = st.text_input(\"List of Tags (comma-separated)\")\n    label_values = st.text_input(\"List of Label Values to filter by (e.g. DocumentType\\:ForwardingOrder,ReceiverCountry\\:TW)\")\n    excluded_ids = st.text_input(\"List of ids to exclude from sample (comma-separated)\")\n    ci_check = st.checkbox(\"Set Necessary CI Interval\")\n    if ci_check:        \n        ci_low = st.number_input(\"Confidence Interval (low) to achieve\", value=0.95)\n\n   \n\n\n    submit = st.button(\"Submit\")\n    if submit:\n        query = get_invoices_query(users, timeframe, doc_count, docs_per_cluster, combined_label_types, label_group_types, tags, label_values, excluded_ids)\n        with conn.cursor() as cur:\n            cur.execute(query)\n            data = fetch_data_as_dict(cur)\n            ids = [str(item['Id']) for item in data]\n            ids_comma_separated = \", \".join(ids)\n\n            st.subheader(\"Invoice Ids for Evaluation:\")\n            st.success(ids_comma_separated)\n        ",
        "file_name": "label_qa_control_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llama2_chatbot_feature.py": {
        "summary": "The file `llama2_chatbot_feature.py` implements a chatbot using Streamlit and the LLaMA2 language model for text generation. Here\u2019s a concise summary of its main components:\n\n1. **Dependencies**: It imports necessary libraries including Streamlit for the web app framework, a custom LLM_Llama2 class for the LLaMA2 model, and a text generation client.\n\n2. **Caching Functions**: It defines two cached functions:\n   - `ChatModel`: Creates a client for generating responses based on user prompts, with parameters for URL, timeout, temperature, and top_p.\n   - `LLama2Chatbot`: Initializes an instance of the LLM_Llama2 model.\n\n3. **Sidebar Creation**: The `create_llm_sidebar` function sets up a sidebar in the Streamlit app for user inputs, allowing configuration of the model's URL, timeout, temperature, and top_p values.\n\n4. **Chatbot Logic**:\n   - `start_chatbot`: This function manages the chat interface, displaying messages in the chat history and allowing users to input their prompts.\n   - It maintains chat history in the session state and provides an option to clear conversations.\n   - It generates responses from the LLaMA2 model based on the user's input and previous messages.\n\n5. **User Interaction**: The chat input is processed, and if the last message isn't from the assistant, it generates a new response using the defined model, which is then displayed in the chat.\n\nOverall, the script sets up a functional chat interface using LLaMA2 for generating intelligent responses based on user queries.",
        "content": "import streamlit as st\nfrom llms.llm_llama2 import LLM_Llama2\nfrom text_generation import Client\nimport os\n\n@st.cache_resource()\ndef ChatModel(url, timeout, temperature, top_p):\n    client = Client(url, timeout=timeout)\n    def generate(prompt):\n        return client.generate(prompt, temperature=temperature, top_p=top_p, max_new_tokens=2048, return_full_text=False).generated_text    \n    return generate\n\n\n@st.cache_resource()\ndef LLama2Chatbot(url, timeout, temperature, top_p):\n    return LLM_Llama2(url, timeout, temperature, top_p)\n\n\ndef create_llm_sidebar():\n    with st.sidebar:\n        st.title('\u00f0\u0178\u2019\u00ac LLM Chatbot')\n\n        st.subheader('Text-Generation-Inference Url')\n        url = st.text_input('Enter Url', 'http://192.168.137.80:8090')\n        timeout_sec = st.text_input('Enter timeout [sec]', '120')\n\n        # check timeout and convert to int\n        try:\n            timeout = int(timeout_sec)\n        except ValueError:\n            st.error('Timeout must be an integer')\n            return\n\n        # Refactored from <https://github.com/a16z-infra/llama2-chatbot>\n        st.subheader('Models and parameters')\n        \n        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=2.0, value=0.1, step=0.01)\n        top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n        return url, timeout, temperature, top_p\n        \n\ndef start_chatbot():\n    url, timeout, temperature, top_p= create_llm_sidebar()\n    chat_model =ChatModel(url, timeout, temperature, top_p)   \n\n    # Store LLM generated responses\n    if \"messages\" not in st.session_state.keys():\n        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n\n    # Display or clear chat messages\n    for message in st.session_state.messages:\n        with st.chat_message(message[\"role\"]):\n            st.write(message[\"content\"])\n\n    def clear_chat_history():\n        st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n    st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n\n    # Function for generating LLaMA2 response\n    def generate_llama2_response(prompt_input):\n        string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n\n        for dict_message in st.session_state.messages:\n            if dict_message[\"role\"] == \"user\":\n                string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\\\n\\\\n\"\n            else:\n                string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\\\n\\\\n\"\n        output = chat_model(f\"prompt {string_dialogue} {prompt_input} Assistant: \")\n        return output\n\n    # User-provided prompt\n    prompt = st.chat_input()\n    if prompt:\n        st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n        with st.chat_message(\"user\"):\n            st.write(prompt)\n\n    # Generate a new response if last message is not from assistant\n    if st.session_state.messages[-1][\"role\"] != \"assistant\":\n        with st.chat_message(\"assistant\"):\n            with st.spinner(\"Thinking...\"):\n                response = generate_llama2_response(prompt)\n                placeholder = st.empty()\n                full_response = ''\n                for item in response:\n                    full_response += item\n                    placeholder.markdown(full_response)\n                placeholder.markdown(full_response)\n        message = {\"role\": \"assistant\", \"content\": full_response}\n        st.session_state.messages.append(message)\n",
        "file_name": "llama2_chatbot_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llama2_prompt_example.py": {
        "summary": "The file `llama2_prompt_example.py` is a Python script that utilizes the Llama-2 model from Hugging Face's Transformers library to extract contact information from invoices in a specified JSON format. Here\u2019s a concise summary of its key components and functionality:\n\n1. **Model Setup**: The script imports necessary libraries and sets up a Llama-2 model (specifically `meta-llama/Llama-2-13b-chat-hf`) for text generation. It configures the model for quantization to optimize memory usage and loads it on a CUDA device if available.\n\n2. **Prompt Template**: A prompt template is created that instructs the model to extract sender and receiver contact details from provided invoice texts. The model is directed to strictly adhere to a specific JSON output structure without adding any extra context.\n\n3. **Example Invoices**: The script includes example invoice texts for which it runs the extraction process. Each invoice is provided as input to the LLMChain, which handles the interaction with the Llama-2 model.\n\n4. **Response Handling**: The script processes the model's output, converting it from a string to a JSON object for better readability and prints it.\n\n5. **Error Handling**: Basic error handling is implemented when attempting to convert the model's response to JSON to handle cases where the extraction may fail.\n\nOverall, the script demonstrates a practical application of a language model for structured data extraction from unstructured text (invoices).",
        "content": "from torch import cuda, bfloat16\nimport transformers\nfrom typing import Union\nimport json\n\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain.memory import ConversationBufferWindowMemory\n\n#model_id = 'meta-llama/Llama-2-70b-chat-hf'\nmodel_id = 'meta-llama/Llama-2-13b-chat-hf'\n\n\ndevice = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n\n# set quantization configuration to load large model with less GPU memory\n# this requires the `bitsandbytes` library\nbnb_config = transformers.BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type='nf4',\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=bfloat16\n)\n\n# begin initializing HF items, need auth token for these\nhf_auth = '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'\nmodel_config = transformers.AutoConfig.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\n\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,\n    device_map='auto',\n    use_auth_token=hf_auth\n)\nmodel.eval()\nprint(f\"Model loaded on {device}\")\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(\n    model_id,\n    use_auth_token=hf_auth\n)\n\ngenerate_text = transformers.pipeline(\n    model=model, tokenizer=tokenizer,\n    return_full_text=True,  # langchain expects the full text\n    task='text-generation',\n    # we pass model parameters here too\n    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n    max_new_tokens=256,  # mex number of tokens to generate in the output\n    repetition_penalty=1.1  # without this output begins repeating\n)\n\n# create a Llama2 chat template for contact extraction from invoices\nBOS, EOS = \"<s>\", \"</s>\"\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\nB_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\nsys_msg = B_SYS + \"\"\"Assistant, your primary and ONLY objective is to extract contact information from invoices and present it DIRECTLY in the specified JSON format. Do NOT add any additional context, framing, or explanation. The output should be a JSON object and nothing else.\n\nThe exact structure of the desired JSON output is:\n\n```json\n{{\n    \"action\": \"Final Answer\",\n    \"action_input\": {{\n        \"sender\": {{\n            \"name\": \"<Sender Name>\",\n            \"address\": \"<Sender Address>\"\n        }},\n        \"receiver\": {{\n            \"name\": \"<Receiver Name>\",\n            \"address\": \"<Receiver Address>\"\n        }}\n    }}\n}}\n```\nNote: The fields <Sender Name>, <Sender Address>, <Receiver Name>, and <Receiver Address> are placeholders and should be replaced with the relevant details extracted from the invoice.\n\nTo guide you, here are two examples of previous interactions that were correctly handled:\n\nUser: Extract all contacts from the invoice and return them as a JSON object. Differentiate between invoice sender and receiver.\n\n### Invoice:\nRECHNUNG\nHandelsagentur Fux\nDATUM: 25.03.2020\nRechnung Nr.: 1954746731\nKUNDEN-ID: HVK1A\nSchwarzstra\u00c3\u0178e 45 5020 Salzburg\nRECHNUNGSADRESSE LIEFERADRESSE\nMassimo Mustermann\nMatch GmbH\nBergheimerstra\u00c3\u0178e 14\n5020 Salzburg\n+436608553947\nRechnungsadresse\nBestellnummer: 258934 Bestelldatum: 15.3.2020\nAuftragsnummer: A1237B Auftragsdatum: 15.3.2020\nBESCHREIBUNG\nMenge Steuersatz Preis (netto)\nLieferdatum: 20.3.2020 Lieferscheinnummer: LS185\nSteinway Konzert Fl\u00c3\u00bcgel Wei\u00c3\u0178 1 20 % 499 000.00 \u00e2\u201a\u00ac\nDirigierstab Elfenbein 1 20 % 780.00 \u00e2\u201a\u00ac\nLieferdatum: 22.3.2020 Lieferscheinnummer: LS187\nnVidia GPU M60 'Tesla'\n4 20 % 28 560.00 \u00e2\u201a\u00ac\nMars Riegel\n1000 10 % 800.00 \u00e2\u201a\u00ac\nGesamtbetrag netto 529 140.00 \u00e2\u201a\u00ac\n10 % 20 %\nSteuerbetrag 80.00 \u00e2\u201a\u00ac 105 668.00 \u00e2\u201a\u00ac 105 748.00 \u00e2\u201a\u00ac\nNetto Betrag\n800.00 \u00e2\u201a\u00ac\n528 340.00 \u00e2\u201a\u00ac 529 140.00 \u00e2\u201a\u00ac\nSumme brutto 880.00 \u00e2\u201a\u00ac 634 008.00 \u00e2\u201a\u00ac 634 888.00 \u00e2\u201a\u00ac\nZahlung: innerhalb von 10 Tagen 2 % Skonto\n30 Tage netto\nAlle Zahlungen an Handelsagentur Fux\n###\n\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n    \"action_input\": {{\n        \"sender\": {{\n            \"name\": \"Handelsagentur Fux\",\n            \"address\": \"Schwarzstra\u00c3\u0178e 45 5020 Salzburg\",\n        }},\n        \"receiver\": {{\n            \"name\": \"Massimo Mustermann\",\n            \"address\": \"Match GmbH, Bergheimerstra\u00c3\u0178e 14, 5020 Salzburg\"\n        }}\n    }}\n}}\n```\n\nUser: Extract all contacts from the invoice and return them as a JSON object. Differentiate between invoice sender and receiver.\n\n### Invoice:\nVerwaltung\nund Braust\u00c3\u00a4tte\nLandgrabengasse 15-18\n90321 N\u00c3\u00bcrnberg\nTelefon\n0911 2211-0\nTelefax\n0911 2211-111\nenju.isti\u00c4\u00b1\u00c4\u00b1er-Zr\u00c3\u00a3u \u00c3\u2020trn\u00c3\u00aderj\nRechnung\nAbr Per : Tagesfaktura Angustiner\nBitte bei Zahlung /R\u00c3\u00bcckfrage angeben\nDatum Belegnr. Kundennr. Seite\n28.02.2016 16616022803 56899865 1/1\nWarenempf\u00c3\u00a4nger: 56899865\nAngustiner-Br\u00c3\u00a4u N\u00c3\u00bcrnberg AG I Landgrabengasse 15-18 190321 N\u00c3\u00bcrnberg\nAn\nSebastian Becker\nLudwigstra\u00c3\u0178e 83\n91765 N\u00c3\u00bcrnberg\nART-NR BEZEICHNUNG / INHALT Menge HL/LTR Preis Betrag - EUR\nLIEFERUNG 512864812 vom 28.02.2016 Ab: Depot Ro\u00c3\u0178tal\n1820 Angustiner Kellerbier dunkel P-Keg .20 L 8 1,600 135,00 1.080,00\n1252 Angustiner Lagerbier hell P-Keg .20 L 5 1,000 143,00 715,00\n6200 Angustiner Spezi Mixgetr\u00c3\u00a4nk 20/0,5 57 5,700 8,00 456,00\nSumme Warenwert 2.251,00\n###\n\nAssistant: ```json\n{{\"action\": \"Final Answer\",\n    \"action_input\": {{\n        \"sender\": {{\n            \"name\": \"Angustiner-Br\u00c3\u00a4u N\u00c3\u00bcrnberg AG\",\n            \"address\": \"Landgrabengasse 15-18, 90321 N\u00c3\u00bcrnberg\"\n        }},\n        \"receiver\": {{\n            \"name\": \"Sebastian Becker\",\n            \"address\": \"Ludwigstra\u00c3\u0178e 83, 91765 N\u00c3\u00bcrnberg\"\n        }}\n    }}\n}}\n```\n\nProcess the invoice provided by the user and extract the necessary contact details. Remember to STRICTLY adhere to the aforementioned JSON structure and provide NO additional context or explanation.\"\"\" + E_SYS\n\n\n\ninst_msg = \"\"\"User: Extract all contacts from the invoice and return them as a JSON object.\n\n{invoice}\n\"\"\"\n\ntemplate = B_INST + sys_msg  + inst_msg + E_INST\nprompt_template = PromptTemplate(input_variables=[\"invoice\"], template=template)\n\nfrom langchain.llms import HuggingFacePipeline\n\nllm = HuggingFacePipeline(pipeline=generate_text)\n\nllama2_chain = LLMChain(\n    llm=llm,\n    prompt=prompt_template,\n    verbose=False,\n    memory=ConversationBufferWindowMemory(k=1),\n    llm_kwargs={\"max_length\": 4096}\n)\n\ninvoice_text = \"\"\" \\\n### Invoice:\nElektronikartikel Hergl\nBaumweg 92\n92330 N\u00c3\u00bcrnberg\nRechnungsdatum\nRechnungsnummer\n03.12.2015\nBS35-158213\n327623\n30 Tage\n02.01.2016\nEmpf\u00c3\u00a4nger:\nHerr\nSebastian Becker\nLudwigstra\u00c3\u0178e 83\n91765 N\u00c3\u00bcrnberg\nKundennummer\nZahlungsziel\nF\u00c3\u00a4lligkeitsdatum\nUSt. %\nGesamt\nBezeichnung\nAnzahl\nEinheit\nNP/Einheit\nUSt. ?\nSt\u00c3\u00bcck\nSt\u00c3\u00bcck\nTaschenradio ,Vox'\n20,25 ?\n7,70 ?\n48,20 ?\n2\n19 %\n50,23 ?\n9,54 ?\n59,77 ?\nMP3-Player Medion BestSound\n1\n19 %\n90,73 ?\nNettobetrag\nUSt.\n17,24 ?\nGesamtbetrag\n107,97 ?\nZus\u00c3\u00a4tzliche Informationen\nWenn Sie innerhalb von 15 Tagen nach Rechnungsausstellung zahlen erhalten Sie 2 % Skonto.\nElektronikartikel Hergl\nBaumweg 92\n92330 N\u00c3\u00bcrnberg\nDeutschland\nUSt.-IdNr. DE 379233689\nKontaktinformationen\nHeimrich Hergl\nTelefon +49 91 223135-0\nBankverbindung\nIBAN\nSWIFT/ BIC\nDE71362500002542863326\nSPMHDE3EXXX\nEmail: heinrich.hergl@elektronik.de\nwww.herglpowergmbh.de\n###\n\"\"\"\n\nresponse = llama2_chain.predict(\n    invoice=invoice_text\n)\n\ntry:\n    print(json.dumps(json.loads(response), indent=4, sort_keys=True))\nexcept:\n    print(response)\n\ninvoice_text = \"\"\" \\\n### Invoice:\nSulzbacher Stra\u00c3\u0178e 11\n73731 Dekkendorf\nTel: 08 22 / 123 44 00\nMobil: 01 22 / 123 45 00\nAlb\n1\nGlaserei Galler \u00c2\u00bb\nD\u00c3\u00b6\u00c3\u00b6rtherweg 11\n12321 M\u00c3\u00bcnsterberg\nTel: 01 23 / 383 683 500\nMobil: 01 22 / 123 45 00\n Ihr Fenster Service\nSebastian Becker\nLudwigstra\u00c3\u0178e 83 RECHNUNG\n91765 N\u00c3\u00bcrnberg Kunden-Nr. 98556\nWohnanlage Mieter RECHNUNGS-N R. 036589\nDATUM 03.01.16\nAUFTRAG NR.\nBESCHREIBUNG\nMENGE EURO EURO\nMontage von 1 Alu Panzer Fenster eingestellt\nund ge\u00c3\u00b6lt.\nArbeiten erledigt am 01.01.2016 und am\n03.01.2016\nAlu- Rollopanzer; Sondermodell 5,11 159,00 812,49\nKleinmaterial \u00c3\u2013l Schauben Entsorgung von 1 5,30 5,30\nKleinteilen usw.)\nEntsorgung von Altpanzer pauschal 1 25,00 25,00\nArbeitszeit 5 50,00 250,00\nAnfahrtspauschale f\u00c3\u00bcr 2 Monteure 1 25,00 25,00\nZahlbar sofort rein netto.\nZwischensumme Euro\n1.117, 79\nWir gew\u00c3\u00a4hren Ihnen f\u00c3\u00bcr die von uns ausgef\u00c3\u00bchrten\nMwSt -19 % Euro\n212,38\nArbeiten eine Garantie von 2 Jahren.\nGesamt Euro ?\n1.330,17\nBesonderer Hinweis:\nAufgrund einer Vorschrift des Finanzamtes gilt f\u00c3\u00bcr jede\nRechnung eine Aufbewahrungsfrist von 2 Jahren!\nBankverbindungen:\nVolksbank K\u00c3\u00b6ln Nord Berliner Volksbank\nIBAN: DE02362500001233567104 DE61100900001544867110\nBIC: SPMHDE3EXXX BIC: BEVODEBBXXX\nwww.gute fensterl.de\nUSt-ID: 111111168\nfenster@t-onlinel.de\n###\n\"\"\"\n\nresponse = llama2_chain.predict(\n    invoice=invoice_text\n)\n\n# now convert the 'response' string to a JSON object and pretty print it in the terminal use green color for the JSON keys and values\ntry:\n    print(json.dumps(json.loads(response), indent=4, sort_keys=True))\nexcept:\n    print(response)\n\n\ninvoice_text = \"\"\" \\\n### Invoice:\nMag Verena Stienitzka\nGorianstra\u00c3\u0178e 12\n5020 Salzburg\nOesterreich\nRechnungsnr.: 6000112131\nRechnungsdatum: 04.07.2016\nVersanddatum: 04.07.2016\nKundennr.: AT70061846\nBestellnr.: AT40114458\nHotline: 08000 / 54 54 54\nRechnung\nSehr geehrte Kundin und Kunde,\nherzlichen Dank f\u00c3\u00bcr Ihre Bestellung vom 04.07.2016 im NIVEA Online Shop\nWir haben folgende Auftragsdaten zu Ihrer Bestellung zusammengefasst:\nZahlungsart: Online \u00c3\u0153berweisung\nUST %\nGesamtpreis\nBrutto EUR\nArt.-Nr.\nArtikelbezeichnung\nMenge\nEinzelpreis\nEUR\nNIVEA Professional Hyalurons\u00c3\u00a4ure Nachtpf\n958140100005\n958120100005\n958190100006\nVSK\n1\n1\n1\n1\n20\n26,00\n26,00\n26,00\n3,95\n26,00\n26,00\n26,00\n3,95\nNIVEA Professional Hyalurons\u00c3\u00a4ure Augenpf\n20\nNIVEA Professional Hyalurons\u00c3\u00a4ure CC Crea\n20\nVersandkosten\n20\n81,95\n13,65\n68,30\n81,95\nBruttorechnungsbetrag\nEnthaltene USt.\nNettorechnungsbetrag\nzu zahlender Betrag\nSollten Sie Fragen zu Ihrer Rechnung haben kontaktieren Sie uns bitte per Kontaktformular oder telefonisch\nunter 08000 / 54 54 54kostenlos aus \u00c3\u2013sterreich; Mo-Sa 9:00 - 19:00 Uhr).\nWir w\u00c3\u00bcnschen Ihnen viel Freude mit unseren Produkten und freuen uns Sie bald wieder im NIVEA Online Shop begr\u00c3\u00bc\u00c3\u0178en zu\nd\u00c3\u00bcrfen!\nHerzliche Gr\u00c3\u00bc\u00c3\u0178e\nIhr NIVEA Team\nNIVEA Online Shop Beiersdorf Ges mbH\nSchleefstra\u00c3\u0178e la EURO PLAZA Geb\u00c3\u00a4ude H\n44287 Dortmund Lehrbachgasse 13\nKundenservice: A- 1120 Wien\nTelefon: 08000 54 54 54 Firmenbuch und Firmenbuchnummer:\n\u00c3\u2013ffnungszeiten: Handelsgericht Wien FN 88176 x\nMo.-Sa. 09:00 - 19:00 Uhr Umsatzsteueridentifikationsnummer:\nATU36743707\n###\n\"\"\"\n\nresponse = llama2_chain.predict(\n    invoice=invoice_text\n)\n\n# now convert the 'response' string to a JSON object and pretty print it in the terminal use green color for the JSON keys and values\ntry:\n    print(json.dumps(json.loads(response), indent=4, sort_keys=True))\nexcept:\n    print(response)\n\n\n",
        "file_name": "llama2_prompt_example.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llama2_prompt_feature.py": {
        "summary": "The provided Python script is a Streamlit application that leverages the Llama 2 model for text generation. Here's a concise summary of its key components:\n\n1. **Imports and Setup**: The script imports necessary libraries, including Streamlit for building the web interface and Langchain for managing LLM interactions. It also imports a custom Llama 2 model class.\n\n2. **Sidebar Creation**: The `create_llama2_sidebar` function defines a sidebar where users can enter configuration parameters such as the inference URL, timeout, temperature, and top-p values for the model.\n\n3. **Main Feature - Invoice Extractor**: The `invoice_extractor_feature` function implements the main functionality:\n   - It reads prompt templates from local files and allows users to upload a custom prompt template.\n   - A prompt template is constructed using user-defined and default templates.\n   - An instance of `LLMChain` is created to manage interactions with the Llama 2 model, maintaining a conversation history.\n\n4. **Chat Interface**: The application maintains a chat interface with message handling where:\n   - User inputs are captured and displayed.\n   - Responses from the Llama 2 model are generated based on user prompts.\n   - The output is processed to extract JSON data if available, formatted, and displayed.\n\n5. **State Management**: The script employs Streamlit's session state to keep track of messages exchanged during the chat session.\n\nOverall, this script provides a user-friendly interface for interacting with the Llama 2 text generation model, enabling users to customize prompts and view model responses in a conversational format.",
        "content": "import streamlit as st\nfrom constants import B_INST, E_INST, B_SYS, E_SYS, inst_msg\n#from torch import cuda, bfloat16\n#import transformers\nfrom langchain import LLMChain, PromptTemplate\nfrom langchain.memory import ConversationBufferWindowMemory\n#from langchain.llms import HuggingFacePipeline\nimport json\nimport re\n\nfrom llms.llm_llama2 import LLM_Llama2\n\ndef create_llama2_sidebar():\n    with st.sidebar:\n        st.title('\u00f0\u0178\u00a6\u2122\u00f0\u0178\u2019\u00ac Blu Llama 2')\n\n        st.subheader('Text-Generation-Inference Url')\n        url = st.text_input('Enter Url', 'http://192.168.137.80:8080')\n        timeout_sec = st.text_input('Enter timeout [sec]', '120')\n\n        # check timeout and convert to int\n        try:\n            timeout = int(timeout_sec)\n        except ValueError:\n            st.error('Timeout must be an integer')\n            return\n\n        # Refactored from <https://github.com/a16z-infra/llama2-chatbot>\n        st.subheader('Models and parameters')\n        \n        temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=2.0, value=0.01, step=0.01)\n        top_p = st.sidebar.slider('top_p', min_value=0.0, max_value=1.0, value=0.9, step=0.01)\n        return url, timeout, temperature, top_p\n\n\ndef invoice_extractor_feature():\n    url, timeout, temperature, top_p = create_llama2_sidebar()\n    llm = LLM_Llama2(url, timeout, temperature, top_p)\n\n    with open('default_system_prompt_template.txt') as f:\n        default_prompt_template_value = f.read()\n\n    with (open('default_user_instruct_prompt_template.txt')) as f:\n        default_instruction_prompt_template_value = f.read()\n\n    # Upload a prompt template file\n    uploaded_file = st.file_uploader('Upload a prompt template text file.', type=['txt'])\n\n    if uploaded_file is not None:\n        loaded_text = uploaded_file.read().decode('utf-8')\n    else:\n        loaded_text = default_prompt_template_value\n\n    # Add text area component\n    input_prompt_template = st.text_area('Define a prompt template.', value=loaded_text, height=300)\n    \n    instr_prompt_template = st.text_area('Define an instruction prompt template.', value=default_instruction_prompt_template_value, height=60)\n\n    # Define a prompt_template\n    sys_msg = B_SYS + input_prompt_template + E_SYS\n    template = B_INST + sys_msg + instr_prompt_template + E_INST\n    prompt_template = PromptTemplate(input_variables=['Content'], template=template)\n    llama2_chain = LLMChain(\n        llm=llm,\n        prompt=prompt_template,\n        verbose=False,\n        memory=ConversationBufferWindowMemory(k=1),\n        llm_kwargs={'max_length': 4096}\n    )\n\n    # Store LLM generated responses\n    if 'messages' not in st.session_state.keys():\n        st.session_state.messages = [{'role': 'assistant', 'content': 'How may I help you?'}]\n\n    # Display chat messages\n    for message in st.session_state.messages:\n        with st.chat_message(message['role']):\n            st.write(message['content'])\n\n\n    # User-provided prompt\n    if prompt := st.chat_input():\n        st.session_state.messages.append({'role': 'user', 'content': prompt})\n        with st.chat_message('user'):\n            st.write(prompt)\n\n    # Generate a new response if last message is not from assistant\n    if st.session_state.messages[-1]['role'] != 'assistant':\n        with st.chat_message('assistant'):\n            with st.spinner('Thinking...'):\n                output = llama2_chain.predict(Content=prompt)\n\n                if output:\n                    # find last [/INST] and remove everything before it\n                    last_inst = output.rfind(E_INST)\n                    output = output[last_inst + len(E_INST):]\n                    try:\n                        json_match = re.search(r'\\{.*\\}', output, re.DOTALL)\n                        json_str = json_match.group()\n                        json_data = json.loads(json_str)\n\n                        # take the json_str and pretty print it\n                        pretty_json = json.dumps(json_data, indent=4, ensure_ascii=False)\n                        output = f'```json\\n{pretty_json}\\n```'\n                        st.write(output)\n                    except:\n                        output = output.replace('\\n', '\\n\\n')\n                        st.write(output)\n                else:\n                    output = 'Please try again later.'\n                    st.write(output)\n\n        message = {'role': 'assistant', 'content': output}\n        st.session_state.messages.append(message)\n",
        "file_name": "llama2_prompt_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llm_bm_tool.py": {
        "summary": "The file `llm_bm_tool.py` is a Streamlit application designed for benchmarking with a focus on comparing labels and predictions using a large language model (LLM). It contains the following key components:\n\n1. **Session State Initialization**: A function initializes various session state variables to manage user inputs, outputs, and logs.\n\n2. **Main Application Function**: The `llm_bm_tool` function sets up the app's title and sidebar for user configuration, including uploading ZIP files for labels and predictions, entering additional instructions, and processing these files.\n\n3. **File Processing**: The `process_files` function handles the extraction of uploaded ZIP files, matches label and prediction files, and sends the data to the LLM for processing. It also logs the processing steps and errors.\n\n4. **Prompt Generation**: A function generates a structured prompt for the LLM, detailing how to compare label and prediction contents.\n\n5. **Data Handling**: The application parses the LLM output, normalizes values, and calculates performance metrics (accuracy, precision, recall, F1 score) based on true positives, false positives, true negatives, and false negatives.\n\n6. **Visualization**: The app provides interactive tabs for viewing processed data, sent requests, received responses, and logs. It uses Matplotlib and Seaborn to visualize metrics and confusion matrices.\n\n7. **Error Handling and Cleanup**: The app includes error handling for processing and ensures temporary files are cleaned up after execution.\n\nOverall, this script facilitates a user-friendly interface for comparing labeled data against predictions, helping users to analyze and visualize the performance of their models effectively.",
        "content": "import streamlit as st\nfrom llm_workflow_feature import predict, create_llm_sidebar\nimport zipfile\nimport os\nimport pandas as pd\nimport json\nimport tempfile\nimport shutil\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef initialize_session_state():\n    if 'headers' not in st.session_state:\n        st.session_state.headers = None\n    if 'additional_properties' not in st.session_state:\n        st.session_state.additional_properties = None\n    if 'outputs' not in st.session_state:\n        st.session_state.outputs = []\n    if 'dataframes' not in st.session_state:\n        st.session_state.dataframes = []\n    if 'last_error_message' not in st.session_state:\n        st.session_state.last_error_message = ''\n    if 'code_to_execute' not in st.session_state:\n        st.session_state.code_to_execute = ''\n    if 'log_messages' not in st.session_state:\n        st.session_state.log_messages = []\n    if 'sent_requests' not in st.session_state:\n        st.session_state.sent_requests = []\n\n\ndef llm_bm_tool():\n    st.title(\"LLM-based Benchmarking Tool\")\n\n    # Use the sidebar to set up the URL and other settings\n    url, config, customer_name, customer_role = create_llm_sidebar(\n        default_endpoint=\"simple-llm-chat\",\n        customer_role_default=\"Admin\",\n        model_name_default=\"gpt-4o\",\n        temperature_default=0.0,\n        max_tokens_default=4096,\n        seed_default=\"random\",\n        override_instruct_prompt_default=\"{Content}\",\n        override_system_prompt_default=\"You are an assistant that processes data.\",\n    )\n\n    initialize_session_state()\n\n    # Wrap the file upload, additional instructions, and process button in a container\n    with st.container():\n        st.subheader(\"Upload Files\")\n        col1, col2 = st.columns(2)\n\n        with col1:\n            label_zip = st.file_uploader(\"Upload Labels ZIP File\", type=['zip'], key='label_zip')\n        with col2:\n            prediction_zip = st.file_uploader(\"Upload Predictions ZIP File\", type=['zip'], key='prediction_zip')\n\n        # Add a text field for additional instructions\n        st.subheader(\"Additional Instructions\")\n        user_instructions = st.text_area(\n            \"Enter any additional instructions or information to include in the prompt:\",\n            value='',\n            height=150\n        )\n\n        # Move the Process Files button here\n        if st.button(\"Process Files\"):\n            if label_zip and prediction_zip:\n                with st.spinner(\"Processing files...\"):\n                    process_files(\n                        label_zip, prediction_zip, url, config,\n                        customer_name, customer_role, user_instructions\n                    )\n            else:\n                st.error(\"Please upload both Labels and Predictions ZIP files.\")\n\n    # Define tabs here\n    tab1, tab2, tab3, tab4 = st.tabs([\n        \"Processed Output\", \"\u00f0\u0178\u201c\u00a4 Sent Requests\", \"\u00f0\u0178\u201c\u00a5 Received Responses\", \"Log\"\n    ])\n\n    # Now, fill the placeholders in the tabs\n    with tab1:\n        if st.session_state.dataframes:\n            ordered_columns = ['file_name', 'detail_name', 'label_text', 'prediction_text', 'tp', 'fp', 'fn', 'tn']\n\n            combined_df = pd.concat(st.session_state.dataframes, ignore_index=True)\n            combined_df = combined_df[\n                ordered_columns + [col for col in combined_df.columns if col not in ordered_columns]]\n\n            # Create columns for layout\n            col_left, col_right = st.columns([2, 1])\n\n            with col_left:\n                st.write(\"### Combined DataFrame\")\n                # Download button above the DataFrame\n                tsv = combined_df.to_csv(sep='\\t', index=False)\n                st.download_button(\n                    label=\"Download as TSV\",\n                    data=tsv,\n                    file_name='results.tsv',\n                    mime='text/tab-separated-values'\n                )\n                # Convert DataFrame to markdown\n                st.dataframe(combined_df, use_container_width=True)\n\n            with col_right:\n                # Add confusion matrix at the top\n                st.write(\"### Confusion Matrix\")\n                # Calculate total tp, fp, fn, tn\n                total_tp = combined_df['tp'].sum()\n                total_fp = combined_df['fp'].sum()\n                total_fn = combined_df['fn'].sum()\n                total_tn = combined_df['tn'].sum()\n\n                # Calculate metrics\n                accuracy, precision, recall, f1 = calculate_metrics(total_tp, total_fp, total_fn, total_tn)\n\n                # Create a figure with 2 rows: one for metrics and one for confusion matrix\n                fig, (ax_metrics, ax_cm) = plt.subplots(2, 1, figsize=(6, 8), gridspec_kw={'height_ratios': [1, 4]})\n\n                # Display metrics on the first subplot\n                ax_metrics.axis('off')  # Turn off the axis\n                metrics_text = (f\"Accuracy: {accuracy:.2%}\\n\"\n                                f\"Precision: {precision:.2%}\\n\"\n                                f\"Recall: {recall:.2%}\\n\"\n                                f\"F1 Score: {f1:.2%}\")\n                ax_metrics.text(0.5, 0.5, metrics_text, ha='center', va='center', fontsize=12, fontweight='bold')\n\n                cm = pd.DataFrame({\n                    'Predicted Positive': [total_tp, total_fp],\n                    'Predicted Negative': [total_fn, total_tn]\n                }, index=['Actual Positive', 'Actual Negative'])\n\n                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm)\n                ax_cm.set_title('Confusion Matrix')\n                st.pyplot(fig)\n\n                # Add bar chart for accuracy per file_name\n                st.write(\"### Accuracy per File Name\")\n                file_accuracy = combined_df.groupby('file_name').apply(\n                    lambda x: (x['tp'].sum() + x['tn'].sum()) / (\n                            x['tp'].sum() + x['fp'].sum() + x['fn'].sum() + x['tn'].sum())\n                ).reset_index(name='accuracy')\n                file_accuracy['accuracy'] = file_accuracy['accuracy'] * 100  # Convert to percentage\n\n                fig_acc, ax_acc = plt.subplots()\n                sns.barplot(x='file_name', y='accuracy', data=file_accuracy, ax=ax_acc)\n                ax_acc.set_ylabel('Accuracy (%)')\n                ax_acc.set_xlabel('File Name')\n                ax_acc.set_title('Accuracy per File Name')\n                plt.xticks(rotation=90)\n                st.pyplot(fig_acc)\n\n        else:\n            st.write(\"No data to display.\")\n\n    with tab2:\n        st.write(\"### Sent Requests\")\n        if st.session_state.sent_requests:\n            for idx, request in enumerate(st.session_state.sent_requests):\n                st.write(f\"#### Request {idx + 1}\")\n                st.json(request)\n        else:\n            st.write(\"No request data available.\")\n\n    with tab3:\n        st.write(\"### Received Responses\")\n        if st.session_state.outputs:\n            for idx, output in enumerate(st.session_state.outputs):\n                st.write(f\"#### Response {idx + 1}\")\n                st.code(output, language='json')\n        else:\n            st.write(\"No response data available.\")\n\n    with tab4:\n        st.write(\"### Log\")\n        if 'log_messages' in st.session_state:\n            st.write(\"\\n\\n\".join(st.session_state.log_messages))\n        else:\n            st.write(\"No logs available.\")\n\n\n\ndef process_files(label_zip, prediction_zip, url, config, customer_name, customer_role, user_instructions):\n    # Initialize log messages in session state\n    st.session_state.log_messages = []\n    log_messages = st.session_state.log_messages\n\n    # Reset session state variables\n    st.session_state.dataframes = []\n    st.session_state.outputs = []\n    st.session_state.sent_requests = []\n\n    log_messages.append(\"### Starting file processing...\")\n\n    # Create temporary directories\n    temp_dir = tempfile.mkdtemp()\n    label_dir = os.path.join(temp_dir, 'labels')\n    prediction_dir = os.path.join(temp_dir, 'predictions')\n    os.makedirs(label_dir, exist_ok=True)\n    os.makedirs(prediction_dir, exist_ok=True)\n    log_messages.append(f\"Temporary directories created at `{temp_dir}`\")\n\n    try:\n        # Save and extract label ZIP\n        log_messages.append(\"Extracting Labels ZIP file...\")\n        label_zip_path = os.path.join(temp_dir, 'labels.zip')\n        with open(label_zip_path, 'wb') as f:\n            f.write(label_zip.getbuffer())\n        with zipfile.ZipFile(label_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(label_dir)\n        log_messages.append(\"Labels ZIP extracted.\")\n\n        # Save and extract prediction ZIP\n        log_messages.append(\"Extracting Predictions ZIP file...\")\n        prediction_zip_path = os.path.join(temp_dir, 'predictions.zip')\n        with open(prediction_zip_path, 'wb') as f:\n            f.write(prediction_zip.getbuffer())\n        with zipfile.ZipFile(prediction_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(prediction_dir)\n        log_messages.append(\"Predictions ZIP extracted.\")\n\n        # Get list of all label files recursively\n        label_files = {}\n        for root, dirs, files in os.walk(label_dir):\n            for f in files:\n                base_name = os.path.splitext(f)[0]\n                label_files[base_name] = os.path.join(root, f)\n\n        # Get list of all prediction files recursively\n        prediction_files = {}\n        for root, dirs, files in os.walk(prediction_dir):\n            for f in files:\n                base_name = os.path.splitext(f)[0]\n                prediction_files[base_name] = os.path.join(root, f)\n\n        log_messages.append(f\"Total label files: {len(label_files)}, Total prediction files: {len(prediction_files)}\")\n\n        # Match files by basename\n        common_files = set(label_files.keys()) & set(prediction_files.keys())\n        log_messages.append(f\"Found {len(common_files)} matching files: {list(common_files)}\")\n\n        if not common_files:\n            st.error(\"No matching files found between labels and predictions.\")\n            return\n\n        # Initialize progress bar\n        progress_bar = st.progress(0)\n        total_files = len(common_files)\n        current_file = 0\n\n        # Process each file pair individually\n        dataframes = []\n\n        for base_name in common_files:\n            current_file += 1\n            progress = current_file / total_files\n            progress_bar.progress(progress)\n\n            label_file_path = label_files[base_name]\n            prediction_file_path = prediction_files[base_name]\n\n            # Read contents\n            with open(label_file_path, 'r') as f:\n                label_content = f.read()\n            with open(prediction_file_path, 'r') as f:\n                prediction_content = f.read()\n\n            # Prepare prompt\n            prompt = generate_prompt(label_content, prediction_content, user_instructions)\n\n            # Prepare additional properties for the predict function\n            config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideContent\"] = prompt\n\n            # Store the sent request\n            st.session_state.sent_requests.append(config.copy())\n\n            # Send to LLM and get response\n            output, headers = predict(\n                None, config, url, customer_name, customer_role)\n\n            if output:\n                llm_output = output.get('LlmOutput', \"No output received.\")\n                log_messages.append(f\"Received response for file {base_name}\")\n                # Log the LLM response\n                st.session_state.outputs.append(llm_output)\n            else:\n                log_messages.append(f\"No response received for file {base_name}\")\n                continue\n\n            # Store headers\n            st.session_state.headers = headers\n\n            # Parse the response into a DataFrame\n            df = parse_llm_output(llm_output)\n\n            if df is not None:\n                df['file_name'] = base_name\n                # Move 'file_name' to the first column\n                cols = df.columns.tolist()\n                cols.insert(0, cols.pop(cols.index('file_name')))\n                df = df[cols]\n\n                # Normalize values\n                df['label_text'] = df['label_text'].apply(normalize_value)\n                df['prediction_text'] = df['prediction_text'].apply(normalize_value)\n\n                # Calculate tp, fp, fn, tn for each row\n                df['tp'] = ((df['label_text'] == df['prediction_text']) & (\n                            df['label_text'] != '')).astype(int)\n                df['tn'] = ((df['label_text'] == '') & (df['prediction_text'] == '')).astype(int)\n                df['fp'] = (((df['label_text'] == '') & (df['prediction_text'] != '')) |\n                            ((df['label_text'] != df['prediction_text']) & (\n                                        df['prediction_text'] != ''))).astype(int)\n                df['fn'] = ((df['label_text'] != '') & (df['prediction_text'] == '')).astype(int)\n\n                # Ensure that each row has exactly one of tp, fp, fn, tn equal to 1\n                df[['tp', 'fp', 'fn', 'tn']] = df[['tp', 'fp', 'fn', 'tn']].astype(int)\n                dataframes.append(df)\n            else:\n                log_messages.append(f\"Failed to parse LLM output for file {base_name}\")\n\n        progress_bar.empty()  # Remove the progress bar\n\n        if dataframes:\n            combined_df = pd.concat(dataframes, ignore_index=True)\n            st.session_state.dataframes.append(combined_df)\n        else:\n            st.error(\"No dataframes were created.\")\n            log_messages.append(\"No dataframes were created.\")\n\n        log_messages.append(\"### File processing completed.\")\n\n    except Exception as e:\n        st.error(f\"An error occurred during file processing: {e}\")\n        log_messages.append(f\"An error occurred during file processing: {e}\")\n\n    finally:\n        # Clean up temporary directories\n        shutil.rmtree(temp_dir)\n        log_messages.append(\"Temporary files cleaned up.\")\n        # Store log messages in session state\n        st.session_state.log_messages = log_messages\n\n\ndef generate_prompt(label_content, prediction_content, user_instructions):\n    prompt = (\n        \"You are a data analyst. Given the label content and prediction content in JSON format, \"\n        \"extract the relevant details from both, including nested details, and compare them.\\n\\n\"\n        \"Provide the output as a JSON array with the following fields for each detail:\\n\\n\"\n        \"- detail_name: The key detail name (e.g., 'Name', 'ID number', including nested keys).\\n\"\n        \"- label_text: The formated value extracted from the label content.\\n\"\n        \"- prediction_text: The formated value extracted from the prediction content.\\n\"\n        \"- label_text_raw: The unformatted value extracted from the label content.\\n\"\n        \"- prediction_text_raw: The unformatted value extracted from the prediction content.\\n\"\n        \"**Normalize the values before writing to account for formatting differences.** \"\n        \"For example:\\n\"\n        \"- Convert numbers with commas to periods (e.g., '38,5' to '38.5').\\n\"\n        \"- Remove leading zeros from numbers (e.g., '0700' to '700').\\n\"\n        \"- Ensure dates are in the same format.\\n\"\n        \"- Treat text like 'vorhanden' and '(vorhanden)' as equal.\\n\"\n        \"- Remove extra spaces and special characters.\\n\"\n        \"- If two dates are the same, but only the year is missing, then remove the year for comparison.\\n\"\n        \"- Do not change the values itself, but use the same format for comparison.\\n\"\n        \"\\n\"\n        \"Label Content:\\n\"\n        f\"{label_content}\\n\\n\"\n        \"Prediction Content:\\n\"\n        f\"{prediction_content}\\n\\n\"\n    )\n\n    # Append user instructions if provided\n    if user_instructions.strip():\n        prompt += \"Additional instructions:\\n\\n\"\n        prompt += user_instructions.strip() + \"\\n\\n\"\n\n    # Final instructions\n    prompt += (\n        \"Provide only the JSON array as output, enclosed between <BEGIN OUTPUT> and <END OUTPUT>. \"\n        \"Do not include any explanations or additional text.\"\n    )\n\n    return prompt\n\n\ndef parse_llm_output(llm_output):\n    try:\n        # Extract JSON from the output\n        json_match = re.search(r'<BEGIN OUTPUT>\\s*(.*?)\\s*<END OUTPUT>', llm_output, re.DOTALL)\n        if json_match:\n            json_content = json_match.group(1)\n            # Parse the JSON content\n            data = json.loads(json_content)\n            df = pd.DataFrame(data)\n            return df\n        else:\n            return None\n    except Exception as e:\n        return None\n\n\ndef normalize_value(value):\n    if isinstance(value, str):\n        # Replace commas with periods in numbers\n        value = re.sub(r'(\\d+),(\\d+)', r'\\1.\\2', value)\n        # Remove leading zeros from numbers\n        value = re.sub(r'\\b0+(\\d+)\\b', r'\\1', value)\n        # Normalize specific strings\n        value = value.lower()\n        value = value.replace('(vorhanden)', 'vorhanden')\n        value = value.replace('leer', '')\n        value = value.replace('/', '')\n        value = value.replace('----', '')\n\n        value = value.replace(' ', '')\n\n        # Strip whitespace\n        value = value.strip()\n    return value\n\ndef calculate_metrics(tp, fp, fn, tn):\n    # Total samples\n    total = tp + fp + fn + tn\n\n    # Accuracy: This is the best, folks, it\u00e2\u20ac\u2122s just how well you did overall.\n    accuracy = (tp + tn) / total if total > 0 else 0\n\n    # Precision: We\u00e2\u20ac\u2122re talking about how many selected items are relevant!\n    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n\n    # Recall: That\u00e2\u20ac\u2122s your sensitivity, how many relevant items were selected.\n    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    # F1 Score: The harmonic mean of precision and recall \u00e2\u20ac\u201c simply amazing!\n    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n\n    return accuracy, precision, recall, f1\n\nif __name__ == \"__main__\":\n    llm_bm_tool()\n",
        "file_name": "llm_bm_tool.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llm_bm_tool_old.py": {
        "summary": "The file `llm_bm_tool_old.py` is a Streamlit application designed for LLM-based benchmarking. It facilitates the uploading of ZIP files containing labels and predictions, processes these files, and generates a DataFrame based on the analysis of the provided data. Key features include:\n\n1. **Session State Initialization**: The application initializes session states to manage headers, additional properties, outputs, dataframes, and error messages.\n\n2. **File Uploads**: Users can upload ZIP files containing label and prediction data. The app requires both files to process.\n\n3. **Processing Files**: Upon clicking the \"Process Files\" button, the app extracts the contents of the uploaded ZIP files, matches files based on their base names, and transforms the label contents into a standardized JSON format.\n\n4. **LLM Interaction**: The application generates prompts for an LLM to analyze the transformed data and generate Python code to create a pandas DataFrame. The code is executed in a safe environment to prevent security issues.\n\n5. **Output Display**: The app includes tabs for displaying processed outputs, sent requests, received responses, and logs. Users can download the resulting DataFrame as a TSV file.\n\n6. **Error Handling**: The application includes robust error handling throughout the processing steps, providing feedback to users when errors occur.\n\n7. **Future Enhancements**: The comments at the end of the file suggest plans to improve the user interface and data visualization, such as moving the \"Process Files\" button and adding charts like confusion matrices.\n\nOverall, the script provides a comprehensive interface for processing and analyzing benchmarking data using LLMs.",
        "content": "import streamlit as st\nfrom llm_workflow_feature import predict, create_llm_sidebar\nimport zipfile\nimport os\nimport pandas as pd\nimport json\nimport tempfile\nimport shutil\nimport re\n\ndef initialize_session_state():\n    if 'headers' not in st.session_state:\n        st.session_state.headers = None\n    if 'additional_properties' not in st.session_state:\n        st.session_state.additional_properties = None\n    if 'outputs' not in st.session_state:\n        st.session_state.outputs = []\n    if 'dataframes' not in st.session_state:\n        st.session_state.dataframes = []\n    if 'last_error_message' not in st.session_state:\n        st.session_state.last_error_message = ''\n    if 'code_to_execute' not in st.session_state:\n        st.session_state.code_to_execute = ''\n\ndef llm_bm_tool():\n    st.title(\"LLM-based Benchmarking Tool\")\n\n    # Use the sidebar to set up the URL and other settings\n    url, model, api_key, _, _, customer_name, customer_role = create_llm_sidebar(simple_sidebar=True)\n\n    initialize_session_state()\n\n    st.subheader(\"Upload Files\")\n    col1, col2 = st.columns(2)\n\n    with col1:\n        label_zip = st.file_uploader(\"Upload Labels ZIP File\", type=['zip'], key='label_zip')\n    with col2:\n        prediction_zip = st.file_uploader(\"Upload Predictions ZIP File\", type=['zip'], key='prediction_zip')\n\n    # Add a text field for additional instructions\n    st.subheader(\"Additional Instructions\")\n    user_instructions = st.text_area(\n        \"Enter any additional instructions or information to include in the prompt:\",\n        value='',\n        height=150\n    )\n\n    # Define tabs here\n    tab1, tab2, tab3, tab4 = st.tabs([\"Processed Output\", \"\u00f0\u0178\u201c\u00a4 Sent Request\", \"\u00f0\u0178\u201c\u00a5 Received Response\", \"Log\"])\n\n    with tab1:\n        pass  # Placeholder for Processed Output\n    with tab2:\n        pass  # Placeholder for Sent Request\n    with tab3:\n        pass  # Placeholder for Received Response\n    with tab4:\n        log_container = st.container()\n\n    if st.button(\"Process Files\"):\n        if label_zip and prediction_zip:\n            with st.spinner(\"Processing files...\"):\n                process_files(label_zip, prediction_zip, url, api_key, customer_name, customer_role, user_instructions, log_container)\n        else:\n            st.error(\"Please upload both Labels and Predictions ZIP files.\")\n\n    # Now, fill the placeholders in the tabs\n    with tab1:\n        if st.session_state.dataframes:\n            combined_df = pd.concat(st.session_state.dataframes, ignore_index=True)\n            st.write(\"### Combined DataFrame\")\n            st.write(combined_df)\n\n            # Download button for TSV\n            tsv = combined_df.to_csv(sep='\\t', index=False)\n            st.download_button(\n                label=\"Download as TSV\",\n                data=tsv,\n                file_name='results.tsv',\n                mime='text/tab-separated-values'\n            )\n        else:\n            st.write(\"No data to display.\")\n\n    with tab2:\n        st.write(\"### Sent Request\")\n        if st.session_state.headers and st.session_state.additional_properties:\n            st.json(st.session_state.headers)\n            st.json(st.session_state.additional_properties)\n        else:\n            st.write(\"No request data available.\")\n\n    with tab3:\n        st.write(\"### Received Response\")\n        if st.session_state.outputs:\n            last_output = st.session_state.outputs[-1]\n            st.code(last_output, language='python')\n        else:\n            st.write(\"No response data available.\")\n\ndef process_files(label_zip, prediction_zip, url, api_key, customer_name, customer_role, user_instructions, log_container):\n    log_container.write(\"### Starting file processing...\")\n\n    # Reset the DataFrame list at the beginning of a new run\n    st.session_state.dataframes = []\n\n    # Create temporary directories\n    temp_dir = tempfile.mkdtemp()\n    label_dir = os.path.join(temp_dir, 'labels')\n    prediction_dir = os.path.join(temp_dir, 'predictions')\n    os.makedirs(label_dir, exist_ok=True)\n    os.makedirs(prediction_dir, exist_ok=True)\n    log_container.write(f\"Temporary directories created at `{temp_dir}`\")\n\n    try:\n        # Save and extract label ZIP\n        log_container.write(\"Extracting Labels ZIP file...\")\n        label_zip_path = os.path.join(temp_dir, 'labels.zip')\n        with open(label_zip_path, 'wb') as f:\n            f.write(label_zip.getbuffer())\n        with zipfile.ZipFile(label_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(label_dir)\n        log_container.write(\"Labels ZIP extracted.\")\n\n        # Save and extract prediction ZIP\n        log_container.write(\"Extracting Predictions ZIP file...\")\n        prediction_zip_path = os.path.join(temp_dir, 'predictions.zip')\n        with open(prediction_zip_path, 'wb') as f:\n            f.write(prediction_zip.getbuffer())\n        with zipfile.ZipFile(prediction_zip_path, 'r') as zip_ref:\n            zip_ref.extractall(prediction_dir)\n        log_container.write(\"Predictions ZIP extracted.\")\n\n        # Get list of all label files recursively\n        label_files = {}\n        for root, dirs, files in os.walk(label_dir):\n            for f in files:\n                base_name = os.path.splitext(f)[0]\n                label_files[base_name] = os.path.join(root, f)\n\n        # Get list of all prediction files recursively\n        prediction_files = {}\n        for root, dirs, files in os.walk(prediction_dir):\n            for f in files:\n                base_name = os.path.splitext(f)[0]\n                prediction_files[base_name] = os.path.join(root, f)\n\n        log_container.write(f\"Total label files: {len(label_files)}, Total prediction files: {len(prediction_files)}\")\n\n        # Match files by basename\n        common_files = set(label_files.keys()) & set(prediction_files.keys())\n        log_container.write(f\"Found {len(common_files)} matching files: {list(common_files)}\")\n\n        if not common_files:\n            st.error(\"No matching files found between labels and predictions.\")\n            return\n\n        # Read contents and collect all data\n        all_base_names = []\n        label_contents_dict = {}  # Mapping from base_name to label content\n        prediction_contents_dict = {}  # Mapping from base_name to prediction content\n\n        for base_name in common_files:\n            label_file_path = label_files[base_name]\n            prediction_file_path = prediction_files[base_name]\n\n            # Read contents\n            with open(label_file_path, 'r') as f:\n                label_content = f.read()\n            with open(prediction_file_path, 'r') as f:\n                prediction_content = f.read()\n\n            all_base_names.append(base_name)\n            label_contents_dict[base_name] = label_content\n            prediction_contents_dict[base_name] = prediction_content\n\n        # Transform all labels upfront\n        log_container.write(\"Transforming all label files...\")\n        transformed_label_contents_dict = transform_labels(\n            label_contents_dict, api_key, url, customer_name, customer_role, log_container)\n\n        if transformed_label_contents_dict is None:\n            st.error(\"Failed to transform all labels.\")\n            return\n\n        # Now prepare examples for code generation using transformed labels\n        example_files = list(common_files)[:10]\n        log_container.write(f\"Using {len(example_files)} examples to generate code.\")\n\n        examples = []\n        for base_name in example_files:\n            examples.append({\n                'base_name': base_name,\n                'transformed_label_content': transformed_label_contents_dict[base_name],\n                'prediction_content': prediction_contents_dict[base_name]\n            })\n\n        log_container.write(\"Generating code using the selected examples.\")\n\n        # Generate the initial prompt\n        prompt = generate_prompt(examples, user_instructions=user_instructions)\n\n        # Prepare additional properties for the predict function\n        st.session_state.additional_properties = {\n            \"SystemPrompt\": \"You are an assistant that writes Python code.\",\n            \"InstructPrompt\": \"{Content}\",\n            \"Content\": prompt,\n            \"ApiKey\": api_key\n        }\n\n        # Try executing the generated code\n        max_attempts = 3\n        attempt = 0\n        code_executed = False\n        code_to_execute = None  # Initialize code_to_execute\n\n        while attempt < max_attempts and not code_executed:\n            attempt += 1\n            log_container.write(f\"Attempt {attempt} to generate and execute code...\")\n            try:\n                # Get the assistant's response\n                output, headers = predict(\n                    None, st.session_state.additional_properties, url, customer_name, customer_role)\n\n                if output:\n                    llm_output = output.get('LlmOutput', \"No output received.\")\n                    log_container.write(\"Received response from LLM.\")\n                else:\n                    llm_output = \"No response received, please try again.\"\n                    st.error(llm_output)\n                    break  # Exit the loop if no response is received\n\n                # Store outputs and headers\n                st.session_state.outputs.append(llm_output)\n                st.session_state.headers = headers\n\n                # Extract code from llm_output using custom markers\n                code_match = re.search(r'<BEGIN CODE>\\s*(.*?)\\s*<END CODE>', llm_output, re.DOTALL)\n                if code_match:\n                    code_to_execute = code_match.group(1)\n                    st.session_state.code_to_execute = code_to_execute\n                    log_container.code(code_to_execute, language='python')\n                else:\n                    st.error(\"LLM response does not contain code in expected format.\")\n                    log_container.error(\"LLM response does not contain code in expected format.\")\n                    log_container.code(llm_output)\n                    st.session_state.last_error_message = \"LLM response does not contain code in expected format.\"\n                    continue  # Try again\n\n                # Execute the generated code safely\n                log_container.write(\"Executing generated code with example data...\")\n                # Use the example data to test the code\n                test_base_names = [ex['base_name'] for ex in examples]\n                test_transformed_label_contents = [ex['transformed_label_content'] for ex in examples]\n                test_prediction_contents = [ex['prediction_content'] for ex in examples]\n\n                df = execute_generated_code(\n                    code_to_execute,\n                    test_base_names,\n                    test_transformed_label_contents,\n                    test_prediction_contents\n                )\n\n                if df is not None:\n                    log_container.write(\"DataFrame created with example data.\")\n                    # Do not append the test df to st.session_state.dataframes\n                    code_executed = True\n                else:\n                    error_message = st.session_state.last_error_message\n                    st.error(\"Failed to create DataFrame with example data.\")\n                    log_container.error(\"Failed to create DataFrame with example data.\")\n                    # Prepare prompt with the failing code and error\n                    log_container.write(\"Resending request to LLM with failing code and error message...\")\n\n                    # Generate a new prompt with the error message\n                    prompt = generate_prompt(\n                        examples, error_message, code_to_execute, user_instructions=user_instructions)\n                    st.session_state.additional_properties[\"Content\"] = prompt\n\n            except Exception as e:\n                error_message = str(e)\n                st.error(f\"An error occurred while processing example data: {error_message}\")\n                log_container.error(f\"An error occurred while processing example data: {error_message}\")\n                st.session_state.last_error_message = error_message\n                # Prepare prompt with the failing code and error\n                log_container.write(\"Resending request to LLM with failing code and error message...\")\n\n                # Generate a new prompt with the error message\n                prompt = generate_prompt(\n                    examples, error_message, code_to_execute, user_instructions=user_instructions)\n                st.session_state.additional_properties[\"Content\"] = prompt\n\n        if not code_executed:\n            st.error(\"Failed to execute code after multiple attempts.\")\n            log_container.error(\"Failed to execute code after multiple attempts.\")\n            return\n\n        log_container.write(f\"Total files to process: {len(all_base_names)}\")\n\n        # Prepare lists for code execution\n        all_transformed_label_contents = [transformed_label_contents_dict[base_name] for base_name in all_base_names]\n        all_prediction_contents = [prediction_contents_dict[base_name] for base_name in all_base_names]\n\n        # Execute the code with all data\n        df = execute_generated_code(\n            st.session_state.code_to_execute,\n            all_base_names,\n            all_transformed_label_contents,\n            all_prediction_contents\n        )\n\n        if df is not None:\n            log_container.write(\"DataFrame created for all files.\")\n            st.session_state.dataframes.append(df)\n        else:\n            st.error(\"Failed to create DataFrame for all files.\")\n            log_container.error(\"Failed to create DataFrame for all files.\")\n\n        log_container.write(\"### File processing completed.\")\n\n    except Exception as e:\n        st.error(f\"An error occurred during file processing: {e}\")\n        log_container.error(f\"An error occurred during file processing: {e}\")\n\n    finally:\n        # Clean up temporary directories\n        shutil.rmtree(temp_dir)\n        log_container.write(\"Temporary files cleaned up.\")\n\ndef transform_label(label_content, api_key, url, customer_name, customer_role, log_container):\n    # Prepare the prompt with updated instructions\n    prompt = (\n        \"You are an assistant that transforms data. \"\n        \"Given the following label content, convert it into a JSON format similar to the prediction files. \"\n        \"Ensure that the structure matches the prediction format so that they can be compared directly. \"\n        \"**Treat any occurrences of '/' or 'leer' as empty strings ('').**\\n\\n\"\n        \"Label Content:\\n\"\n        f\"{label_content}\\n\\n\"\n        \"Provide the output in JSON format, enclosed between <BEGIN JSON> and <END JSON>.\"\n    )\n\n    additional_properties = {\n        \"SystemPrompt\": \"You are an assistant that transforms data.\",\n        \"InstructPrompt\": \"{Content}\",\n        \"Content\": prompt,\n        \"ApiKey\": api_key\n    }\n\n    try:\n        output, headers = predict(\n            None, additional_properties, url, customer_name, customer_role)\n        if output:\n            llm_output = output.get('LlmOutput', \"No output received.\")\n            # Extract JSON from the output\n            json_match = re.search(r'<BEGIN JSON>\\s*(.*?)\\s*<END JSON>', llm_output, re.DOTALL)\n            if json_match:\n                transformed_label_content = json_match.group(1)\n                log_container.code(transformed_label_content, language='json')\n                return transformed_label_content\n            else:\n                log_container.error(\"LLM response does not contain JSON in expected format.\")\n                log_container.code(llm_output)\n                return None\n        else:\n            log_container.error(\"No response received from LLM.\")\n            return None\n    except Exception as e:\n        log_container.error(f\"An error occurred while transforming label: {e}\")\n        return None\n\ndef transform_labels(label_contents_dict, api_key, url, customer_name, customer_role, log_container):\n    transformed_label_contents_dict = {}\n    for base_name, label_content in label_contents_dict.items():\n        log_container.write(f\"Transforming label for file {base_name}...\")\n        transformed_label = transform_label(\n            label_content, api_key, url, customer_name, customer_role, log_container)\n        if transformed_label is not None:\n            transformed_label_contents_dict[base_name] = transformed_label\n        else:\n            log_container.error(f\"Failed to transform label for file {base_name}.\")\n            return None\n    return transformed_label_contents_dict\n\ndef execute_generated_code(code_to_execute, base_names, transformed_label_contents, prediction_contents):\n    # Preprocess code: remove import statements\n    code_to_execute = re.sub(r'^\\s*import .+$', '', code_to_execute, flags=re.MULTILINE)\n    # Replace print statements with st.write\n    code_to_execute = code_to_execute.replace('print(', 'st.write(')\n    # Set up a restricted execution environment\n    import builtins\n\n    # Create a filtered copy of __builtins__\n    safe_builtins = {}\n    for builtin_name in dir(builtins):\n        if builtin_name in ['eval', 'exec', 'compile', 'open', '__import__', 'input', 'help', 'dir']:\n            continue  # Exclude potentially unsafe built-ins\n        safe_builtins[builtin_name] = getattr(builtins, builtin_name)\n\n    # Include necessary built-ins for function definitions\n    safe_builtins['__build_class__'] = builtins.__build_class__\n    safe_builtins['globals'] = globals\n    safe_builtins['locals'] = locals\n\n    safe_globals = {\n        \"__builtins__\": safe_builtins,\n        \"re\": re,\n        \"pd\": pd,\n        \"json\": json,\n        \"base_names\": base_names,\n        \"label_contents\": transformed_label_contents,\n        \"prediction_contents\": prediction_contents\n    }\n    safe_locals = {}\n\n    try:\n        # Execute the code\n        exec(code_to_execute, safe_globals, safe_locals)\n\n        # Retrieve the DataFrame\n        df = safe_locals.get('df')\n        if df is not None and isinstance(df, pd.DataFrame):\n            return df\n        else:\n            error_message = \"Generated code did not produce a DataFrame named 'df'.\"\n            st.error(error_message)\n            st.session_state.last_error_message = error_message\n            return None\n    except Exception as e:\n        error_message = f\"Error executing generated code: {e}\"\n        st.error(error_message)\n        st.session_state.last_error_message = error_message\n        return None\n\ndef generate_prompt(examples, error_message=None, code_to_execute=None, user_instructions=''):\n    prompt = (\n        \"You are a data analyst. Given lists of base names and contents of transformed label files and prediction files, \"\n        \"both in JSON format, analyze the examples provided and generate Python code to create a pandas DataFrame. \"\n        \"The DataFrame should include relevant details extracted from the JSON data. \"\n        \"Think about what key information is present in both labels and predictions, such as 'Name', 'ID number', etc., \"\n        \"and generate code to extract those details.\\n\\n\"\n        \"The DataFrame should have the following columns:\\n\\n\"\n        \"- file_name: The base name of the files (from the list 'base_names').\\n\"\n        \"- detail_name: The key detail name (e.g., 'Name', 'ID number').\\n\"\n        \"- label: The value extracted from the label JSON content.\\n\"\n        \"- prediction: The value extracted from the prediction JSON content.\\n\"\n        \"- correct: 1 if the label and prediction values match, else 0.\\n\\n\"\n        \"The variables 'base_names', 'label_contents', and 'prediction_contents' are lists of strings, \"\n        \"where each string contains the JSON content of a file.\\n\\n\"\n        \"Check the Label and the Prediction Content on how to extract and \"\n        \"match the details and also nested details for each. \"\n        \"Your task is to write Python code to process the data and create the DataFrame 'df'.\\n\\n\"\n    )\n\n    if error_message and code_to_execute:\n        prompt += (\n            f\"You provided the following code which failed to execute:\\n\\n\"\n            f\"<BEGIN CODE>\\n{code_to_execute}\\n<END CODE>\\n\\n\"\n            f\"It failed with the following error:\\n\\n\"\n            f\"{error_message}\\n\\n\"\n            \"Please correct the code so that it works with the given lists of data.\\n\\n\"\n        )\n\n    prompt += \"Here are examples of the contents of the files:\\n\\n\"\n\n    for i, example in enumerate(examples, start=1):\n        prompt += f\"Example {i}:\\nBase Name: {example['base_name']}\\n\"\n        prompt += f\"Transformed Label Content (JSON):\\n{example['transformed_label_content']}\\n\\n\"\n        prompt += f\"Prediction File Content (JSON):\\n{example['prediction_content']}\\n\\n\"\n\n    # Append user instructions if provided\n    if user_instructions.strip():\n        prompt += \"Additional instructions:\\n\\n\"\n        prompt += user_instructions.strip() + \"\\n\\n\"\n\n    prompt += (\n        \"Provide only the Python code to process the lists 'base_names', 'label_contents', and 'prediction_contents', \"\n        \"and create the DataFrame 'df'. Do not include any explanations. \"\n        \"Do not define 'base_names', 'label_contents', or 'prediction_contents'; they are already provided. \"\n        \"Do not include any file reading code and do not include any import statements. \"\n        \"Assume that all needed modules are already imported.\"\n        \"Enclose the code between <BEGIN CODE> and <END CODE>.\"\n    )\n\n    return prompt\n\nif __name__ == \"__main__\":\n    llm_bm_tool()\n\n\n\"\"\"\nProcess Button Position: Moved the \"Process Files\" button to be above the tabs, directly under the \"Additional Instructions\" field.\nmaybe make a container around the upload files, additonal instructions and the process button?\n\nDataFrame Display: Displayed the DataFrame as markdown using st.markdown() with code formatting for better readability.\n\nmake a Confusion Matrix for the dataframe, if we need any more info, then adjust the building of the df in the prompt to include that info\n\nPlot the counts for each unique detail_name using a bar chart\n\"\"\"",
        "file_name": "llm_bm_tool_old.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llm_chat_feature.py": {
        "summary": "The file `llm_chat_feature.py` is a Streamlit application that implements a chatbot interface powered by a large language model (LLM). It initializes session states for conversation history, configuration, output, and memory. The application allows users to submit messages, which are processed and responded to by the LLM. Key functionalities include:\n\n1. **Session State Management**: Initializes and manages various states like conversation history, memory, headers, and output.\n2. **Memory and Conversation Controls**: Provides functions to clear memory, clear conversation, or clear everything, facilitating user control over the session.\n3. **Text Processing**: Includes functions to split long user input into manageable chunks and build prompts for the LLM based on the user input and session state.\n4. **User Input Handling**: Processes user input, checks for length, and manages LLM interactions, including sending requests and handling responses.\n5. **Display Functions**: Renders the conversation history, memory, and request/response data in a user-friendly format within the Streamlit app.\n6. **UI Elements**: Incorporates various Streamlit components for user interaction, like text areas for input, buttons for clearing states, and tabs for organizing output display.\n\nOverall, the script sets up an interactive chatbot that engages users through a structured conversation flow while managing context and response generation efficiently.",
        "content": "\nimport streamlit as st\nfrom llm_helpers import predict, create_llm_sidebar\nimport re\n\n\ndef initialize_session_state():\n    if 'conversation' not in st.session_state:\n        st.session_state.conversation = []\n    if 'headers' not in st.session_state:\n        st.session_state.headers = None\n    if 'config' not in st.session_state:\n        st.session_state.config = None\n    if 'output' not in st.session_state:\n        st.session_state.output = None\n    if 'memory' not in st.session_state:\n        st.session_state.memory = ''\n\n\ndef handle_clear_memory():\n    st.session_state.memory = ''\n    st.experimental_rerun()\n\n\ndef handle_clear_conversation():\n    st.session_state.conversation = []\n    st.experimental_rerun()\n\n\ndef handle_clear_everything():\n    st.session_state.memory = ''\n    st.session_state.conversation = []\n    st.experimental_rerun()\n\n\ndef split_text_into_chunks(text, max_words):\n    words = text.split()\n    chunks = []\n    for i in range(0, len(words), max_words):\n        chunk = ' '.join(words[i:i + max_words])\n        chunks.append(chunk)\n    return chunks\n\n\ndef get_first_last_n_words(text, n):\n    words = text.split()\n    if len(words) <= 2 * n:\n        return text\n    else:\n        return \" \".join(words[:n]), \" \".join(words[-n:])\n\n\ndef build_prompt(user_input):\n    prompt_components = []\n\n    if st.session_state.memory:\n        prompt_components.append(f\"==>Memory:{st.session_state.memory}\\n\")\n\n    prompt_components.append(f\"==>New User Input:\\n{user_input}\\n\")\n\n    max_tokens = 128000\n    max_tokens = int(max_tokens / 2.59)\n    token_count = sum(len(component.split()) for component in prompt_components)\n\n    reversed_conversation = st.session_state.conversation[::-1]\n    included_messages = []\n\n    for message in reversed_conversation:\n        if message['role'] == 'user' and message['content'] == user_input:\n            continue\n\n        message_text = f\"{message['role'].capitalize()}:\\n{message['content']}\\n\"\n        message_tokens = len(message_text.split())\n\n        if token_count + message_tokens <= max_tokens:\n            included_messages.append(message_text)\n            token_count += message_tokens\n        else:\n            break\n\n    print(f\"Used Tokens: {token_count}/{max_tokens}\")\n    included_messages = included_messages[::-1]\n    conversation_history = f\"Conversation History:\\n{''.join(included_messages)}\"\n    full_prompt = f\"==>{conversation_history}\" + f\"{''.join(prompt_components)}\"\n\n    return full_prompt\n\n\ndef handle_send_action(user_input, url, config, customer_name, customer_role):\n    user_input = user_input.strip()\n\n    if user_input:\n        max_tokens = 128000\n        max_tokens = int(max_tokens / 2.59)\n        reserved_tokens = 5000\n        max_input_tokens = max_tokens - reserved_tokens\n        user_input_tokens = len(user_input.split())\n\n        # Save the config in session state for consistency and debugging purposes\n        st.session_state.config = config\n\n        if user_input_tokens > max_input_tokens:\n            print(\"Use Chunking\")\n            original_system_prompt = config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideSystemPrompt\"]\n            chunk_sytem_prompt = \"You are an advanced LLM chatbot ready to assist the user. Please provide a concise summary of the information so far or fulfill what the user requested.\"\n            config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideSystemPrompt\"] = chunk_sytem_prompt\n\n            progress_bar = st.progress(0)\n            max_chunk_size = max_input_tokens // 2\n            chunks = split_text_into_chunks(user_input, max_chunk_size)\n            summaries = []\n\n            N = 500\n            # first_n_words, last_n_words = get_first_last_n_words(user_input, N)\n\n            st.write(\"The input is too long and will be split into chunks.\")\n            st.write(f\"Number of Chunks: {len(chunks)}\")\n            st.write(f\"Changed System Prompt temporarily for chunking to '{chunk_sytem_prompt}'\")\n\n            for i, chunk in enumerate(chunks):\n                print(f\"Chunk {i + 1}/{len(chunks)}\")\n                progress_bar.progress((i + 1) / len(chunks))\n\n                prompt_components = []\n                if st.session_state.memory:\n                    prompt_components.append(f\"==>Memory:{st.session_state.memory}\\n\")\n                # prompt_components.append(f\"==>First Part of User Input:\\n{first_n_words}\\n\")\n                prompt_components.append(f\"==>New User Input Chunk:\\n{chunk}\\n\")\n                # prompt_components.append(f\"==>Last Part of User Input:\\n{last_n_words}\\n\")\n                prompt_components.append(\n                    \"Please provide a concise summary of the information so far or fulfill what the user requested.\\n\")\n                full_prompt = ''.join(prompt_components)\n\n                # Update OverrideContent within config for the current chunk\n                config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideContent\"] = full_prompt\n\n                try:\n                    output, headers = predict(\n                        None, config, url, customer_name, customer_role)\n                    summary = output.get('LlmOutput', \"No output received.\")\n                    summaries.append(summary.strip())\n                    st.session_state.headers = headers\n                    st.session_state.output = output\n                except Exception as e:\n                    st.session_state.conversation.append({'role': 'assistant', 'content': f\"An error occurred: {e}\"})\n                    st.write(f\"An error occurred: {e}\")\n                    st.session_state.output = {'error': str(e)}\n                    st.session_state.headers = None\n\n            final_prompt_components = []\n            if st.session_state.memory:\n                final_prompt_components.append(f\"==>Memory:{st.session_state.memory}\\n\")\n            # final_prompt_components.append(f\"==>First Part of User Input:\\n{first_n_words}\\n\")\n            final_prompt_components.append(\"==>Summaries of Chunks:\\n\")\n            for i, summary in enumerate(summaries):\n                final_prompt_components.append(f\"Summary of next Chunk:\\n{summary}\\n\")\n            # final_prompt_components.append(f\"==>Last Part of User Input:\\n{last_n_words}\\n\")\n            final_prompt_components.append(\"Please fulfill what the user requested in his input.\\n\")\n            full_prompt = ''.join(final_prompt_components)\n\n            # Update OverrideContent with the final prompt\n            config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideSystemPrompt\"] = original_system_prompt\n            config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideContent\"] = full_prompt\n\n            try:\n                st.session_state.output, st.session_state.headers = predict(\n                    None, config, url, customer_name, customer_role)\n                if st.session_state.output:\n                    llm_output = st.session_state.output.get('LlmOutput', \"No output received.\")\n                else:\n                    llm_output = \"No response received, please try again.\"\n\n                memory_match = re.search(r'\\{\\{\\{Memory:(.*?)\\}\\}\\}', llm_output, re.DOTALL)\n                if memory_match:\n                    st.session_state.memory += ' ' + memory_match.group(1).strip()\n                    llm_output = llm_output.replace(memory_match.group(0), '').strip()\n\n                st.session_state.conversation.append({'role': 'assistant', 'content': llm_output})\n\n            except Exception as e:\n                llm_output = f\"An error occurred: {e}\"\n                st.session_state.conversation.append({'role': 'assistant', 'content': llm_output})\n                st.session_state.output = {'error': str(e)}\n                st.session_state.headers = None\n\n            st.experimental_rerun()\n\n        else:\n            st.session_state.conversation.append({'role': 'user', 'content': user_input})\n            full_prompt = build_prompt(user_input)\n\n            # Set final prompt as OverrideContent in config\n            config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideContent\"] = full_prompt\n\n            try:\n                st.session_state.output, st.session_state.headers = predict(\n                    None, config, url, customer_name, customer_role)\n\n                if st.session_state.output:\n                    llm_output = st.session_state.output.get('LlmOutput', \"No output received.\")\n                else:\n                    llm_output = \"No response received, please try again.\"\n\n                memory_match = re.search(r'\\{\\{\\{Memory:(.*?)\\}\\}\\}', llm_output, re.DOTALL)\n                if memory_match:\n                    st.session_state.memory += ' ' + memory_match.group(1).strip()\n                    llm_output = llm_output.replace(memory_match.group(0), '').strip()\n\n                st.session_state.conversation.append({'role': 'assistant', 'content': llm_output})\n\n            except Exception as e:\n                llm_output = f\"An error occurred: {e}\"\n                st.session_state.conversation.append({'role': 'assistant', 'content': llm_output})\n                st.session_state.output = {'error': str(e)}\n                st.session_state.headers = None\n\n            st.experimental_rerun()\n\n\ndef display_conversation():\n    if st.session_state.conversation:\n        for message in reversed(st.session_state.conversation):\n            with st.chat_message(message[\"role\"]):\n                st.write(message[\"content\"])\n\n\ndef display_memory():\n    st.markdown(\"### Memory\")\n    if st.session_state.memory:\n        st.markdown(\n            f\"<div style='padding: 10px; border: 1px solid #ccc; border-radius: 5px;'>\"\n            f\"{st.session_state.memory}</div>\",\n            unsafe_allow_html=True)\n    else:\n        st.write(\"No memory yet.\")\n\n\ndef llm_chat_feature():\n    st.title(\"BluDelta AzureGPT ChatBot\")\n\n    system_prompt = \"You are an advanced LLM chatbot ready to assist the user. At the beginning of your response, provide a list of important infos of the conversation as Memory in the format '{{{Memory: ...}}}'. Only include new information. Do not include any other text within '{{{ }}}'. After that write the response for the user.\"\n    instruct_prompt = \"{Content}\"\n\n    url, config, customer_name, customer_role = create_llm_sidebar(\n        default_endpoint=\"simple-llm-chat\",\n        customer_role_default=\"Admin\",\n        model_name_default=\"gpt-4o\",\n        temperature_default=0.7,\n        max_tokens_default=4096,\n        seed_default=\"random\",\n        override_instruct_prompt_default=instruct_prompt,\n        override_system_prompt_default=system_prompt,\n    )\n\n    initialize_session_state()\n\n    with st.form(key='input_form', clear_on_submit=True):\n        user_input = st.text_area(\"Enter your message, send with CTRL + Enter:\", value='', height=150)\n        instruct_append = st.text_area(\"Append to instruction prompt:\", value='', height=120)\n        send_clicked = st.form_submit_button(\"Send\")\n\n    _, col1, col2, col3 = st.columns([8, 1, 1, 1])\n    with col1:\n        clear_memory_clicked = st.button(\"Clear Memory\")\n    with col2:\n        clear_conversation_clicked = st.button(\"Clear Conversation\")\n    with col3:\n        clear_everything_clicked = st.button(\"Clear Both\")\n\n    if clear_memory_clicked:\n        handle_clear_memory()\n\n    if clear_conversation_clicked:\n        handle_clear_conversation()\n\n    if clear_everything_clicked:\n        handle_clear_everything()\n\n    if send_clicked:\n        config[\"LLMConfig\"][\"LLMFeatureOverrides\"][\"OverrideSystemPrompt\"] += instruct_append\n        handle_send_action(user_input, url, config, customer_name, customer_role)\n\n    tab1, tab2, tab3 = st.tabs([\"Processed Output\", \"\u00f0\u0178\u201c\u00a4 Sent Request\", \"\u00f0\u0178\u201c\u00a5 Received Response\"])\n\n    with tab1:\n        conversation_col, memory_col = st.columns([3, 1])\n        with conversation_col:\n            display_conversation()\n        with memory_col:\n            display_memory()\n\n    with tab2:\n        st.write(\"### Sent Request Data\")\n        if st.session_state.headers is not None:\n            st.json(st.session_state.headers)\n        else:\n            st.write(\"No headers available.\")\n\n        if st.session_state.config is not None:\n            st.json(st.session_state.config)\n        else:\n            st.write(\"No config available.\")\n\n    with tab3:\n        st.write(\"### Raw Response Data\")\n        if st.session_state.output is not None:\n            st.write(st.session_state.output)\n        else:\n            st.write(\"No response data available.\")\n\n\nif __name__ == \"__main__\":\n    llm_chat_feature()\n",
        "file_name": "llm_chat_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llm_helpers.py": {
        "summary": "The file `llm_helpers.py` contains helper functions for a Streamlit application designed to interface with a language model (LLM) API. Below is a concise summary of its key components:\n\n1. **Imports**: The script imports necessary libraries, including `requests`, `requests_toolbelt`, `os`, `streamlit`, `json`, and `random`.\n\n2. **Function `create_llm_sidebar`**: \n   - This function creates a sidebar in the Streamlit app for user input related to an LLM configuration.\n   - It allows users to specify various settings such as the base URL, API endpoint, customer information, and LLM provider configurations (including API keys and model names).\n   - Users can configure LLM feature overrides (like enabling images), parameter configuration (like temperature and max tokens), and OCR configuration.\n   - The function constructs a final URL based on user inputs and initializes a configuration dictionary.\n\n3. **Function `predict`**: \n   - This function sends requests to the specified workflow endpoint using the provided configuration and optional file input.\n   - It prepares a multipart request containing the configuration in JSON format and any uploaded file(s).\n   - The function handles the response, including success and error states, and returns the response data if successful.\n\nOverall, the script facilitates user interaction for configuring LLM-related tasks and making API requests in a structured manner.",
        "content": "import requests\nimport requests_toolbelt\nimport os\nimport streamlit as st\nimport json\nimport random\n\n\n# \"http://192.168.137.106:8080\")  # http://192.168.137.80:8082\ndef create_llm_sidebar(\n        base_url_default=\"http://192.168.137.80:8082\",\n        default_endpoint=\"document\",\n        customer_name_default=\"default-customer\",\n        customer_role_default=\"User\",\n        llm_provider_default=\"Default\",\n        supports_image_processing_default=\"Default\",\n        api_key_name_default=\"\",\n        api_key_default=\"\",\n        resource_name_default=\"\",\n        deployment_id_default=\"\",\n        model_name_default=\"\",\n        api_version_default=\"\",\n        temperature_default=None,\n        seed_default=None,\n        max_tokens_default=None,\n        top_p_default=None,\n        frequency_penalty_default=None,\n        presence_penalty_default=None,\n        ocr_engine_default=\"Default\",\n        is_return_with_images_default=\"Default\",\n        max_pages_default=None,\n        as_plain_text_default=\"Default\",\n        as_plain_text_per_page_default=\"Default\",\n        override_instruct_prompt_default=\"\",\n        override_system_prompt_default=\"\"\n):\n    with st.sidebar:\n        st.title('Settings')\n\n        # Workflow URL Section\n        st.subheader('Workflow URL')\n        base_url = st.text_input(\"Enter Base URL\", base_url_default)\n        predefined_endpoints = [\n            'document',\n            'llm-workflow',\n            'generic-llm-workflow',\n            'lmdx',\n            'simple-llm-chat',\n            'order-confirmation',\n            'quotation',\n            'receipt',\n            'doc-splitter',\n        ]\n\n        if default_endpoint not in predefined_endpoints:\n            predefined_endpoints.insert(0, default_endpoint)\n\n        selected_endpoint = st.selectbox(\n            \"Select an API Endpoint or enter below\",\n            predefined_endpoints,\n            index=predefined_endpoints.index(default_endpoint)\n        )\n\n        custom_endpoint = st.text_input('Enter custom Endpoint', selected_endpoint)\n        url = f\"{base_url}/{custom_endpoint}\"\n\n        if custom_endpoint == 'document':\n            selected_document_type = st.selectbox(\n                'Select a predefined document type or enter below',\n                ['Default', 'timesheet', 'invoice', 'receipt', 'delivery-note']\n            )\n            document_type = st.text_input('Enter Document Type', selected_document_type)\n            if document_type and document_type != 'Default':\n                url += f\"?document-type={document_type}\"\n\n        st.write(f\"Final URL: {url}\")\n\n        # Initialize the configuration dictionary\n        config = {\n            \"OcrConfig\": {},\n            \"LLMConfig\": {\n                \"LLMProviderConfig\": {\n                    \"Settings\": {}\n                },\n                \"LLMFeatureOverrides\": {},\n                \"LLMParameterConfig\": {}\n            }\n        }\n\n        # Customer Information Section\n        st.subheader(\"Customer Information\")\n        customer_name = st.text_input('Enter Customer Name', customer_name_default)\n        customer_role = st.selectbox('Select Customer Role', [customer_role_default, 'Admin'])\n\n        # LLM Provider Configuration Section\n        with st.expander(\"LLM Provider Configuration\"):\n            llm_provider = st.selectbox(\"Select LLM Provider\", [\"Default\", \"AzureGPT\", \"OpenAI\", \"Claude\", \"Mistral\"],\n                                        index=[\"Default\", \"AzureGPT\", \"OpenAI\", \"Claude\", \"Mistral\"].index(llm_provider_default))\n            supports_image_processing = st.selectbox(\"LLM Supports Image Processing\", [\"Default\", True, False],\n                                                     index=[\"Default\", True, False].index(supports_image_processing_default))\n            api_key_name = st.text_input(\"API Key Name\", api_key_name_default)\n            api_key = st.text_input(\"API Key\", api_key_default, type=\"password\")\n\n            st.markdown(\"**Settings**\")\n            resource_name = st.text_input(\"Resource Name\", resource_name_default)\n            deployment_id = st.text_input(\"Deployment ID\", deployment_id_default)\n            model_name = st.text_input(\"Model Name\", model_name_default)\n            api_version = st.text_input(\"API Version\", api_version_default)\n\n            llm_provider = llm_provider if llm_provider != 'Default' else None\n            supports_image_processing = None if supports_image_processing == 'Default' else supports_image_processing\n            api_key_name = api_key_name or None\n            api_key = api_key or None\n            resource_name = resource_name or None\n            deployment_id = deployment_id or None\n            model_name = model_name or None\n            api_version = api_version or None\n\n            config[\"LLMConfig\"][\"LLMProviderConfig\"] = {\n                \"Name\": llm_provider,\n                \"SupportsImageProcessing\": supports_image_processing,\n                \"ApiKeyName\": api_key_name,\n                \"ApiKey\": api_key,\n                \"Settings\": {\n                    \"ResourceName\": resource_name,\n                    \"DeploymentId\": deployment_id,\n                    \"ModelName\": model_name,\n                    \"ApiVersion\": api_version\n                }\n            }\n\n        # LLM Feature Overrides Section\n        with st.expander(\"LLM Feature Overrides\"):\n            enable_images = st.selectbox(\"Enable Images\", [\"Default\", True, False])\n            enable_text = st.selectbox(\"Enable Text\", [\"Default\", True, False])\n            override_content = st.text_area(\n                \"Override Content. Replaces '{Content}' in Instruct Prompt\", value=\"\")\n            override_instruct_prompt = st.text_area(\"Override Instruct Prompt\", value=override_instruct_prompt_default)\n            override_system_prompt = st.text_area(\"Override System Prompt\", value=override_system_prompt_default)\n\n            enable_images = None if enable_images == 'Default' else enable_images\n            enable_text = None if enable_text == 'Default' else enable_text\n            override_content = override_content or None\n            override_instruct_prompt = override_instruct_prompt or None\n            override_system_prompt = override_system_prompt or None\n\n            config[\"LLMConfig\"][\"LLMFeatureOverrides\"] = {\n                \"EnableImages\": enable_images,\n                \"EnableText\": enable_text,\n                \"OverrideOcrTextContent\": override_content,\n                \"OverrideInstructPrompt\": override_instruct_prompt,\n                \"OverrideSystemPrompt\": override_system_prompt\n            }\n\n        # LLM Parameter Configuration Section\n        with st.expander(\"LLM Parameter Configuration\"):\n            temperature = st.text_input(\"Temperature\", value=str(temperature_default) if temperature_default is not None else \"\")\n            seed_value = random.randint(-2 ** 31, 2 ** 31 - 1) if seed_default == 'random' else seed_default\n            seed = st.text_input(\"Seed\", value=int(seed_value) if seed_value is not None else 42)\n            max_tokens = st.text_input(\"Max Tokens\", value=str(max_tokens_default) if max_tokens_default is not None else \"\")\n            top_p = st.text_input(\"Top P\", value=str(top_p_default) if top_p_default is not None else \"\")\n            frequency_penalty = st.text_input(\"Frequency Penalty\", value=str(frequency_penalty_default) if frequency_penalty_default is not None else \"\")\n            presence_penalty = st.text_input(\"Presence Penalty\", value=str(presence_penalty_default) if presence_penalty_default is not None else \"\")\n\n            config[\"LLMConfig\"][\"LLMParameterConfig\"] = {\n                \"Temperature\": float(temperature) if temperature else None,\n                \"Seed\": int(seed) if seed else None,\n                \"MaxTokens\": int(max_tokens) if max_tokens else None,\n                \"TopP\": float(top_p) if top_p else None,\n                \"FrequencyPenalty\": float(frequency_penalty) if frequency_penalty else None,\n                \"PresencePenality\": float(presence_penalty) if presence_penalty else None\n            }\n\n        # OCR Configuration Section\n        with st.expander(\"OCR Configuration\"):\n            ocr_engine = st.selectbox(\"Select OCR Engine\", [\"Default\", \"AzureRead\", \"Nuance\"],\n                                      index=[\"Default\", \"AzureRead\", \"Nuance\"].index(ocr_engine_default))\n            is_return_with_images = st.selectbox(\"Return with Images\", [\"Default\", True, False],\n                                                 index=[\"Default\", True, False].index(is_return_with_images_default))\n            max_pages = st.text_input(\"Max Pages\", value=str(max_pages_default) if max_pages_default is not None else \"\")\n            as_plain_text = st.selectbox(\"As Plain Text\", [\"Default\", True, False],\n                                         index=[\"Default\", True, False].index(as_plain_text_default))\n            as_plain_text_per_page = st.selectbox(\"As Plain Text Per Page\", [\"Default\", True, False],\n                                                  index=[\"Default\", True, False].index(as_plain_text_per_page_default))\n\n            config[\"OcrConfig\"] = {\n                \"OcrEngine\": ocr_engine if ocr_engine != 'Default' else None,\n                \"IsReturnWithImages\": None if is_return_with_images == 'Default' else is_return_with_images,\n                \"MaxPages\": int(max_pages) if max_pages.strip() else None,\n                \"AsPlainText\": None if as_plain_text == 'Default' else as_plain_text,\n                \"AsPlainTextPerPage\": None if as_plain_text_per_page == 'Default' else as_plain_text_per_page\n            }\n\n        st.subheader(\"Operation Data\")\n        customer_role = st.selectbox(\"Select Operation Data\", [\"Default\", \"True\", \"False\"])\n        if customer_role == \"True\":\n            config[\"PropertyStore\"] = {\"OperationData.All\": \"true\"}\n\n        return url, config, customer_name, customer_role\n\n\n\n# @st.cache_data(show_spinner=False)\ndef predict(file_path, config, workflow_endpoint, customer_name, customer_role):\n    headers = {\n        'X-ApiKey': 'YOUR_API_KEY',\n        'X-CustomerName': customer_name,\n        'X-Role': customer_role,\n        'User-Agent': 'Bludelta AI Workflow Service Client',\n        # Content-Type will be set by MultipartEncoder\n    }\n\n    def clean_config(d):\n        if isinstance(d, dict):\n            return {k: clean_config(v) for k, v in d.items() if v is not None}\n        else:\n            return d\n\n    cleaned_config = clean_config(config)\n    payload = json.dumps(cleaned_config)\n\n    # Prepare the fields for the multipart request\n    fields = {\n        'json': ('json_data', payload, 'application/json'),  # JSON part\n    }\n\n    # If there is a file, add the file field\n    if file_path:\n        file_name = os.path.basename(file_path)\n        print(f\"Processing file {file_name}\")\n\n        with open(file_path, 'rb') as file:\n            fields['files'] = (file_name, file, 'application/octet-stream')\n\n            m = requests_toolbelt.MultipartEncoder(fields=fields)\n            headers['Content-Type'] = m.content_type  # Set Content-Type to multipart/form-data\n\n            response = requests.post(workflow_endpoint, data=m, headers=headers)\n    else:\n        print(\"Processing without file\")\n\n        # Create MultipartEncoder without the file field\n        m = requests_toolbelt.MultipartEncoder(fields=fields)\n        headers['Content-Type'] = m.content_type  # Set Content-Type to multipart/form-data\n\n        response = requests.post(workflow_endpoint, data=m, headers=headers)\n\n    if response.status_code == 200:\n        print(\"Successfully processed request\")\n        return response.json(), headers\n    else:\n        print(f\"Failed to process request. Status code: {response.status_code}\")\n        return None, headers\n",
        "file_name": "llm_helpers.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llm_workflow_feature.py": {
        "summary": "The file `llm_workflow_feature.py` is a Streamlit application designed to facilitate the upload and processing of various document types (PDFs, images, and scripts) or ZIP files containing multiple documents. It utilizes a sidebar for configuration settings and provides a user interface for file upload and processing.\n\nKey functionalities include:\n\n1. **File Upload**: Users can upload single files or ZIP files containing multiple documents. The application supports various file types such as PDF, PNG, JPEG, TIFF, and PowerShell scripts.\n\n2. **Processing Logic**: Once a file is uploaded and the \"Send\" button is clicked, the application processes the input:\n   - For ZIP files, it reads the contents, processes each valid file, and returns results in a new ZIP file.\n   - For single files, it extracts the contents, performs predictions via a helper function, and displays the processed output.\n\n3. **Output Handling**: The processed results are displayed using tabs, which include:\n   - A tab for the processed output in a pretty-printed JSON format.\n   - A tab showing the request data sent to the API.\n   - A tab for raw response data.\n\n4. **Downloading Results**: Users can download both the processed results as a ZIP file and the output as JSON.\n\n5. **Temporary File Management**: The application manages temporary files and directories, ensuring cleanup after processing to maintain a clean environment.\n\n6. **Error Handling**: Basic error handling is implemented to provide feedback in case of processing failures.\n\nThe script makes use of various libraries like `pandas` for data manipulation, `PIL` for image handling, and `zipfile` for managing ZIP archives. Additionally, it includes a helper function for flattening nested JSON structures to present the data in a tabular format.",
        "content": "import os\nimport streamlit as st\n\nimport tempfile\nimport json\nimport re\nimport pandas as pd\nfrom PIL import Image\nimport shutil\nimport zipfile\nfrom io import BytesIO\nfrom llm_helpers import predict, create_llm_sidebar\n\nTEMP_DIR = 'tmp'\nALLOWED_FILE_TYPES = ['pdf', 'png', 'jpg', 'jpeg', 'tiff', 'ps1']\n\n\ndef llm_workflow_feature():\n    url, api_config, customer_name, customer_role = create_llm_sidebar()\n\n    # Upload a document or a ZIP file\n    document_file = st.file_uploader('Upload a document or ZIP file.',\n                                     type=['pdf', 'png', 'jpg', 'jpeg', 'tiff', 'ps1', 'zip'])\n\n    # Add a Send button\n    send_button = st.button(\"Send\")\n\n    try:\n        if send_button:\n            if document_file is not None:\n                if document_file.name.endswith(\".zip\"):\n                    # Handle ZIP file upload\n                    handle_zip_file(document_file, api_config, url, customer_name, customer_role)\n                else:\n                    # Handle single file upload\n                    handle_single_file(document_file, api_config, url, customer_name, customer_role)\n            else:\n                # Handle request without file\n                handle_request_without_file(api_config, url, customer_name, customer_role)\n\n    except Exception as e:\n        st.write(f\"Error: {e}\")\n        print(f\"Error:{e.__traceback__}\\n{e}\")\n\n    cleanup_temp_dir(TEMP_DIR)\n\n\ndef handle_request_without_file(api_config, url, customer_name, customer_role):\n    # Since no file is uploaded, set file_path to None\n    file_path = None\n\n    output, headers = predict(file_path, api_config, url, customer_name, customer_role)\n    display_processed_output(output, None, headers, api_config)\n\n\ndef handle_single_file(document_file, api_config, url, customer_name, customer_role):\n    # Process a single file (existing functionality)\n    file_path = os.path.join(TEMP_DIR, document_file.name)\n\n    # Must write the file temporarily to disk\n    with open(file_path, \"wb\") as f:\n        f.write(document_file.getbuffer())\n\n    output, headers = predict(file_path, api_config, url, customer_name, customer_role)\n    display_processed_output(output, document_file, headers, api_config)\n\n\n@st.cache_data\ndef process_zip_file(zip_file_content, api_config, url, customer_name, customer_role):\n    # Use BytesIO to read the zip file\n    with zipfile.ZipFile(BytesIO(zip_file_content), 'r') as zip_ref:\n        # Process files as before\n        # Instead of extracting files to disk, we can process files in memory\n        valid_files = [file for file in zip_ref.namelist() if file.split('.')[-1].lower() in ALLOWED_FILE_TYPES]\n\n        total_files = len(valid_files)\n        processed_json_files = []\n        progress_bar = st.progress(0)\n\n        # Process files and update progress\n        for i, file_name in enumerate(valid_files, start=1):\n            with zip_ref.open(file_name) as file:\n                file_bytes = file.read()\n                # Process file_bytes\n                # Save to temp file if necessary\n                with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n                    temp_file.write(file_bytes)\n                    temp_file.flush()\n                    temp_file_path = temp_file.name\n\n                output, headers = predict(temp_file_path, api_config, url, customer_name, customer_role)\n\n                # Remove temp file\n                os.unlink(temp_file_path)\n\n                if output:\n                    pretty_json, _ = process_output(output)\n                    json_filename = f\"{os.path.splitext(os.path.basename(file_name))[0]}.json\"\n                    processed_json_files.append((json_filename, pretty_json.encode('utf-8')))\n\n            # Update progress\n            progress_bar.progress(i / total_files)\n\n    # Create ZIP of processed JSON files in memory\n    result_zip_buffer = BytesIO()\n    with zipfile.ZipFile(result_zip_buffer, 'w') as zipf:\n        for json_filename, json_data in processed_json_files:\n            zipf.writestr(json_filename, json_data)\n    result_zip_buffer.seek(0)\n    return result_zip_buffer\n\n\ndef process_output(predict_output):\n    if 'LlmOutput' in predict_output:\n        llm_output = predict_output['LlmOutput']\n        # Clean up the output\n        llm_output = llm_output.replace('\\n', '').replace('```json', '').replace('```', '')\n        json_data = json.loads(llm_output)\n    elif isinstance(predict_output, dict) or isinstance(predict_output, list):\n        json_data = predict_output\n    else:\n        json_match = re.search(r'\\{.*\\}', predict_output, re.DOTALL)\n        json_str = json_match.group()\n        json_data = json.loads(json_str)\n\n    pretty_json = json.dumps(json_data, indent=4, ensure_ascii=False)\n    return pretty_json, json_data\n\n\ndef handle_zip_file(zip_file, additional_properties, url, customer_name, customer_role):\n    # Read zip file content\n    zip_file_content = zip_file.getbuffer().tobytes()\n\n    # Call the cached processing function\n    result_zip_buffer = process_zip_file(zip_file_content, additional_properties, url, customer_name, customer_role)\n\n    # Provide download button for the ZIP containing results\n    st.download_button(\n        label=\"Download Processed Results ZIP\",\n        data=result_zip_buffer,\n        file_name=\"processed_results.zip\",\n        mime=\"application/zip\"\n    )\n\n\ndef display_processed_output(output, document_file, headers, additional_properties):\n    # Create tabs for the default view, sent request, and received response\n    tab1, tab3, tab4 = st.tabs([\"Processed Output\", \"\u00f0\u0178\u201c\u00a4 Sent Request\", \"\u00f0\u0178\u201c\u00a5 Received Response\"])\n    col1, col2 = st.columns([1, 1])\n\n    with tab1:\n        if output:\n            try:\n                with col1:\n                    pretty_json, json_data = process_output(output)\n\n                    # Provide a download button for the pretty JSON\n                    st.download_button(\n                        label=\"Download JSON\",\n                        data=pretty_json,\n                        file_name=f\"{document_file.name if document_file else 'response'}.json\",\n                        mime=\"application/json\"\n                    )\n\n                    try:\n                        # Flatten the JSON structure\n                        flattened_json = flatten_json(json_data)\n                        df = pd.DataFrame(flattened_json.items(), columns=['Key', 'Value'])\n                        st.write(\"### Flattened JSON Table\")\n                        st.write(df.to_markdown(index=False))\n                    except Exception as e:\n                        st.write(f\"Error Creating Result Table: {e}\")\n\n                    # Pretty print the response JSON\n                    st.write(\"### JSON Result\")\n                    st.markdown(f'```json\\n{pretty_json}\\n```')\n\n                with col2:\n                    if document_file and document_file.type.startswith('image/'):\n                        st.write(\"### Uploaded Image\")\n                        image = Image.open(document_file)\n                        st.image(image, use_column_width=True)\n                    # todo render pdf\n                    else:\n                        st.write(\"### Uploaded Document\")\n                        st.write(f\"Cannot display document preview: {document_file.name}, {document_file.type}\")\n\n            except Exception as e:\n                st.write(f\"Error: {e}\")\n        else:\n            st.write('No Output fetched, please try again.')\n\n    with tab3:\n        st.write(\"### Sent Request Data\")\n        st.json(headers)\n        st.json(additional_properties)\n\n    with tab4:\n        st.write(\"### Raw Response Data\")\n        st.write(output)  # Display the raw received response\n\n\ndef flatten_json(nested_json, parent_key='', sep='.'):\n    \"\"\"\n    Recursively flattens a nested JSON object.\n\n    Parameters:\n    nested_json (dict): The JSON object to flatten\n    parent_key (str): The base key string for the nested keys\n    sep (str): Separator to use between keys\n\n    Returns:\n    dict: Flattened JSON\n    \"\"\"\n    items = []\n    if isinstance(nested_json, dict):\n        for k, v in nested_json.items():\n            if k == \"DocumentTexts\" or k == \"DocumentBinaries\":\n                continue\n            new_key = f'{parent_key}{sep}{k}' if parent_key else k\n            if isinstance(v, dict):\n                items.extend(flatten_json(v, new_key, sep=sep).items())\n            elif isinstance(v, list):\n                for i, item in enumerate(v):\n                    items.extend(flatten_json(item, f'{new_key}[{i}]', sep=sep).items())\n            else:\n                items.append((new_key, v))\n    elif isinstance(nested_json, list):\n        for i, item in enumerate(nested_json):\n            items.extend(flatten_json(item, f'{parent_key}[{i}]', sep=sep).items())\n    else:\n        items.append((parent_key, nested_json))\n    return dict(items)\n\ndef cleanup_temp_dir(temp_dir):\n    \"\"\"\n    Cleans up the specified temp directory after processing is complete.\n    \"\"\"\n    if os.path.exists(temp_dir):\n        shutil.rmtree(temp_dir)\n    os.makedirs(temp_dir)\n    st.write(f\"Temp directory {temp_dir} cleaned up.\")\n",
        "file_name": "llm_workflow_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\ocr_feature.py": {
        "summary": "The `ocr_feature.py` script is a Streamlit application designed to perform Optical Character Recognition (OCR) using two engines: AzureRead and NuanceOCR. The application allows users to upload PDF or image files and select the OCR engine they wish to use. \n\n### Key Components:\n1. **Sidebar Creation**: The sidebar includes options for selecting the OCR engine and entering a URL for the API endpoint. It also displays a color legend for the different OCR engines.\n\n2. **File Upload and Tabs**: Users can upload files and navigate between three tabs:\n   - OCR Json: Displays the raw JSON results from the OCR API.\n   - OCR BBox Table: Shows a table with bounding box information for detected words.\n   - Document with Bounding Boxes: Displays the uploaded document with visual bounding boxes around recognized text.\n\n3. **API Interaction**: The script retrieves an API key from environment variables, constructs appropriate URLs based on the selected OCR engine, and sends requests to the OCR services. Results are processed to extract bounding boxes for recognized text.\n\n4. **Result Rendering**: The application displays results in various formats, including JSON, tables, and annotated images. It handles both images and PDF documents, converting PDF pages to images when necessary.\n\n5. **Bounding Box Extraction**: Functions are included to parse JSON responses and extract bounding boxes for individual words, which are then used for visualization.\n\n6. **Error Handling**: Basic error handling is included for API responses and file processing.\n\nOverall, this script provides a user-friendly interface for performing OCR and visualizing results, making it a useful tool for document analysis and text extraction.",
        "content": "import os\nfrom typing import List\nimport pandas as pd\nimport streamlit as st\nimport httpx\nimport streamlit as st\nimport json\nfrom PIL import Image, ImageDraw\nimport io\nfrom pdf2image import convert_from_bytes\nfrom utils import draw_bounding_boxes_on_image, draw_bounding_boxes_on_pdf, process_document_binaries, extract_locations_from_json\n\nocr_colors = {\n    \"AzureRead\": \"red\",\n    \"NuanceOCR\": \"blue\"\n}\n\n\ndef create_sidebar():\n    with st.sidebar:\n        st.title(\"OCR Feature\")\n\n        # select between AzureRead and NuanceOCR\n        ocr_engine = st.selectbox(\"Select OCR Engine:\", (\"AzureRead\", \"NuanceOCR\", \"Both\"))\n\n        predefined_urls = [\n            \"http://52.236.159.89/workflow-server\",\n            \"http://localhost:8080\"\n        ]\n\n        # Display a text input field for the URL\n        selected_url_index = st.selectbox(\"Select URL\", [(url, idx) for idx, url in enumerate(predefined_urls)], format_func=lambda x: x[0])\n\n        # Retrieve the selected URL\n        selected_url = selected_url_index[0]\n\n        # Allow users to edit the URL directly\n        edited_url = st.text_input(\"Edit URL (Final URL)\", selected_url)\n\n        # If the edited URL is different from the selected URL, update it\n        if edited_url != selected_url:\n            selected_url = edited_url\n\n\n        # show ocr colors in form of a legend with colored rectangles\n        st.markdown(\"__OCR Engine Colors:__\")\n        for engine, color in ocr_colors.items():\n            st.markdown(f\"<span style='color:{color}'>\u00e2\u2013\u00a0</span> {engine}\", unsafe_allow_html=True)\n        return ocr_engine, selected_url\n\ndef ocr_feature():\n    ocr_engine, selected_url = create_sidebar()\n\n    # Display a file uploader widget\n    uploaded_file = st.file_uploader(\"Choose a document (pdf/png)\", type=[\"pdf\", \"png\", \"jpg\", \"jpeg\"])\n\n    tab1, tab2, tab3 = st.tabs([\"OCR Json\", \"OCR BBox Table\", \"Document with Bounding Boxes\"])\n\n    if uploaded_file:\n        files = {\"file\": uploaded_file.getvalue()}\n\n        # read APIKey from env variable, check if it exists\n        apiKey = get_api_key()\n\n        # Define the URL and headers        \n        urls, colors = create_urls(selected_url, ocr_engine)\n\n        ocr_results = [send_request(url, apiKey, files) for url in urls]\n\n        bounding_boxes = [extract_bounding_boxes(data) for data in ocr_results]\n\n        # display ocr_results in json format\n        with tab1:\n            dispay_ocr_results(urls, ocr_results)\n            pass\n\n        with tab2:\n            # create two columns for each ocr engine\n            col1, col2 = st.columns(2)\n            for i, bboxes in enumerate(bounding_boxes):\n                table_markdown = render_bbox_table(bboxes)\n                if i == 0:\n                    col1.title(\"AzureRead\")\n                    col1.markdown(table_markdown)\n                else:\n                    col2.title(\"NuanceOCR\")\n                    col2.markdown(table_markdown)\n\n        with tab3:        \n            if uploaded_file.type == \"image/png\" or uploaded_file.type == \"image/jpeg\" or uploaded_file.type == \"image/jpg\":\n                results = create_result_images_from_image(ocr_results, bounding_boxes, colors, uploaded_file)\n                if len(results) == 1:\n                    image = results[0][0]\n                    st.image(image, caption=\"Document with Bounding Boxes\", use_column_width=False)\n                else:\n                    col1, col2 = st.columns(2)\n                    image1 = results[0][0]\n                    image2 = results[1][0]\n\n                    # resize the images to fit the column width\n                    image1 = image1.resize((int(image1.width/2), int(image1.height/2)))\n                    image2 = image2.resize((int(image2.width/2), int(image2.height/2)))\n                    \n                    col1.image(image1, caption=\"Document with Bounding Boxes\", use_column_width=False)\n                    col2.image(image2, caption=\"Document with Bounding Boxes\", use_column_width=False)\n     \n            elif uploaded_file.type == \"application/pdf\":\n                with st.expander(\"PDF with Bounding Boxes\", expanded=True):\n                    for i, ocr_result in enumerate(ocr_results):\n                        # If ocr images are included in the response\n                        pages = process_document_binaries(ocr_result)\n                        # else take the uploaded pdf\n                        if len(pages) == 0:\n                            # check if there are pages and if Dpi is set\n                            if 'Pages' not in ocr_results[0] or 'Dpi' not in ocr_results[0]['Pages'][0]:\n                                st.error(\"No pages or Dpi found in JSON response\")\n                                return\n                            pages = convert_from_bytes(uploaded_file.getvalue(), dpi=ocr_results[0]['Pages'][0]['Dpi'])\n                        \n                        for j, page in enumerate(pages):\n                            color = colors[i]\n                            page = draw_bounding_boxes_on_image(page, bounding_boxes=bounding_boxes[i], color=color, page=j+1)\n\n                            #resize the image with bounding boxes\n                            #page_with_boxes = page_with_boxes.resize((int(page_with_boxes.width/2), int(page_with_boxes.height/2)))\n                            st.image(page, caption=f\"Page {j+1} with Bounding Boxes\", use_column_width=False)\n\n                        st.markdown(\"---\")\n                        st.markdown(\"---\")\n\ndef get_api_key():\n    if 'APIKEY' not in os.environ:\n        #st.error(\"APIKEY not found in environment variables\")\n        return \"\"\n    return os.environ['APIKEY']\n\ndef create_urls(base_url, ocr_engine) -> List:\n    urls = []\n    colors = {}\n    if ocr_engine == \"AzureRead\":\n        urls.append(f\"{base_url}/meta-ocr\")\n        colors[0] = ocr_colors[\"AzureRead\"]\n    elif ocr_engine == \"NuanceOCR\":\n        urls.append(f\"{base_url}/ocr\")\n        colors[0] = ocr_colors[\"NuanceOCR\"]\n    elif ocr_engine == \"Both\":\n        urls.append(f\"{base_url}/meta-ocr\")\n        urls.append(f\"{base_url}/ocr\")\n        colors[0] = ocr_colors[\"AzureRead\"]\n        colors[1] = ocr_colors[\"NuanceOCR\"]\n\n    return urls, colors\n\ndef send_request(url, apiKey, files):\n    headers = {\n        \"X-ApiKey\": apiKey,\n        \"User-Agent\": \"Bludelta Workflow Client\"\n    }\n\n    response = httpx.post(url, headers=headers, files=files, timeout=120)\n\n    if response.status_code != 200:\n        st.error(f\"Error: {response.status_code}\")\n        return {}\n    return response.json()\n\n# # Function to extract bounding boxes for all words\ndef extract_bounding_boxes(data):    \n    \"\"\"\n    Parses the following JSON structure to extract bounding boxes for all words:\n\n      \"DocumentTexts\": [\n    {\n      \"Line\": {\n        \"Location\": {\n          \"Height\": 42,\n          \"Left\": 167,\n          \"Page\": 1,\n          \"Top\": 60,\n          \"Width\": 356\n        },\n        \"Text\": \"Parkhaus Richthalle\",\n        \"Words\": [\n          {\n            \"Location\": {\n              \"Height\": 42,\n              \"Left\": 167,\n              \"Page\": 1,\n              \"Top\": 60,\n              \"Width\": 151\n            },\n            \"Text\": \"Parkhaus\"\n          },\n          {\n            \"Location\": {\n              \"Height\": 37,\n              \"Left\": 337,\n              \"Page\": 1,\n              \"Top\": 65,\n              \"Width\": 186\n            },\n            \"Text\": \"Richthalle\"\n          }\n        ]\n    \n    \"\"\"\n\n    bounding_boxes = []\n    lines = data[\"DocumentTexts\"]\n    for line in lines:\n        words = line['Line']['Words']\n        for word in words:\n            location = word['Location']\n            bounding_boxes.append({\n                \"Text\": word['Text'],\n                \"BoundingBox\": {\n                    \"x\": location['Left'],\n                    \"y\": location['Top'],\n                    \"width\": location['Width'],\n                    \"height\": location['Height'],\n                    \"page\": location['Page']\n                }\n            })\n        \n    return bounding_boxes\n\ndef get_plain_text(data):\n    plain_text = \"\"\n    for line in data[\"DocumentTexts\"]:\n        plain_text += line[\"Line\"][\"Text\"] + \"\\n\"\n    return plain_text\n\ndef extract_bounding_boxes_old_format(data):    \n    bounding_boxes = []\n    for region in data.get(\"Regions\", []):\n        for line in region.get(\"Lines\", []):\n            for word in line.get(\"Words\", []):\n                bbox = word.get(\"BoundingBox\", {})\n                bounding_boxes.append({\n                    \"Text\": word.get(\"Text\", \"\"),\n                    \"BoundingBox\": {\n                        \"Left\": bbox.get(\"Left\"),\n                        \"Top\": bbox.get(\"Top\"),\n                        \"Width\": bbox.get(\"Width\"),\n                        \"Height\": bbox.get(\"Height\")\n                    }\n                })\n    return bounding_boxes\n\ndef dispay_ocr_results(urls, ocr_results):\n    for url, result in zip(urls, ocr_results):\n        st.write(f\"URL: {url}\")\n        #st.json(result)\n        # write json into a markdown code block\n        st.code(json.dumps(result, indent=4))\n\ndef render_bbox_table(bounding_boxes):\n    # create a markdown table with bounding boxes for each ocr engine\n    # there shall be a table row for bounding box of each word and it shall contain\n    # the word, x, y, width, height, page, ocr engine\n    # | Word | x | y | width | height | page | OCR Engine |\n    table_markdown = \\\n\"\"\"\n__Bounding Box Table__\n\n| Word | x   | y   | width | height | page |\n| ---  | --- | --- | ---   | ---    | ---  |\n\"\"\"\n\n            # Append each row to the markdown string\n    for bbox in bounding_boxes:\n        x, y, width, height = bbox['BoundingBox']['x'], bbox['BoundingBox']['y'], bbox['BoundingBox']['width'], bbox['BoundingBox']['height']\n        page = bbox['BoundingBox']['page']\n        text = bbox['Text']\n        table_markdown += f\"| {text} | {x} | {y} | {width} | {height} | {page} |\\n\"\n\n    return table_markdown\n   \ndef create_result_images_from_image(ocr_results, bounding_boxes, colors, uploaded_file):\n    results = []\n    assert len(ocr_results) == len(bounding_boxes), \"Length of ocr_results, bounding_boxes should be the same\"\n\n    for i in range(len(ocr_results)):\n        images = process_document_binaries(ocr_results[i])\n        if len(images) == 1:\n            image = images[0]\n        else:\n            image = Image.open(uploaded_file)\n        \n        color = colors[i]\n        image = draw_bounding_boxes_on_image(image, bounding_boxes[i], color=color)\n        results.append((image, color))\n\n    return results",
        "file_name": "ocr_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\package_upload_feature.py": {
        "summary": "The `package_upload_feature.py` script is a Streamlit application that facilitates the uploading of document packages to Bludelta's training pipeline. It connects to a SQL Server database to retrieve customer names and presents a user interface for package uploads. Here are the key components:\n\n1. **Database Connection**: It establishes a connection to either the `bcsdbdev` or `bcsdb-auth` database using the `pyodbc` library, depending on the specified database name.\n\n2. **Customer Retrieval**: The script queries the database to fetch a list of customer names from the `Customer` table.\n\n3. **User Interface**: The Streamlit app provides several input fields for the user to enter:\n   - API URL\n   - API Key and Identifier\n   - Document Type (with a dropdown selection)\n   - Customer name (optional)\n   - Tags (optional)\n   - OCR Engine selection (optional)\n   - File uploader for ZIP packages\n\n4. **Package Upload Logic**: Upon clicking the \"Upload Package\" button, it validates the required fields and constructs a multipart/form-data payload for the API request. The request is sent to the specified API URL, and based on the response, it provides user feedback on the upload status.\n\n5. **Error Handling**: The script includes error handling for both valid and invalid responses, displaying appropriate messages to the user based on the outcome of the upload process.\n\nOverall, this script is designed to streamline the process of uploading packages to a specified API, ensuring that users can easily input their data and receive feedback on their submissions.",
        "content": "import sys\nimport streamlit as st\nimport requests\nimport pyodbc\nimport pandas as pd\nimport json\n\ndef get_bcsdb_connection(database, username, password):\n    if sys.platform == 'linux':\n        driver_name = 'ODBC Driver 17 for SQL Server'\n    elif sys.platform == 'win32':\n        driver_name = 'SQL Server'\n        \n    if database == \"bcsdbdev\":\n        conn = pyodbc.connect(f\"DRIVER={{{driver_name}}};SERVER=bcdbserverdev.database.windows.net;DATABASE=bcsdbdev;UID={username};PWD={password};\")\n    elif database == \"bcsdb-auth\":\n        conn = pyodbc.connect(f\"DRIVER={{{driver_name}}};SERVER=bcdbserver.database.windows.net;DATABASE=bcsdb-auth;UID={username};PWD={password};\")\n\n    return conn    \n\ndef get_customer_names(conn):\n        # Execute the SELECT statement\n        query = \"\"\"\n            SELECT [Name]\n            FROM dbo.Customer\n        \"\"\"\n        df = pd.read_sql(query, conn)\n        return df[\"Name\"].tolist()\n\n\ndef package_upload():\n    conn = get_bcsdb_connection(\"bcsdbdev\", \"dbadmin\", \"!user_123\")\n    if conn == None:\n        return\n\n\n    st.title(\"Bludelta Learn API - Package Upload\")\n\n    st.markdown(\"\"\"\n    ### Upload a package of documents with ground truth into Bludelta's training pipeline\n    \"\"\")\n\n    # Input field for the URL with default value\n    url = st.text_input(\"Enter the API URL:\", \"https://learn.bludelta.ai/v1/Package\", key=\"package-upload-url\")\n\n    # Columns for ApiKey, ApiIdentifier, and Document Type\n    col1, col2, col3 = st.columns(3)\n    api_key = col1.text_input(\"X-ApiKey:\", key=\"package-upload-api-key\")\n    api_identifier = col2.text_input(\"X-ApiIdentifier (optional):\", key=\"package-upload-api-identifier\")\n    document_type = col3.selectbox(\n        \"Document Type\", \n        options=[\"\", \"DeliveryNote\", \"Notice\", \"VehicleRegistration\", \"Order\", \"CertificateOfOrigin\", \"CustomsDutyReceipt\", \n                 \"DangerousGoodsNote\", \"TransitNoteT1\", \"TransitNoteT2\", \"ProformaInvoice\", \"ExportAccompanyingDocument\", \n                 \"DeliveryNoteCustoms\", \"DeliveryNoteTransit\", \"OrderConfirmation\", \"Quotation\"],\n        key=\"package-upload-document-type\")\n    \n    customers = get_customer_names(conn)\n    customer = st.selectbox(\"Customer (optional; DEPRECATED?)\", options=customers, key=\"package-upload-customer\")\n\n    tags = st.text_input(\"Tags (optional) comma separated such as JP, #22454:\", key=\"package-upload-tags\")\n    ocr_engine = st.selectbox(\"OCR Engine (optional)\", options=[\"\", \"AzureReadOcr\"], key=\"package-upload-ocr-engine\") #Easy OCR could be added here\n\n    #\"\"\"\n    property_store = {\"OCREngine\": ocr_engine} #, \"Language\": \"\" could be added.\n    json_data = json.dumps(property_store)\n    json_file = (None, json_data, 'application/json')\n    #\"\"\"\n\n    uploaded_package = st.file_uploader(\"Upload Package (ZIP file):\", type=[\"zip\", \"application/x-zip-compressed\"])\n\n    # Button to trigger the upload\n    if st.button(\"Upload Package\"):\n        # Check if all required fields are filled\n        if not api_key or not uploaded_package or not url:\n            st.error(\"Please fill in all required fields.\")\n            return\n\n        # Headers including the authentication keys\n        headers = {\n            \"X-ApiKey\": api_key,\n            \"X-ApiIdentifier\": api_identifier,\n        }\n\n        # Multipart/form-data payload\n        if ocr_engine is not None:\n            files = {\n                \"DocumentType\": (None, document_type),\n                \"Package\": uploaded_package,\n                #\"CompanyName\" : (None, customer),\n                \"Tags\":(None, tags),\n                \"PropertyStore\": json_file\n            }\n        else:\n            files = {\n                \"DocumentType\": (None, document_type),\n                \"Package\": uploaded_package,\n                #\"CompanyName\" : (None, customer),\n                \"Tags\":(None, tags),\n                #\"PropertyStore\": json_file\n            }\n\n        # Making the POST request\n        response = requests.post(url, headers=headers, files=files)\n\n        # Handling the response\n        if response.status_code == 200:\n            try:\n                # Ensure the response is valid JSON before accessing it\n                package_id = response.json().get(\"packageId\")\n                st.success(f\"Package uploaded successfully! Package ID: {package_id}\")\n            except ValueError:\n                st.error(\"Received an invalid JSON response from the server.\")\n        else:\n            try:\n                # Ensure the error response is valid JSON\n                error_details = response.json()\n                st.error(f\"Error uploading package:\\nStatus: {error_details.get('status')}\\nDetail: {error_details.get('detail')}\")\n            except ValueError:\n                # Fallback if the error response is not JSON\n                st.error(f\"Error uploading package:\\nStatus: {response.status_code}\\nResponse: {response.text}\")\n\n ",
        "file_name": "package_upload_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\plugin_info_feature.py": {
        "summary": "The provided Python script, `plugin_info_feature.py`, is a Streamlit application that allows users to fetch and display system or plugin information from a specified API. \n\nKey components of the script include:\n- A title for the Streamlit app: \"System or Plugin Info\".\n- A dropdown menu for users to select between two predefined URLs (one for plugin info and one for system info).\n- A text input field for users to enter an API key (`X-API-Key`).\n- A button that, when clicked, sends a GET request to the selected URL with the specified API key in the headers.\n- The response data is displayed in JSON format on the app interface. \n- Error handling is implemented to catch and display any issues that arise during the data fetching process.\n\nOverall, the script provides a user-friendly interface for accessing and displaying information from the specified APIs.",
        "content": "import requests\nimport streamlit as st\nimport json\n\ndef plugin_info_feature():\n    st.title(\"System or Plugin Info\")\n\n    # Configurable URL input\n    # a selection box for the user to choose a URL\n    url = st.selectbox(\"Select a URL to fetch system or plugin info:\",\n                       (\"https://api.bludelta.ai/v1-18/plugin/info?format=json\",\n                        \"https://api.bludelta.ai/v1-18/system/info?format=json\"))\n\n    #url = st.text_input(\"Enter the URL to fetch plugin info:\", \"https://api.bludelta.ai/v1-18/plugin/info?format=json\")\n    xapi_key = st.text_input(\"Enter the X-API-Key:\", \"\")\n    if st.button(\"Fetch Plugin Info\"):\n        try:\n            headers = {'X-ApiKey': xapi_key}\n            response = requests.get(url, headers=headers)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n            plugin_data = response.json()\n            st.json(plugin_data)\n        except requests.RequestException as e:\n            st.error(f\"Error fetching data from {url}: {e}\")\n\n",
        "file_name": "plugin_info_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\utils.py": {
        "summary": "The `utils.py` file contains several utility functions for processing images and documents. Key functions include:\n\n1. **get_api_key()**: Generates a base64-encoded SHA-512 hash from random bytes to create an API key.\n2. **get_api_identifier()**: Generates a base64-encoded MD5 hash from random bytes to create an API identifier.\n3. **draw_bounding_boxes_on_image(image, bounding_boxes, color='red', width=1, page=None)**: Draws specified bounding boxes on an image, optionally filtering by page.\n4. **draw_bounding_boxes_on_pdf(pages, bounding_boxes_data)**: Draws bounding boxes on each page of a PDF based on provided data, grouping boxes by page.\n5. **extract_locations_from_json(data)**: Recursively extracts values associated with \"Location\" keys from a JSON structure.\n6. **process_document_binaries(data)**: Processes document data to decode images from binary content, handling EXIF data for orientation correction.\n\nOverall, the file provides functionalities for image manipulation and document processing, particularly in contexts where bounding boxes and image orientation are relevant.",
        "content": "import hashlib\nimport secrets\nimport base64\nimport pyodbc\nimport io\nfrom PIL import Image, ImageDraw, ExifTags\n\ndef get_api_key():\n    random_bytes = secrets.token_bytes(1024)\n    sha512_hash = hashlib.sha512(random_bytes).digest()\n    return base64.b64encode(sha512_hash).decode()\n\ndef get_api_identifier():\n    random_bytes = secrets.token_bytes(1024)\n    md5_hash = hashlib.md5(random_bytes).digest()\n    return base64.b64encode(md5_hash).decode()\n\ndef draw_bounding_boxes_on_image(image, bounding_boxes, color='red', width=1, page:int=None):\n    \"\"\"\n    Draw bounding boxes on the image using the provided bounding box data.\n    \"\"\"\n    draw = ImageDraw.Draw(image)\n    for box in bounding_boxes:\n        if 'BoundingBox' in box:\n            box = box['BoundingBox']\n        x, y, width, height, page_ = box['x'], box['y'], box['width'], box['height'], box['page']\n\n        # Only draw if the box is on the specified page\n        if page is not None and page == page_:\n            print(f\"Drawing bounding box on page {page} == {page_}\")                    \n            draw.rectangle([x, y, x+width, y+height], outline=color, width=1)\n        elif page is None:\n            draw.rectangle([x, y, x+width, y+height], outline=color, width=1)\n    return image\n\ndef draw_bounding_boxes_on_pdf(pages, bounding_boxes_data):\n    \"\"\"\n    Draw bounding boxes on each page of the PDF using the provided bounding box data.\n    \"\"\"\n    # Group bounding boxes by page\n    grouped_bboxes = {}\n    for bbox_data in bounding_boxes_data:\n        page = bbox_data.get(\"Pages\")\n        if page not in grouped_bboxes:\n            grouped_bboxes[page] = []\n        grouped_bboxes[page].append(bbox_data)\n\n    # Draw bounding boxes on respective pages\n    pages_with_boxes = []\n    for i, page in enumerate(pages):\n        # Only draw if there are bounding boxes for this page\n        if i+1 in grouped_bboxes: \n            for box in grouped_bboxes[i+1]:\n                x, y, width, height = box['Left'], box['Top'], box['Width'], box['Height']\n                draw = ImageDraw.Draw(page)\n                draw.rectangle([x, y, x+width, y+height], outline='red', width=2)\n        pages_with_boxes.append(page)\n    \n    return pages_with_boxes\n\ndef extract_locations_from_json(data):\n    \"\"\"\n    Recursively search for all \"Location\" keys in the JSON tree and return their values.\n    \"\"\"\n    locations = []\n    if isinstance(data, dict):\n        for key, value in data.items():\n            if key == \"Location\":\n                locations.append(value)\n            elif key == \"DocumentTexts\":\n                continue\n            else:\n                locations.extend(extract_locations_from_json(value))\n    elif isinstance(data, list):\n        for item in data:\n            locations.extend(extract_locations_from_json(item))\n    return locations\n\ndef process_document_binaries(data):\n    pages = []  # Initialize an empty list to store decoded images\n    ocrImages = \"OcrTransformed\"  # The key to search for\n    should_expand = True\n    # Check if 'DocumentBinaries' exists in data\n    if 'DocumentBinaries' in data:\n        for item in data['DocumentBinaries']:\n            if item['ContentType'] == ocrImages and 'Embedding' in item:\n                embeddings = item['Embedding']\n                for embedding in embeddings:\n                    if isinstance(embedding, str):\n                        decoded_image = base64.b64decode(embedding)\n                        image = Image.open(io.BytesIO(decoded_image))\n\n                        # Handle EXIF orientation tag\n                        try:\n                            for orientation in ExifTags.TAGS.keys():\n                                if ExifTags.TAGS[orientation] == 'Orientation':\n                                    break\n                            \n                            exif = image._getexif()\n                            if exif is not None:\n                                exif = dict(exif.items())\n                                orientation = exif.get(orientation, 1)\n                                \n                                if orientation == 3:\n                                    image = image.rotate(180, expand=should_expand)\n                                elif orientation == 6:\n                                    image = image.rotate(270, expand=should_expand)\n                                elif orientation == 8:\n                                    image = image.rotate(90, expand=should_expand)\n                        except (AttributeError, KeyError, IndexError):\n                            # cases: image don't have getexif\n                            pass                        \n\n                        pages.append(image)                        \n\n    return pages\n",
        "file_name": "utils.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\workflow_feature.py": {
        "summary": "The `workflow_feature.py` file is a Streamlit application designed for processing documents, specifically for extracting and displaying essential information from various document formats (PDF, PNG, JPEG). Here are the key components and functionalities of the script:\n\n1. **Imports**: The script utilizes several libraries including `os`, `json`, `streamlit`, `httpx`, `PIL` for image processing, `io`, `pdf2image` for converting PDF to images, and `requests` for handling HTTP requests.\n\n2. **Functions**:\n   - **display_document_essential_details**: Displays essential document details in a Markdown table format.\n   - **display_all_line_items_as_table**: Extracts and formats line item data from the JSON response into a Markdown table.\n   - **display_json_as_markdown**: Formats and displays basic document information and extracted details as Markdown.\n   - **draw_bounding_boxes_on_image**: Draws bounding boxes on uploaded images based on provided coordinates.\n   - **draw_bounding_boxes_on_pdf**: Draws bounding boxes on each page of a PDF based on provided coordinates.\n   - **extract_locations_from_json**: Recursively searches for and returns all \"Location\" keys in the JSON data.\n   - **create_sidebar**: Creates a sidebar in the Streamlit app for URL selection and timeout input.\n   - **process_document_binaries**: Processes document binaries to extract images encoded in base64.\n   - **add_property_store**: Converts a PropertyStore string into a JSON format.\n   - **add_customer_name_role_to_header**: Adds customer details to the request header.\n   - **document_bounding_boxes_feature**: The main function that orchestrates the document processing workflow, including file upload, making API calls, and displaying results in different tabs.\n\n3. **User Interface**: The application provides a sidebar for URL selection, allows users to upload documents, and input additional properties related to the document. It displays results in three tabs: a summary, JSON response, and the document with drawn bounding boxes.\n\n4. **Error Handling**: The script includes error handling for various operations, such as checking for the API key and ensuring valid inputs.\n\nOverall, this file is a comprehensive tool for processing and visualizing document data using machine learning and image processing techniques within a user-friendly web interface.",
        "content": "import os\nimport json\nimport streamlit as st\nimport httpx\nfrom PIL import Image, ImageDraw\nimport io\nfrom pdf2image import convert_from_bytes\nimport base64\nimport requests\nfrom requests_toolbelt.multipart.encoder import MultipartEncoder\n\n\ndef display_document_essential_details(json_data):\n    # get all details from \"DocumentEssentials\" where a details has no items\n    details = [item for item in json_data[\"DocumentEssentials\"] if not item.get(\"Items\")]\n\n    # a details has the following structure:\n    # {\n    #   \"Confidence\":0.7882\n    #   \"ConfidenceThreshold\":-1\n    #   \"Label\":\"Quotation.Date\"\n    #   \"Location\":{\n#           \"Height\":30\n    #       \"Left\":2070\n    #       \"Page\":1\n    #       \"Top\":1037\n    #       \"Width\":192\n    # }\n    # \"Text\":\"18.01.2022\"\n    # \"Value\":\"2022-01-18\"\n    # }\n    # take this structure and display it as markdown table\n\n    headers = [\"Label\", \"Text\", \"Value\", \"Confidence\", \"ConfidenceThreshold\"]\n    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\" + \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n\n    for detail in details:\n        row = [str(detail.get(header, \"N/A\")) for header in headers]\n        markdown_table += \"| \" + \" | \".join(row) + \" |\\n\"\n\n    return markdown_table    \n\n# Function to display all Line.Item data as a Markdown table\ndef display_all_line_items_as_table(json_data):\n    # Extract all Line.Item entries\n    line_items = [item for item in json_data[\"DocumentEssentials\"] if item.get(\"Label\") == \"Line.Item\"]\n    \n    if not line_items:\n        st.markdown(\"No line items found.\")\n        return\n\n    # Assuming the structure of line items is consistent across all entries\n    # Extract headers from the first line item assuming all line items have the same structure\n    headers = [item[\"Label\"].split('.')[0] for item in line_items[0][\"Items\"]]\n    \n    # Create Markdown table header\n    markdown_table = \"| \" + \" | \".join(headers) + \" |\\n\" + \"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\\n\"\n    \n    # Iterate through each Line.Item to add rows to the table\n    for line_item in line_items:\n        row = [item.get(\"Text\", \"N/A\") if item.get(\"Text\") else \"N/A\" for item in line_item[\"Items\"]]\n        markdown_table += \"| \" + \" | \".join(row) + \" |\\n\"\n    \n    # Display using Streamlit\n    return markdown_table\n\n# Function to display JSON data in Markdown format\ndef display_json_as_markdown(json_data):\n    # Extracting and formatting basic information\n    bludoc_version = json_data.get(\"BluDoc.Version\", \"\")\n    created_datetime = json_data.get(\"Created.DateTime\", \"\")\n    software_name = json_data.get(\"CreatorSoftware.Name\", \"\")\n    software_version = json_data.get(\"CreatorSoftware.Version\", \"\")\n    languages = \", \".join(json_data.get(\"Document.Languages\", []))\n    document_type = json_data.get(\"Document.Type\", \"\")\n\n    details = display_document_essential_details(json_data)\n    line_item_table = display_all_line_items_as_table(json_data)\n    #tables = display_document_essentials_tables(json_data)\n\n    # Basic document information\n    markdown_text = f\"\"\"\n## Document Information\n\n- **BluDoc Version**: {bludoc_version}\n- **Created On**: {created_datetime}\n- **Creator Software**: {software_name} v{software_version}\n- **Languages**: {languages}\n- **Document Type**: {document_type}\n\n## Document Essentials\n\n## Details\n{details}\n\n### Line Items\n{line_item_table}\n\"\"\"\n    return markdown_text\n\ndef draw_bounding_boxes_on_image(image, bounding_boxes):\n    \"\"\"\n    Draw bounding boxes on the image using the provided bounding box data.\n    \"\"\"\n    draw = ImageDraw.Draw(image)\n    for box in bounding_boxes:\n        x, y, width, height = box['x'], box['y'], box['width'], box['height']\n        draw.rectangle([x, y, x+width, y+height], outline='red', width=2)\n    return image\n\ndef draw_bounding_boxes_on_pdf(pages, bounding_boxes_data):\n    \"\"\"\n    Draw bounding boxes on each page of the PDF using the provided bounding box data.\n    \"\"\"\n    # Group bounding boxes by page\n    grouped_bboxes = {}\n    for bbox_data in bounding_boxes_data:\n        page = bbox_data[\"Page\"]\n        if page not in grouped_bboxes:\n            grouped_bboxes[page] = []\n        grouped_bboxes[page].append(bbox_data)\n\n    # Draw bounding boxes on respective pages\n    pages_with_boxes = []\n    for i, page in enumerate(pages):\n        # Only draw if there are bounding boxes for this page\n        if i+1 in grouped_bboxes: \n            for box in grouped_bboxes[i+1]:\n                x, y, width, height = box['Left'], box['Top'], box['Width'], box['Height']\n                draw = ImageDraw.Draw(page)\n                draw.rectangle([x, y, x+width, y+height], outline='red', width=2)\n        pages_with_boxes.append(page)\n    \n    return pages_with_boxes\n\ndef extract_locations_from_json(data):\n    \"\"\"\n    Recursively search for all \"Location\" keys in the JSON tree and return their values.\n    \"\"\"\n    locations = []\n    if isinstance(data, dict):\n        for key, value in data.items():\n            if key == \"Location\":\n                locations.append(value)\n            elif key == \"DocumentTexts\":\n                continue\n            else:\n                locations.extend(extract_locations_from_json(value))\n    elif isinstance(data, list):\n        for item in data:\n            locations.extend(extract_locations_from_json(item))\n    return locations\n\ndef create_sidebar():\n    with st.sidebar:\n        st.title('Blu Delta Workflows')\n\n        # List of predefined URLs to choose from\n        predefined_urls = [\n            \"https://capture.bludelta.ai/quotation/v1/quotation\",\n            \"https://capture.bludelta.ai/order-confirmation/v1/order-confirmation\",\n            \"https://capture.bludelta.ai/receipt/v1/receipt\",\n            \"https://capture-dev.bludelta.ai/quotation/v1/quotation\",\n            \"https://capture-dev.bludelta.ai/order-confirmation/v1/order-confirmation\",\n            \"https://capture-dev.bludelta.ai/receipt/v1/receipt\",\n            \"http://localhost:80/quotation\",\n            \"http://localhost:80/order-confirmation\",\n            \"http://localhost:80/receipt\"\n        ]\n\n        # Display a text input field for the URL\n        selected_url_index = st.selectbox(\"Select URL\", [(url, idx) for idx, url in enumerate(predefined_urls)], format_func=lambda x: x[0])\n\n        # Retrieve the selected URL\n        selected_url = selected_url_index[0]\n\n        # Allow users to edit the URL directly\n        edited_url = st.text_input(\"Edit URL (Final URL)\", selected_url)\n\n        # If the edited URL is different from the selected URL, update it\n        if edited_url != selected_url:\n            selected_url = edited_url\n\n        timeout = st.text_input(\"Enter timeout [sec]\", \"120\")\n        # convert to int\n        try:\n            timeout = int(timeout)\n        except ValueError:\n            st.error(\"Timeout must be an integer\")\n            return\n\n        return selected_url, timeout\n\ndef process_document_binaries(data):\n    pages = []  # Initialize an empty list to store decoded images\n    ocrImages = \"OcrTransformed\"  # The key to search for\n    \n    # Check if 'DocumentBinaries' exists in data\n    if 'DocumentBinaries' in data:\n        for item in data['DocumentBinaries']:\n            if item['ContentType'] == ocrImages and 'Embedding' in item:\n                embeddings = item['Embedding']\n                for embedding in embeddings:\n                    if isinstance(embedding, str):\n                        decoded_image = base64.b64decode(embedding)\n                        image = Image.open(io.BytesIO(decoded_image))\n                        pages.append(image)\n                # Once a matching entry is found and processed, break out of the loop\n                break\n\n    return pages\n\ndef add_property_store(property_store_str):\n    \"\"\"\n    Convert the PropertyStore string into a dictionary.\n    @param property_store_str: The PropertyStore string in the format 'key1:value1,key2:value2,...'\n    \"\"\"\n    if property_store_str == \"\":\n        return None\n\n    property_store = dict(item.split(\":\") for item in property_store_str.split(\",\")) if property_store_str else {}\n    json_data = {\"PropertyStore\": property_store}\n    return json.dumps(json_data)\n\ndef add_customer_name_role_to_header(header, customer_name, customer_role):\n    \"\"\"\n    Add the CustomerName and CustomerRole to the header.\n    @param header: The header dictionary to which the CustomerName and CustomerRole will be added.\n    @param customer_name: The customer name to be added.\n    @param customer_role: The customer role to be added.\n    \"\"\"\n    if customer_name:\n        header[\"X-CustomerName\"] = customer_name\n    if customer_role:\n        header[\"X-Role\"] = customer_role\n    return header\n\ndef document_bounding_boxes_feature():\n    st.title(\"Blu Delta Workflows\")\n\n    url, timeout = create_sidebar()\n    st.write(\"URL:\", url)\n    st.write(\"Timeout:\", timeout)\n\n    # add two columns. One for uploading the document and the other one for a list of additional properties which can be added one by one\n    col1, col2 = st.columns([1, 1])\n\n    with col1:\n        # Upload the document\n        uploaded_file = st.file_uploader(\"Choose a document (pdf/png,jpeg,jpg)\", type=[\"pdf\", \"png\", \"jpeg\", \"jpg\"])\n    \n    with col2:\n        # Add PropertyStore input text field\n        property_store_str = st.text_input(\"PropertyStore\", value=\"OperationData.All:false\")\n\n        # Add CustomerName and CustomerRole input text fields in an expander\n        with st.expander(\"Customer Details - only necessary when directly using BludeltaWorkflow\"):\n            customer_name = st.text_input(\"CustomerName\", value=\"\")\n            customer_role = st.text_input(\"CustomerRole\", value=\"\")\n        \n    if uploaded_file:\n        payload = add_property_store(property_store_str)\n\n        # read APIKey from env variable, check if it exists\n        if 'APIKEY' not in os.environ:\n            st.error(\"APIKEY not found in environment variables\")\n            return\n        apiKey = os.environ['APIKEY']\n        \n        # Define the URL and headers        \n        url = url\n        headers = {\n            \"X-ApiKey\": apiKey,\n            \"User-Agent\": \"Bludelta Workflow Client\"\n        }\n\n        headers = add_customer_name_role_to_header(headers, customer_name, customer_role)\n\n        # Make the POST request\n        if payload is not None:\n            m = MultipartEncoder(\n                fields={\n                    'json': ('json_data', payload, 'application/json'),  # JSON part with explicit content type\n                    'files': ('file', uploaded_file.getvalue(), 'application/octet-stream')  # File part\n                }\n            )\n        else:   \n            m = MultipartEncoder(\n                fields={\n                    'files': ('file', uploaded_file.getvalue(), 'application/octet-stream')  # File part\n                }\n            )\n\n        headers['Content-Type'] = m.content_type  # Setting the content type to multipart/form-data\n\n        response = requests.post(f\"{url}\", data=m, headers=headers)        \n\n        data = response.json()\n\n        tab1, tab2, tab3 = st.tabs([\"BluDoc Summary\", \"BluDoc Json Response\", \"Document with Bounding Boxes\"])\n\n        # check if data has a key \"Result\"        \n        bludoc_result = data[\"Result\"] if \"Result\" in data else data\n\n        # Extract all bounding box locations\n        bounding_boxes = extract_locations_from_json(bludoc_result)\n\n        with tab1:\n            try:\n                st.markdown(display_json_as_markdown(bludoc_result), unsafe_allow_html=True)\n            except Exception as e:\n                st.error(f\"Error: {e}\")\n        with tab2:\n            st.json(data)            \n        with tab3:\n            try:\n                # Display the uploaded document with bounding boxes\n                if uploaded_file.type == \"image/png\":\n                    image = Image.open(uploaded_file)\n                    image_with_boxes = draw_bounding_boxes_on_image(image, bounding_boxes)\n                    st.image(image_with_boxes, caption=\"Document with Bounding Boxes\", use_column_width=True)        \n                elif uploaded_file.type == \"application/pdf\":\n                    with st.expander(\"PDF with Bounding Boxes\", expanded=True):\n                        # If ocr images are included in the response\n                        pages = process_document_binaries(bludoc_result)\n                        # else take the uploaded pdf\n                        if len(pages) == 0:\n                            # check if there are pages and if Dpi is set\n                            if 'Pages' not in bludoc_result or 'Dpi' not in bludoc_result['Pages'][0]:\n                                st.error(\"No pages or Dpi found in JSON response\")\n                                return\n                            pages = convert_from_bytes(uploaded_file.getvalue(), dpi=bludoc_result['Pages'][0]['Dpi'])\n                        \n                        pages_with_boxes = draw_bounding_boxes_on_pdf(pages, bounding_boxes)\n                        for i, page_with_boxes in enumerate(pages_with_boxes):\n                            st.image(page_with_boxes, caption=f\"Page {i+1} with Bounding Boxes\", use_column_width=True)\n            except Exception as e:\n                st.error(f\"Error: {e}\")\n\n\n\n\n                    ",
        "file_name": "workflow_feature.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\llms\\llm_llama2.py": {
        "summary": "The file `llm_llama2.py` defines a class `LLM_Llama2`, which is a representation of a LLaMA2 chatbot model using the Langchain framework. The class inherits from the `LLM` base class and is initialized with parameters such as `url`, `timeout`, `temperature`, and `top_p`, which control the chatbot's behavior. \n\nKey features of the class include:\n\n- **Attributes**: It holds the model's name, version, and description, along with user-defined parameters for interaction with the chatbot.\n- **Initialization**: The constructor sets up the parameters for the chatbot.\n- **LLM Type and Parameters**: Properties to identify the model type and return its parameters as a dictionary.\n- **Text Generation**: The `_call` method takes a prompt and generates text using the specified parameters through a `Client` from the `text_generation` library.\n\nOverall, this class provides a structured way to interact with a LLaMA2-based chatbot by encapsulating its configuration and functionality.",
        "content": "from typing import Any, List, Mapping, Optional\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\nfrom langchain.llms.base import LLM\nfrom pydantic import Field\nfrom text_generation import Client\n\n\nclass LLM_Llama2(LLM):\n    \"\"\"LLM for LLaMA2 chatbot\"\"\"\n    llm_name = \"llama2\"\n    llm_version = \"0.1.0\"\n    llm_description = \"LLaMA2 chatbot\"\n    url: str = Field(\"\", description=\"Url of the LLaMA2 chatbot\")\n    timeout: int = Field(120, description=\"Timeout of the LLaMA2 chatbot\")\n    temperature: float = Field(0.1, description=\"Temperature of the LLaMA2 chatbot\")\n    top_p: float = Field(0.9, description=\"Top p of the LLaMA2 chatbot\")\n\n\n\n    def __init__(self, url, timeout, temperature, top_p):\n        super(LLM_Llama2, self).__init__()\n        self.url = url\n        self.timeout = timeout\n        self.temperature  = temperature\n        self.top_p = top_p\n        \n    @property\n    def _llm_type(self) -> str:\n        return self.llm_name\n    \n    @property\n    def _identifying_params(self) -> dict:\n        \"\"\"\n        It should return a dict that provides\n        the information of all the parameters \n        that are used in the LLM. This is useful\n        when we print our llm, it will give use the \n        information of all the parameters.\n        \"\"\"\n        return {\n            \"url\": self.url,\n            \"timeout\": self.timeout,\n            \"temperature\": self.temperature,\n            \"top_p\": self.top_p,\n        }\n        \n    def _call(\n        self, \n        prompt: str,\n        stop: Optional[List[str]] = None,\n        run_manager: Optional[CallbackManagerForLLMRun] = None,\n        **kwargs: Any\n    ) -> str:\n        \"\"\"Generate text from prompt\"\"\"\n        client = Client(self.url, timeout=self.timeout)\n        return client.generate(\n            prompt, \n            temperature=self.temperature, \n            top_p=self.top_p, \n            max_new_tokens=kwargs.get(\"max_new_tokens\", 1024),\n            return_full_text=True).generated_text",
        "file_name": "llm_llama2.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Service\\app.py": {
        "summary": "The file `app.py` is a Flask application that sets up a web server with RESTful API capabilities. It utilizes several libraries including Flask-RESTful for API creation, Flask-CORS for handling Cross-Origin Resource Sharing, and Flasgger for generating Swagger documentation.\n\nKey components of the application include:\n\n- **Flask App Initialization**: The app is initialized with static file serving capabilities.\n- **CORS**: Cross-Origin Resource Sharing is enabled for the app.\n- **API Setup**: A Flask-Restful `Api` is created for managing resources.\n- **Swagger Configuration**: The app is configured to serve Swagger UI for API documentation, with specifications loaded from a YAML file.\n- **Docker Compose Resource**: A resource (`DockerComposeResource`) is defined to generate and return updated Docker Compose YAML when accessed via the `/bludelta/docker-compose` endpoint.\n- **Redoc Integration**: It includes a route for serving a Redoc documentation page at `/redoc`.\n- **Running the App**: The application runs in debug mode on all interfaces at port 5000.\n\nOverall, this script sets up a web service that allows users to interact with Docker Compose configurations through a REST API while providing documentation through Swagger and Redoc.",
        "content": "from flask import Flask, jsonify, send_from_directory, render_template\nfrom flask_restful import Api, Resource\nfrom flask_cors import CORS\nfrom flasgger import Swagger, swag_from\nfrom docker_compose_generator import DockerComposeGenerator\n\napp = Flask(__name__, static_folder='static', static_url_path='/static')\nCORS(app)\napi = Api(app)\n\nswagger_config = {\n    \"headers\": [],\n    \"specs\": [\n        {\n            \"endpoint\": \"swagger\",\n            \"route\": \"/swagger.json\",\n            \"rule_filter\": lambda rule: True,\n            \"model_filter\": lambda tag: True,\n        }\n    ],\n    \"static_url_path\": \"/static\",\n    \"swagger_ui\": True,\n    \"specs_route\": \"/swagger/\",\n}\n\nswagger = Swagger(app, config=swagger_config, template_file=\"swagger_docs/docker_compose.yml\")\n\nclass DockerComposeResource(Resource):\n    @swag_from(\"swagger_docs/docker_compose.yml\")\n    def get(self):\n        generator = DockerComposeGenerator()\n        updated_yaml = generator.update_docker_compose()\n        return jsonify(updated_yaml)\n\napi.add_resource(DockerComposeResource, '/bludelta/docker-compose')\n\nREDOC_URL = '/redoc'\n@app.route(REDOC_URL)\ndef redoc():\n    return render_template('redoc.html')\n\nif __name__ == '__main__':\n    app.run(debug=True, host='0.0.0.0', port=5000)\n",
        "file_name": "app.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Service\\docker_compose_generator.py": {
        "summary": "The `docker_compose_generator.py` file defines a class `DockerComposeGenerator` that automates the process of updating Docker Compose files with the latest image versions from an Azure Container Registry. \n\n### Key Components:\n\n1. **Initialization**:\n   - Reads Azure configuration from a `config.ini` file.\n   - Sets up Azure credentials using `ClientSecretCredential` and initializes the `ContainerRegistryManagementClient`.\n\n2. **Getting Latest Image Versions**:\n   - The method `get_latest_image_versions` authenticates and retrieves repositories from the specified Azure Container Registry.\n   - It collects the latest tag (excluding 'latest') for each repository and returns a dictionary mapping repository names to their latest tags.\n\n3. **Updating Docker Compose**:\n   - The method `update_docker_compose` reads a Docker Compose YAML template file and updates the image tags for services based on the latest versions retrieved from the Azure registry.\n   - It logs the changes made to the image tags.\n\n### Notable Issues:\n- There is a commented-out section indicating a need to fix an error related to the `ContainerRegistryClient` initialization, which requires the `audience` argument to be set.\n\nOverall, this script is designed to facilitate the management of Docker images in a CI/CD pipeline by ensuring that the Docker Compose file reflects the most current versions of images stored in Azure.",
        "content": "import os\nimport configparser\nimport yaml\nfrom azure.identity import ClientSecretCredential\nfrom azure.mgmt.containerregistry import ContainerRegistryManagementClient\nfrom azure.containerregistry import ContainerRegistryClient\nimport docker\n\nclass DockerComposeGenerator:\n    def __init__(self):\n        self.config = configparser.ConfigParser()\n        self.config.read('config.ini')\n        self.azure_config = self.config['azure']\n        self.credentials = ClientSecretCredential(\n            tenant_id=self.azure_config['tenant_id'],\n            client_id=self.azure_config['client_id'],\n            client_secret=self.azure_config['client_secret']\n        )\n        self.registry_client = ContainerRegistryManagementClient(\n            self.credentials,\n            self.azure_config['subscription_id']\n        )\n\n    def get_latest_image_versions(self):\n        \"\"\"\n        Create a ContainerRegistryClient that will authenticate through Active Directory.\n        All necessary credential data is stored in config.ini.\n        Then get all repositories from the given azure registry. The registry name as well\n        as the resource_group are also stored in the config.ini file. Iterate over all repositories\n        and get the latest tag other than 'latest'.\n        Return the images with their tags\n        \"\"\"\n        images = {}\n        registries = self.registry_client.registries.list_by_resource_group(\n            self.azure_config['resource_group']\n        )\n        registry_login_server = None\n        for registry in registries:\n            if registry.name == self.azure_config['registry_name']:\n                registry_login_server = registry.login_server\n                break\n\n        if not registry_login_server:\n            raise ValueError(\"Registry not found\")\n\n        \"\"\"\n        Please fix this part, I get the following error:\n        Exception has occurred: ValueError\n        The argument audience must be set to initialize ContainerRegistryClient.\n        \"\"\"\n\n        audience = \"https://management.azure.com\"\n        repository_client = ContainerRegistryClient(\n            f\"https://{registry_login_server}\",\n            self.credentials,\n            audience=audience\n        )\n\n        repositories = repository_client.list_repository_names()\n\n        for repository in repositories:\n            tags = list(repository_client.list_tag_properties(repository))\n            latest_tag = max(tags, key=lambda x: x.last_updated_on)\n            images[repository] = latest_tag.name\n\n        return images\n\n    def update_docker_compose(self):\n        latest_versions = self.get_latest_image_versions()\n        with open('docker-compose-prod-template.yaml', 'r') as f:\n            docker_compose = yaml.safe_load(f)\n\n        for service in docker_compose['services']:\n            image = docker_compose['services'][service]['image']\n            image = image.split('/')[-1]\n            image_without_tag = image.split(':')[0]\n\n            for repo, version in latest_versions.items():\n                if repo == image_without_tag:\n                    # TODO: make registry name configurable\n                    #docker_compose['services'][service]['image'] = f'blumatixreleaseregistry.azurecr.io/{repo}:{version}'\n                    docker_compose['services'][service]['image'] = f'blumatixdevregistry.azurecr.io/{repo}:{version}'\n                    print(f'old_image: {image} - new_image: {repo}:{version}')\n                    break\n\n        return docker_compose\n",
        "file_name": "docker_compose_generator.py"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\requirements.txt": {
        "summary": "The file `requirements.txt` lists a collection of Python package dependencies for a project. Each line specifies a package along with its version, ensuring that the correct versions are installed when setting up the environment. Key packages included are:\n\n- **Web Development**: `Flask`, `Flask-Cors`, `Flask-RESTful`\n- **Data Science**: `numpy`, `pandas`, `scipy`, `seaborn`, `matplotlib`\n- **Machine Learning**: `torch`, `transformers`, `mlflow`, `langchain`\n- **Azure Services**: `azure-common`, `azure-identity`, `azure-mgmt-containerregistry`\n- **HTTP and Networking**: `aiohttp`, `httpx`, `requests`\n- **Others**: `sqlalchemy`, `pytest`, `streamlit`, `rich`\n\nThe file includes various libraries for handling data, making HTTP requests, and integrating with cloud services. Some packages have commented-out versions, suggesting potential alternatives or updates.",
        "content": "aiohttp==3.8.5\naiosignal==1.3.1\naltair==4.2.2\naniso8601==9.0.1\nanyio==4.0.0\nasync-timeout==4.0.3\nattrs==22.2.0\nazure-common==1.1.28\nazure-containerregistry==1.0.0\nazure-core==1.26.3\nazure-identity==1.7.0\nazure-mgmt-containerregistry==8.1.0\nazure-mgmt-core==1.3.2\nbackoff==2.2.1\nbackports.zoneinfo==0.2.1\nblinker==1.5\ncachetools==5.3.0\ncertifi==2022.12.7\ncffi==1.15.1\ncharset-normalizer==3.1.0\nclarifai==9.7.6\nclarifai-grpc==9.7.5\nclick==8.1.3\ncohere==4.21\ncolorama==0.4.6\ncryptography==39.0.2\ndataclasses-json==0.5.14\ndecorator==5.1.1\ndill==0.3.7\ndocker==6.0.1\ndocopt==0.6.2\nentrypoints==0.4\nexceptiongroup==1.1.3\nfastavro==1.8.2\nfilelock==3.12.2\nflasgger==0.9.5\nFlask==2.2.3\nFlask-Cors==3.0.10\nFlask-RESTful==0.3.9\nflask-swagger-ui==4.11.1\nfrozenlist==1.4.0\nfsspec==2023.6.0\ngitdb==4.0.10\nGitPython==3.1.31\ngoogleapis-common-protos==1.60.0\ngreenlet==2.0.2\ngrpcio==1.57.0\nh11==0.14.0\nhttpcore==0.18.0\nhttpx==0.25.0\nhugchat==0.0.8\nhuggingface-hub==0.16.4\nidna==3.4\nimportlib-metadata==6.1.0\nimportlib-resources==6.0.0\n# install==1.3.5\nisodate==0.6.1\nitsdangerous==2.1.2\nJinja2==3.1.2\njsonschema==4.17.3\nlangchain==0.0.276\nlangsmith #==0.0.27\nmanifest-ml==0.0.1\nmarkdown-it-py==2.2.0\nMarkupSafe==2.1.2\nmarshmallow==3.20.1\nmdurl==0.1.2\nmistune==2.0.5\nmlflow==2.11.3 #2.15.1\nmpmath==1.3.0\nmsal==1.21.0\nmsal-extensions==0.3.1\nmsrest==0.7.1\nmultidict==6.0.4\nmypy-extensions==1.0.0\nnetworkx==3.1\nnlpcloud==1.1.44\nnumexpr==2.8.5\nnumpy==1.24.2\noauthlib==3.2.2\nopenai==0.27.9\nopenlm==0.0.5\nopenpyxl==3.1.5\npackaging==23.0\npandas==1.5.3\npdf2image==1.16.3\nPillow==9.4.0\npipreqs==0.4.11\npkgutil_resolve_name==1.3.10\nportalocker==2.7.0\nprotobuf==3.20.3\npyarrow==11.0.0\npycparser==2.21\npydantic==1.10.12\npydeck==0.8.0\nPygments==2.14.0\nPyJWT==2.6.0\nPympler==1.0.1\npyodbc==4.0.39\npyrsistent==0.19.3\npython-dateutil==2.8.2\npython-rapidjson==1.10\npytz==2022.7.1\npytz-deprecation-shim==0.1.0.post0\npywin32==305\nPyYAML==6.0\nredis==5.0.0\nregex==2023.8.8\nrequests==2.28.2\nrequests-oauthlib==1.3.1\nrequests-toolbelt==1.0.0\nrich==13.4.2\nsafetensors==0.3.3\nscipy==1.10.1\nseaborn==0.13.2\nsemver==2.13.0\nsix==1.16.0\nsmmap==5.0.0\nsniffio==1.3.0\nSQLAlchemy==2.0.20\nsqlitedict==2.1.0\nstreamlit==1.26.0\nsympy==1.12\ntenacity==8.2.2\ntext-generation==0.6.0\ntokenizers==0.13.3\ntoml==0.10.2\ntoolz==0.12.0\ntorch==2.0.1\ntornado==6.2\ntqdm==4.64.1\ntransformers==4.32.1\ntritonclient==2.34.0\ntyping-inspect==0.9.0\ntyping_extensions==4.5.0\ntzdata==2022.7\ntzlocal==4.3\nurllib3==1.26.15\nvalidators==0.20.0\nwatchdog==3.0.0\nwebsocket-client==1.5.1\nWerkzeug==2.2.3\nyarg==0.1.9\nyarl==1.9.2\nzipp==3.15.0\nstreamlit_pdf_viewer==0.0.9\nseaborn",
        "file_name": "requirements.txt"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\requirements.txt": {
        "summary": "The `requirements.txt` file lists the dependencies for a Python project called \"Bennu.\" It specifies exact versions for various packages, including:\n\n- Visualization and data handling libraries like `altair`, `pandas`, and `numpy`.\n- Web frameworks such as `Flask` and its extensions for RESTful APIs and CORS.\n- Azure-related libraries for cloud management and identity services.\n- Additional utilities for tasks such as caching, command-line interfaces, and markdown processing.\n\nNotable package versions include:\n- `Flask==2.2.3`\n- `numpy==1.24.2`\n- `pandas==1",
        "content": "\u00ff\u00fea\u0000l\u0000t\u0000a\u0000i\u0000r\u0000=\u0000=\u00004\u0000.\u00002\u0000.\u00002\u0000\n\u0000\n\u0000a\u0000n\u0000i\u0000s\u0000o\u00008\u00006\u00000\u00001\u0000=\u0000=\u00009\u0000.\u00000\u0000.\u00001\u0000\n\u0000\n\u0000a\u0000t\u0000t\u0000r\u0000s\u0000=\u0000=\u00002\u00002\u0000.\u00002\u0000.\u00000\u0000\n\u0000\n\u0000a\u0000z\u0000u\u0000r\u0000e\u0000-\u0000c\u0000o\u0000m\u0000m\u0000o\u0000n\u0000=\u0000=\u00001\u0000.\u00001\u0000.\u00002\u00008\u0000\n\u0000\n\u0000a\u0000z\u0000u\u0000r\u0000e\u0000-\u0000c\u0000o\u0000n\u0000t\u0000a\u0000i\u0000n\u0000e\u0000r\u0000r\u0000e\u0000g\u0000i\u0000s\u0000t\u0000r\u0000y\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00000\u0000\n\u0000\n\u0000a\u0000z\u0000u\u0000r\u0000e\u0000-\u0000c\u0000o\u0000r\u0000e\u0000=\u0000=\u00001\u0000.\u00002\u00006\u0000.\u00003\u0000\n\u0000\n\u0000a\u0000z\u0000u\u0000r\u0000e\u0000-\u0000i\u0000d\u0000e\u0000n\u0000t\u0000i\u0000t\u0000y\u0000=\u0000=\u00001\u0000.\u00007\u0000.\u00000\u0000\n\u0000\n\u0000a\u0000z\u0000u\u0000r\u0000e\u0000-\u0000m\u0000g\u0000m\u0000t\u0000-\u0000c\u0000o\u0000n\u0000t\u0000a\u0000i\u0000n\u0000e\u0000r\u0000r\u0000e\u0000g\u0000i\u0000s\u0000t\u0000r\u0000y\u0000=\u0000=\u00008\u0000.\u00001\u0000.\u00000\u0000\n\u0000\n\u0000a\u0000z\u0000u\u0000r\u0000e\u0000-\u0000m\u0000g\u0000m\u0000t\u0000-\u0000c\u0000o\u0000r\u0000e\u0000=\u0000=\u00001\u0000.\u00003\u0000.\u00002\u0000\n\u0000\n\u0000b\u0000l\u0000i\u0000n\u0000k\u0000e\u0000r\u0000=\u0000=\u00001\u0000.\u00005\u0000\n\u0000\n\u0000c\u0000a\u0000c\u0000h\u0000e\u0000t\u0000o\u0000o\u0000l\u0000s\u0000=\u0000=\u00005\u0000.\u00003\u0000.\u00000\u0000\n\u0000\n\u0000c\u0000e\u0000r\u0000t\u0000i\u0000f\u0000i\u0000=\u0000=\u00002\u00000\u00002\u00002\u0000.\u00001\u00002\u0000.\u00007\u0000\n\u0000\n\u0000c\u0000f\u0000f\u0000i\u0000=\u0000=\u00001\u0000.\u00001\u00005\u0000.\u00001\u0000\n\u0000\n\u0000c\u0000h\u0000a\u0000r\u0000s\u0000e\u0000t\u0000-\u0000n\u0000o\u0000r\u0000m\u0000a\u0000l\u0000i\u0000z\u0000e\u0000r\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00000\u0000\n\u0000\n\u0000c\u0000l\u0000i\u0000c\u0000k\u0000=\u0000=\u00008\u0000.\u00001\u0000.\u00003\u0000\n\u0000\n\u0000c\u0000o\u0000l\u0000o\u0000r\u0000a\u0000m\u0000a\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00006\u0000\n\u0000\n\u0000c\u0000r\u0000y\u0000p\u0000t\u0000o\u0000g\u0000r\u0000a\u0000p\u0000h\u0000y\u0000=\u0000=\u00003\u00009\u0000.\u00000\u0000.\u00002\u0000\n\u0000\n\u0000d\u0000e\u0000c\u0000o\u0000r\u0000a\u0000t\u0000o\u0000r\u0000=\u0000=\u00005\u0000.\u00001\u0000.\u00001\u0000\n\u0000\n\u0000d\u0000o\u0000c\u0000k\u0000e\u0000r\u0000=\u0000=\u00006\u0000.\u00000\u0000.\u00001\u0000\n\u0000\n\u0000d\u0000o\u0000c\u0000o\u0000p\u0000t\u0000=\u0000=\u00000\u0000.\u00006\u0000.\u00002\u0000\n\u0000\n\u0000e\u0000n\u0000t\u0000r\u0000y\u0000p\u0000o\u0000i\u0000n\u0000t\u0000s\u0000=\u0000=\u00000\u0000.\u00004\u0000\n\u0000\n\u0000f\u0000l\u0000a\u0000s\u0000g\u0000g\u0000e\u0000r\u0000=\u0000=\u00000\u0000.\u00009\u0000.\u00005\u0000\n\u0000\n\u0000F\u0000l\u0000a\u0000s\u0000k\u0000=\u0000=\u00002\u0000.\u00002\u0000.\u00003\u0000\n\u0000\n\u0000F\u0000l\u0000a\u0000s\u0000k\u0000-\u0000C\u0000o\u0000r\u0000s\u0000=\u0000=\u00003\u0000.\u00000\u0000.\u00001\u00000\u0000\n\u0000\n\u0000F\u0000l\u0000a\u0000s\u0000k\u0000-\u0000R\u0000E\u0000S\u0000T\u0000f\u0000u\u0000l\u0000=\u0000=\u00000\u0000.\u00003\u0000.\u00009\u0000\n\u0000\n\u0000f\u0000l\u0000a\u0000s\u0000k\u0000-\u0000s\u0000w\u0000a\u0000g\u0000g\u0000e\u0000r\u0000-\u0000u\u0000i\u0000=\u0000=\u00004\u0000.\u00001\u00001\u0000.\u00001\u0000\n\u0000\n\u0000g\u0000i\u0000t\u0000d\u0000b\u0000=\u0000=\u00004\u0000.\u00000\u0000.\u00001\u00000\u0000\n\u0000\n\u0000G\u0000i\u0000t\u0000P\u0000y\u0000t\u0000h\u0000o\u0000n\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00003\u00001\u0000\n\u0000\n\u0000i\u0000d\u0000n\u0000a\u0000=\u0000=\u00003\u0000.\u00004\u0000\n\u0000\n\u0000i\u0000m\u0000p\u0000o\u0000r\u0000t\u0000l\u0000i\u0000b\u0000-\u0000m\u0000e\u0000t\u0000a\u0000d\u0000a\u0000t\u0000a\u0000=\u0000=\u00006\u0000.\u00001\u0000.\u00000\u0000\n\u0000\n\u0000\n\u0000\n\u0000i\u0000s\u0000o\u0000d\u0000a\u0000t\u0000e\u0000=\u0000=\u00000\u0000.\u00006\u0000.\u00001\u0000\n\u0000\n\u0000i\u0000t\u0000s\u0000d\u0000a\u0000n\u0000g\u0000e\u0000r\u0000o\u0000u\u0000s\u0000=\u0000=\u00002\u0000.\u00001\u0000.\u00002\u0000\n\u0000\n\u0000J\u0000i\u0000n\u0000j\u0000a\u00002\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00002\u0000\n\u0000\n\u0000j\u0000s\u0000o\u0000n\u0000s\u0000c\u0000h\u0000e\u0000m\u0000a\u0000=\u0000=\u00004\u0000.\u00001\u00007\u0000.\u00003\u0000\n\u0000\n\u0000m\u0000a\u0000r\u0000k\u0000d\u0000o\u0000w\u0000n\u0000-\u0000i\u0000t\u0000-\u0000p\u0000y\u0000=\u0000=\u00002\u0000.\u00002\u0000.\u00000\u0000\n\u0000\n\u0000M\u0000a\u0000r\u0000k\u0000u\u0000p\u0000S\u0000a\u0000f\u0000e\u0000=\u0000=\u00002\u0000.\u00001\u0000.\u00002\u0000\n\u0000\n\u0000m\u0000d\u0000u\u0000r\u0000l\u0000=\u0000=\u00000\u0000.\u00001\u0000.\u00002\u0000\n\u0000\n\u0000m\u0000i\u0000s\u0000t\u0000u\u0000n\u0000e\u0000=\u0000=\u00002\u0000.\u00000\u0000.\u00005\u0000\n\u0000\n\u0000m\u0000s\u0000a\u0000l\u0000=\u0000=\u00001\u0000.\u00002\u00001\u0000.\u00000\u0000\n\u0000\n\u0000m\u0000s\u0000a\u0000l\u0000-\u0000e\u0000x\u0000t\u0000e\u0000n\u0000s\u0000i\u0000o\u0000n\u0000s\u0000=\u0000=\u00000\u0000.\u00003\u0000.\u00001\u0000\n\u0000\n\u0000m\u0000s\u0000r\u0000e\u0000s\u0000t\u0000=\u0000=\u00000\u0000.\u00007\u0000.\u00001\u0000\n\u0000\n\u0000n\u0000u\u0000m\u0000p\u0000y\u0000=\u0000=\u00001\u0000.\u00002\u00004\u0000.\u00002\u0000\n\u0000\n\u0000o\u0000a\u0000u\u0000t\u0000h\u0000l\u0000i\u0000b\u0000=\u0000=\u00003\u0000.\u00002\u0000.\u00002\u0000\n\u0000\n\u0000o\u0000p\u0000e\u0000n\u0000p\u0000y\u0000x\u0000l\u0000=\u0000=\u00003\u0000.\u00001\u0000.\u00005\u0000\n\u0000\n\u0000p\u0000a\u0000c\u0000k\u0000a\u0000g\u0000i\u0000n\u0000g\u0000=\u0000=\u00002\u00003\u0000.\u00000\u0000\n\u0000\n\u0000p\u0000a\u0000n\u0000d\u0000a\u0000s\u0000=\u0000=\u00001\u0000.\u00005\u0000.\u00003\u0000\n\u0000\n\u0000P\u0000i\u0000l\u0000l\u0000o\u0000w\u0000=\u0000=\u00009\u0000.\u00004\u0000.\u00000\u0000\n\u0000\n\u0000p\u0000i\u0000p\u0000r\u0000e\u0000q\u0000s\u0000=\u0000=\u00000\u0000.\u00004\u0000.\u00001\u00001\u0000\n\u0000\n\u0000p\u0000o\u0000r\u0000t\u0000a\u0000l\u0000o\u0000c\u0000k\u0000e\u0000r\u0000=\u0000=\u00002\u0000.\u00007\u0000.\u00000\u0000\n\u0000\n\u0000p\u0000r\u0000o\u0000t\u0000o\u0000b\u0000u\u0000f\u0000=\u0000=\u00003\u0000.\u00002\u00000\u0000.\u00003\u0000\n\u0000\n\u0000p\u0000y\u0000a\u0000r\u0000r\u0000o\u0000w\u0000=\u0000=\u00001\u00001\u0000.\u00000\u0000.\u00000\u0000\n\u0000\n\u0000p\u0000y\u0000c\u0000p\u0000a\u0000r\u0000s\u0000e\u0000r\u0000=\u0000=\u00002\u0000.\u00002\u00001\u0000\n\u0000\n\u0000p\u0000y\u0000d\u0000e\u0000c\u0000k\u0000=\u0000=\u00000\u0000.\u00008\u0000.\u00000\u0000\n\u0000\n\u0000P\u0000y\u0000g\u0000m\u0000e\u0000n\u0000t\u0000s\u0000=\u0000=\u00002\u0000.\u00001\u00004\u0000.\u00000\u0000\n\u0000\n\u0000P\u0000y\u0000J\u0000W\u0000T\u0000=\u0000=\u00002\u0000.\u00006\u0000.\u00000\u0000\n\u0000\n\u0000P\u0000y\u0000m\u0000p\u0000l\u0000e\u0000r\u0000=\u0000=\u00001\u0000.\u00000\u0000.\u00001\u0000\n\u0000\n\u0000p\u0000y\u0000r\u0000s\u0000i\u0000s\u0000t\u0000e\u0000n\u0000t\u0000=\u0000=\u00000\u0000.\u00001\u00009\u0000.\u00003\u0000\n\u0000\n\u0000p\u0000y\u0000t\u0000h\u0000o\u0000n\u0000-\u0000d\u0000a\u0000t\u0000e\u0000u\u0000t\u0000i\u0000l\u0000=\u0000=\u00002\u0000.\u00008\u0000.\u00002\u0000\n\u0000\n\u0000p\u0000y\u0000t\u0000z\u0000=\u0000=\u00002\u00000\u00002\u00002\u0000.\u00007\u0000.\u00001\u0000\n\u0000\n\u0000p\u0000y\u0000t\u0000z\u0000-\u0000d\u0000e\u0000p\u0000r\u0000e\u0000c\u0000a\u0000t\u0000i\u0000o\u0000n\u0000-\u0000s\u0000h\u0000i\u0000m\u0000=\u0000=\u00000\u0000.\u00001\u0000.\u00000\u0000.\u0000p\u0000o\u0000s\u0000t\u00000\u0000\n\u0000\n\u0000p\u0000y\u0000w\u0000i\u0000n\u00003\u00002\u0000\n\u0000\n\u0000P\u0000y\u0000Y\u0000A\u0000M\u0000L\u0000=\u0000=\u00006\u0000.\u00000\u0000\n\u0000\n\u0000r\u0000e\u0000q\u0000u\u0000e\u0000s\u0000t\u0000s\u0000=\u0000=\u00002\u0000.\u00002\u00008\u0000.\u00002\u0000\n\u0000\n\u0000r\u0000e\u0000q\u0000u\u0000e\u0000s\u0000t\u0000s\u0000-\u0000o\u0000a\u0000u\u0000t\u0000h\u0000l\u0000i\u0000b\u0000=\u0000=\u00001\u0000.\u00003\u0000.\u00001\u0000\n\u0000\n\u0000r\u0000i\u0000c\u0000h\u0000=\u0000=\u00001\u00003\u0000.\u00003\u0000.\u00002\u0000\n\u0000\n\u0000s\u0000e\u0000m\u0000v\u0000e\u0000r\u0000=\u0000=\u00002\u0000.\u00001\u00003\u0000.\u00000\u0000\n\u0000\n\u0000s\u0000i\u0000x\u0000=\u0000=\u00001\u0000.\u00001\u00006\u0000.\u00000\u0000\n\u0000\n\u0000s\u0000m\u0000m\u0000a\u0000p\u0000=\u0000=\u00005\u0000.\u00000\u0000.\u00000\u0000\n\u0000\n\u0000s\u0000t\u0000r\u0000e\u0000a\u0000m\u0000l\u0000i\u0000t\u0000=\u0000=\u00001\u0000.\u00002\u00000\u0000.\u00000\u0000\n\u0000\n\u0000t\u0000o\u0000m\u0000l\u0000=\u0000=\u00000\u0000.\u00001\u00000\u0000.\u00002\u0000\n\u0000\n\u0000t\u0000o\u0000o\u0000l\u0000z\u0000=\u0000=\u00000\u0000.\u00001\u00002\u0000.\u00000\u0000\n\u0000\n\u0000t\u0000o\u0000r\u0000n\u0000a\u0000d\u0000o\u0000=\u0000=\u00006\u0000.\u00002\u0000\n\u0000\n\u0000t\u0000y\u0000p\u0000i\u0000n\u0000g\u0000_\u0000e\u0000x\u0000t\u0000e\u0000n\u0000s\u0000i\u0000o\u0000n\u0000s\u0000=\u0000=\u00004\u0000.\u00005\u0000.\u00000\u0000\n\u0000\n\u0000t\u0000z\u0000d\u0000a\u0000t\u0000a\u0000=\u0000=\u00002\u00000\u00002\u00002\u0000.\u00007\u0000\n\u0000\n\u0000t\u0000z\u0000l\u0000o\u0000c\u0000a\u0000l\u0000=\u0000=\u00004\u0000.\u00003\u0000\n\u0000\n\u0000u\u0000r\u0000l\u0000l\u0000i\u0000b\u00003\u0000=\u0000=\u00001\u0000.\u00002\u00006\u0000.\u00001\u00005\u0000\n\u0000\n\u0000v\u0000a\u0000l\u0000i\u0000d\u0000a\u0000t\u0000o\u0000r\u0000s\u0000=\u0000=\u00000\u0000.\u00002\u00000\u0000.\u00000\u0000\n\u0000\n\u0000w\u0000a\u0000t\u0000c\u0000h\u0000d\u0000o\u0000g\u0000=\u0000=\u00003\u0000.\u00000\u0000.\u00000\u0000\n\u0000\n\u0000w\u0000e\u0000b\u0000s\u0000o\u0000c\u0000k\u0000e\u0000t\u0000-\u0000c\u0000l\u0000i\u0000e\u0000n\u0000t\u0000=\u0000=\u00001\u0000.\u00005\u0000.\u00001\u0000\n\u0000\n\u0000W\u0000e\u0000r\u0000k\u0000z\u0000e\u0000u\u0000g\u0000=\u0000=\u00002\u0000.\u00002\u0000.\u00003\u0000\n\u0000\n\u0000y\u0000a\u0000r\u0000g\u0000=\u0000=\u00000\u0000.\u00001\u0000.\u00009\u0000\n\u0000\n\u0000z\u0000i\u0000p\u0000p\u0000=\u0000=\u00003\u0000.\u00001\u00005\u0000.\u00000\u0000\n\u0000\n\u0000p\u0000y\u0000o\u0000d\u0000b\u0000c\u0000\n\u0000\n\u0000l\u0000a\u0000n\u0000g\u0000c\u0000h\u0000a\u0000i\u0000n\u0000\n\u0000\n\u0000t\u0000a\u0000b\u0000u\u0000l\u0000a\u0000t\u0000e\u0000",
        "file_name": "requirements.txt"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Frontend\\requirements.txt": {
        "summary": "The file `requirements.txt` lists the dependencies required for the Bennu Frontend project. Key libraries specified include:\n\n- **Data Manipulation and Analysis**: `pandas`, `numpy`, `openpyxl`, `pyarrow`\n- **Web Frameworks**: `Flask`, `streamlit`, `Flask-RESTful`\n- **Data Visualization**: `matplotlib`, `seaborn`, `altair`, `pydeck`\n- **Machine Learning and AI**: `mlflow`, `langchain`, `openai`, `text_generation`\n- **Database and API Interactions**: `pyodbc`, `httpx`, `requests`, `requests-oauthlib`\n- **Azure SDKs**: Various packages for Azure services like `azure-common`, `azure-identity`\n- **Utilities**: `GitPython`, `pipreqs`, `markdown-it-py`, `validators`, `watchdog`\n\nSome packages have specific versions pinned, while others like `text_generation`, `langchain`, and `tabulate` are listed without version constraints. Comments indicate some packages (e.g., `streamlit` and `PyYAML`) may be optional or outdated.",
        "content": "pandas==1.5.3\npyodbc==4.0.39\nPyYAML==6.0.1\nrequests==2.28.2\nrequests-toolbelt==1.0.0\nGitPython==3.1.31\nseaborn==0.13.2\n#streamlit==1.12.0\nstreamlit-pdf-viewer==0.0.9\ntext_generation\nlangchain\npdf2image==1.16.3\nhttpx==0.25.0\nmatplotlib== 3.7.4\nopenpyxl==3.1.5\nmlflow==2.11.3 #2.15.1\n\n\naltair==4.2.2\naltair==4.2.2\naniso8601==9.0.1\nattrs==22.2.0\nazure-common==1.1.28\nazure-containerregistry==1.0.0\nazure-core==1.26.3\nazure-identity==1.7.0\nazure-mgmt-containerregistry==8.1.0\nazure-mgmt-core==1.3.2\nblinker==1.5\ncachetools==5.3.0\ncertifi==2022.12.7\ncffi==1.15.1\ncharset-normalizer==3.1.0\nclick==8.1.3\ncolorama==0.4.6\ncryptography==39.0.2\ndecorator==5.1.1\ndocker==6.0.1\ndocopt==0.6.2\nentrypoints==0.4\nflasgger==0.9.5\nFlask==2.2.3\nFlask-Cors==3.0.10\nFlask-RESTful==0.3.9\nflask-swagger-ui==4.11.1\ngitdb==4.0.10\nGitPython==3.1.31\nidna==3.4\nimportlib-metadata==6.1.0\n\nisodate==0.6.1\nitsdangerous==2.1.2\nJinja2==3.1.2\njsonschema==4.17.3\nmarkdown-it-py==2.2.0\nMarkupSafe==2.1.2\nmdurl==0.1.2\nmistune==2.0.5\nmsal==1.21.0\nmsal-extensions==0.3.1\nmsrest==0.7.1\nnumpy==1.24.2\noauthlib==3.2.2\nopenai==0.27.9\nopenpyxl==3.1.5\npackaging==23.0\npandas==1.5.3\nPillow==9.4.0\npipreqs==0.4.11\nportalocker==2.7.0\nprotobuf==3.20.3\npyarrow==11.0.0\npycparser==2.21\npydeck==0.8.0\nPygments==2.14.0\nPyJWT==2.6.0\nPympler==1.0.1\npyrsistent==0.19.3\npython-dateutil==2.8.2\npytz==2022.7.1\npytz-deprecation-shim==0.1.0.post0\n#pywin32\n#PyYAML==6.0\nrequests==2.28.2\nrequests-oauthlib==1.3.1\nrich==13.3.2\nsemver==2.13.0\nsix==1.16.0\nsmmap==5.0.0\nstreamlit==1.20.0\nstreamlit-ace==0.1.1\ntoml==0.10.2\ntoolz==0.12.0\ntornado==6.2\ntyping_extensions==4.5.0\ntzdata==2022.7\ntzlocal==4.3\nurllib3==1.26.15\nvalidators==0.20.0\nwatchdog==3.0.0\nwebsocket-client==1.5.1\nWerkzeug==2.2.3\nyarg==0.1.9\nzipp==3.15.0\npyodbc\nlangchain\ntabulate\n",
        "file_name": "requirements.txt"
    },
    "C:\\Users\\rudi\\source\\repos\\Tools\\Bennu\\Service\\requirements.txt": {
        "summary": "The file 'requirements.txt' lists the dependencies required for a Python project related to Azure and web development. Key packages include:\n\n- Azure SDK components: `azure-common`, `azure-containerregistry`, `azure-core`, `azure-identity`, `azure-mgmt-containerregistry`, and `azure-mgmt-core`.\n- Web framework and related libraries: `Flask`, `Flask_Cors`, `Flask_RESTful`, and `flasgger`.\n- Additional libraries: `docker_py` for Docker interactions and `PyYAML` for YAML file handling.\n\nEach package is specified with its version, ensuring compatibility in the project environment.",
        "content": "azure-common==1.1.28\nazure-containerregistry==1.0.0     \nazure-core==1.26.3\nazure-identity==1.7.0\nazure-mgmt-containerregistry==8.1.0\nazure-mgmt-core==1.3.2\ndocker_py==1.10.6\nflasgger==0.9.5\nFlask==2.2.3\nFlask_Cors==3.0.10\nFlask_RESTful==0.3.9\nPyYAML==6.0.1\n",
        "file_name": "requirements.txt"
    }
}